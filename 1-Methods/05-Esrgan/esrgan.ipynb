{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "toc_visible": true,
      "authorship_tag": "ABX9TyMaSb9FBJgvGVATamou81i+"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "source: https://github.com/eriklindernoren/PyTorch-GAN/tree/master/implementations/esrgan"
      ],
      "metadata": {
        "id": "nQDuB9x16YsF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YeuE9YOU6Tc6",
        "outputId": "ba21db3e-39ab-4ab1-d828-c1151eb74e86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import os,shutil\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data"
      ],
      "metadata": {
        "id": "Kcu2_9V6dwh2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import random\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Normalization parameters for pre-trained PyTorch models\n",
        "mean = np.array([0.485, 0.456, 0.406])\n",
        "std = np.array([0.229, 0.224, 0.225])\n",
        "\n",
        "\n",
        "def denormalize(tensors):\n",
        "    \"\"\" Denormalizes image tensors using mean and std \"\"\"\n",
        "    for c in range(3):\n",
        "        tensors[:, c].mul_(std[c]).add_(mean[c])\n",
        "    return torch.clamp(tensors, 0, 255)\n",
        "\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, root, hr_shape):\n",
        "        hr_height, hr_width = hr_shape\n",
        "        # Transforms for low resolution images and high resolution images\n",
        "        self.lr_transform = transforms.Compose(\n",
        "            [\n",
        "                transforms.Resize((hr_height // 4, hr_height // 4), Image.BICUBIC),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean, std),\n",
        "            ]\n",
        "        )\n",
        "        self.hr_transform = transforms.Compose(\n",
        "            [\n",
        "                transforms.Resize((hr_height, hr_height), Image.BICUBIC),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean, std),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.files = sorted(glob.glob(root + \"/*.*\"))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img = Image.open(self.files[index % len(self.files)])\n",
        "        img_lr = self.lr_transform(img)\n",
        "        img_hr = self.hr_transform(img)\n",
        "\n",
        "        return {\"lr\": img_lr, \"hr\": img_hr}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)"
      ],
      "metadata": {
        "id": "s6h9_Oqz7atQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Architecture"
      ],
      "metadata": {
        "id": "3OkNmzrpdy4O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "from torchvision.models import vgg19\n",
        "import math\n",
        "\n",
        "\n",
        "class FeatureExtractor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FeatureExtractor, self).__init__()\n",
        "        vgg19_model = vgg19(pretrained=True)\n",
        "        self.vgg19_54 = nn.Sequential(*list(vgg19_model.features.children())[:35])\n",
        "\n",
        "    def forward(self, img):\n",
        "        return self.vgg19_54(img)\n",
        "\n",
        "\n",
        "class DenseResidualBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    The core module of paper: (Residual Dense Network for Image Super-Resolution, CVPR 18)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, filters, res_scale=0.2):\n",
        "        super(DenseResidualBlock, self).__init__()\n",
        "        self.res_scale = res_scale\n",
        "\n",
        "        def block(in_features, non_linearity=True):\n",
        "            layers = [nn.Conv2d(in_features, filters, 3, 1, 1, bias=True)]\n",
        "            if non_linearity:\n",
        "                layers += [nn.LeakyReLU()]\n",
        "            return nn.Sequential(*layers)\n",
        "\n",
        "        self.b1 = block(in_features=1 * filters)\n",
        "        self.b2 = block(in_features=2 * filters)\n",
        "        self.b3 = block(in_features=3 * filters)\n",
        "        self.b4 = block(in_features=4 * filters)\n",
        "        self.b5 = block(in_features=5 * filters, non_linearity=False)\n",
        "        self.blocks = [self.b1, self.b2, self.b3, self.b4, self.b5]\n",
        "\n",
        "    def forward(self, x):\n",
        "        inputs = x\n",
        "        for block in self.blocks:\n",
        "            out = block(inputs)\n",
        "            inputs = torch.cat([inputs, out], 1)\n",
        "        return out.mul(self.res_scale) + x\n",
        "\n",
        "\n",
        "class ResidualInResidualDenseBlock(nn.Module):\n",
        "    def __init__(self, filters, res_scale=0.2):\n",
        "        super(ResidualInResidualDenseBlock, self).__init__()\n",
        "        self.res_scale = res_scale\n",
        "        self.dense_blocks = nn.Sequential(\n",
        "            DenseResidualBlock(filters), DenseResidualBlock(filters), DenseResidualBlock(filters)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.dense_blocks(x).mul(self.res_scale) + x\n",
        "\n",
        "\n",
        "class GeneratorRRDB(nn.Module):\n",
        "    def __init__(self, channels, filters=64, num_res_blocks=16, num_upsample=2):\n",
        "        super(GeneratorRRDB, self).__init__()\n",
        "\n",
        "        # First layer\n",
        "        self.conv1 = nn.Conv2d(channels, filters, kernel_size=3, stride=1, padding=1)\n",
        "        # Residual blocks\n",
        "        self.res_blocks = nn.Sequential(*[ResidualInResidualDenseBlock(filters) for _ in range(num_res_blocks)])\n",
        "        # Second conv layer post residual blocks\n",
        "        self.conv2 = nn.Conv2d(filters, filters, kernel_size=3, stride=1, padding=1)\n",
        "        # Upsampling layers\n",
        "        upsample_layers = []\n",
        "        for _ in range(num_upsample):\n",
        "            upsample_layers += [\n",
        "                nn.Conv2d(filters, filters * 4, kernel_size=3, stride=1, padding=1),\n",
        "                nn.LeakyReLU(),\n",
        "                nn.PixelShuffle(upscale_factor=2),\n",
        "            ]\n",
        "        self.upsampling = nn.Sequential(*upsample_layers)\n",
        "        # Final output block\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(filters, filters, kernel_size=3, stride=1, padding=1),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(filters, channels, kernel_size=3, stride=1, padding=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out1 = self.conv1(x)\n",
        "        out = self.res_blocks(out1)\n",
        "        out2 = self.conv2(out)\n",
        "        out = torch.add(out1, out2)\n",
        "        out = self.upsampling(out)\n",
        "        out = self.conv3(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_shape):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        self.input_shape = input_shape\n",
        "        in_channels, in_height, in_width = self.input_shape\n",
        "        patch_h, patch_w = int(in_height / 2 ** 4), int(in_width / 2 ** 4)\n",
        "        self.output_shape = (1, patch_h, patch_w)\n",
        "\n",
        "        def discriminator_block(in_filters, out_filters, first_block=False):\n",
        "            layers = []\n",
        "            layers.append(nn.Conv2d(in_filters, out_filters, kernel_size=3, stride=1, padding=1))\n",
        "            if not first_block:\n",
        "                layers.append(nn.BatchNorm2d(out_filters))\n",
        "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "            layers.append(nn.Conv2d(out_filters, out_filters, kernel_size=3, stride=2, padding=1))\n",
        "            layers.append(nn.BatchNorm2d(out_filters))\n",
        "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "            return layers\n",
        "\n",
        "        layers = []\n",
        "        in_filters = in_channels\n",
        "        for i, out_filters in enumerate([64, 128, 256, 512]):\n",
        "            layers.extend(discriminator_block(in_filters, out_filters, first_block=(i == 0)))\n",
        "            in_filters = out_filters\n",
        "\n",
        "        layers.append(nn.Conv2d(out_filters, 1, kernel_size=3, stride=1, padding=1))\n",
        "\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, img):\n",
        "        return self.model(img)"
      ],
      "metadata": {
        "id": "9rp0zHhS7l1w"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train"
      ],
      "metadata": {
        "id": "WvqECHC_d47N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import os\n",
        "import numpy as np\n",
        "import math\n",
        "import itertools\n",
        "import sys\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import save_image, make_grid\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "\n",
        "os.makedirs(\"./drive/MyDrive/ColabNotebooks/ESRGAN-pytorch/images/training\", exist_ok=True)\n",
        "os.makedirs(\"./drive/MyDrive/ColabNotebooks/ESRGAN-pytorch/saved_models\", exist_ok=True)\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--epoch\", type=int, default=0, help=\"epoch to start training from\") #0\n",
        "parser.add_argument(\"--n_epochs\", type=int, default=200, help=\"number of epochs of training\") #200\n",
        "parser.add_argument(\"--dataset_name\", type=str, default=\"DIV2K\", help=\"name of the dataset\")\n",
        "parser.add_argument(\"--batch_size\", type=int, default=4, help=\"size of the batches\")\n",
        "parser.add_argument(\"--lr\", type=float, default=0.0002, help=\"adam: learning rate\")\n",
        "parser.add_argument(\"--b1\", type=float, default=0.9, help=\"adam: decay of first order momentum of gradient\")\n",
        "parser.add_argument(\"--b2\", type=float, default=0.999, help=\"adam: decay of first order momentum of gradient\")\n",
        "parser.add_argument(\"--decay_epoch\", type=int, default=100, help=\"epoch from which to start lr decay\")\n",
        "parser.add_argument(\"--n_cpu\", type=int, default=4, help=\"number of cpu threads to use during batch generation\") #8\n",
        "parser.add_argument(\"--hr_height\", type=int, default=256, help=\"high res. image height\")\n",
        "parser.add_argument(\"--hr_width\", type=int, default=256, help=\"high res. image width\")\n",
        "parser.add_argument(\"--channels\", type=int, default=3, help=\"number of image channels\")\n",
        "parser.add_argument(\"--sample_interval\", type=int, default=100, help=\"interval between saving image samples\")\n",
        "parser.add_argument(\"--checkpoint_interval\", type=int, default=5000, help=\"batch interval between model checkpoints\") #5000\n",
        "parser.add_argument(\"--residual_blocks\", type=int, default=23, help=\"number of residual blocks in the generator\")\n",
        "parser.add_argument(\"--warmup_batches\", type=int, default=500, help=\"number of batches with pixel-wise loss only\") #500\n",
        "parser.add_argument(\"--lambda_adv\", type=float, default=5e-3, help=\"adversarial loss weight\") #5 * (10 ^ -3)\n",
        "parser.add_argument(\"--lambda_pixel\", type=float, default=1e-2, help=\"pixel-wise loss weight\") # 1 * (10 ^ -2)\n",
        "opt = parser.parse_args(args=[])\n",
        "#print(opt)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "hr_shape = (opt.hr_height, opt.hr_width)\n",
        "\n",
        "\n",
        "# Initialize generator and discriminator\n",
        "generator = GeneratorRRDB(opt.channels, filters=64, num_res_blocks=opt.residual_blocks).to(device)\n",
        "discriminator = Discriminator(input_shape=(opt.channels, *hr_shape)).to(device)\n",
        "feature_extractor = FeatureExtractor().to(device)\n",
        "\n",
        "# Set feature extractor to inference mode\n",
        "feature_extractor.eval()\n",
        "\n",
        "# Losses\n",
        "criterion_GAN = torch.nn.BCEWithLogitsLoss().to(device)\n",
        "criterion_content = torch.nn.L1Loss().to(device)\n",
        "criterion_pixel = torch.nn.L1Loss().to(device)\n",
        "\n",
        "if opt.epoch != 0:\n",
        "    # Load pretrained models\n",
        "    generator.load_state_dict(torch.load(\"./drive/MyDrive/ColabNotebooks/ESRGAN-pytorch/saved_models/generator_%d.pth\" % opt.epoch))\n",
        "    discriminator.load_state_dict(torch.load(\"./drive/MyDrive/ColabNotebooks/ESRGAN-pytorch/saved_models/discriminator_%d.pth\" % opt.epoch))\n",
        "\n",
        "# Optimizers\n",
        "optimizer_G = torch.optim.Adam(generator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
        "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
        "\n",
        "Tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.Tensor\n",
        "\n",
        "dataloader = DataLoader(\n",
        "    ImageDataset(\"./drive/MyDrive/datasets/DIV2K/DIV2K_train_HR\" , hr_shape=hr_shape),\n",
        "    batch_size=opt.batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=opt.n_cpu,\n",
        ")\n",
        "\n",
        "# ----------\n",
        "#  Training\n",
        "# ----------\n",
        "\n",
        "for epoch in range(opt.epoch, opt.n_epochs):\n",
        "  for i, imgs in enumerate(dataloader):\n",
        "\n",
        "      batches_done = epoch * len(dataloader) + i\n",
        "\n",
        "      # Configure model input\n",
        "      imgs_lr = Variable(imgs[\"lr\"].type(Tensor))\n",
        "      imgs_hr = Variable(imgs[\"hr\"].type(Tensor))\n",
        "\n",
        "      # Adversarial ground truths\n",
        "      valid = Variable(Tensor(np.ones((imgs_lr.size(0), *discriminator.output_shape))), requires_grad=False)\n",
        "      fake = Variable(Tensor(np.zeros((imgs_lr.size(0), *discriminator.output_shape))), requires_grad=False)\n",
        "\n",
        "      # ------------------\n",
        "      #  Train Generators\n",
        "      # ------------------\n",
        "\n",
        "      optimizer_G.zero_grad()\n",
        "\n",
        "      # Generate a high resolution image from low resolution input\n",
        "      gen_hr = generator(imgs_lr)\n",
        "\n",
        "      # Measure pixel-wise loss against ground truth\n",
        "      loss_pixel = criterion_pixel(gen_hr, imgs_hr)\n",
        "\n",
        "      if batches_done < opt.warmup_batches:\n",
        "          # Warm-up (pixel-wise loss only)\n",
        "          loss_pixel.backward()\n",
        "          optimizer_G.step()\n",
        "          print(\n",
        "                \"[Epoch %d/%d] [Batch %d/%d] [G pixel: %f]\"\n",
        "                % (epoch, opt.n_epochs, i, len(dataloader), loss_pixel.item())\n",
        "          )\n",
        "          continue\n",
        "\n",
        "      # Extract validity predictions from discriminator\n",
        "      pred_real = discriminator(imgs_hr).detach()\n",
        "      pred_fake = discriminator(gen_hr)\n",
        "\n",
        "      # Adversarial loss (relativistic average GAN)\n",
        "      loss_GAN = criterion_GAN(pred_fake - pred_real.mean(0, keepdim=True), valid)\n",
        "\n",
        "      # Content loss\n",
        "      gen_features = feature_extractor(gen_hr)\n",
        "      real_features = feature_extractor(imgs_hr).detach()\n",
        "      loss_content = criterion_content(gen_features, real_features)\n",
        "\n",
        "      # Total generator loss\n",
        "      loss_G = loss_content + opt.lambda_adv * loss_GAN + opt.lambda_pixel * loss_pixel\n",
        "\n",
        "      loss_G.backward()\n",
        "      optimizer_G.step()\n",
        "\n",
        "      # ---------------------\n",
        "      #  Train Discriminator\n",
        "      # ---------------------\n",
        "\n",
        "      optimizer_D.zero_grad()\n",
        "\n",
        "      pred_real = discriminator(imgs_hr)\n",
        "      pred_fake = discriminator(gen_hr.detach())\n",
        "\n",
        "      # Adversarial loss for real and fake images (relativistic average GAN)\n",
        "      loss_real = criterion_GAN(pred_real - pred_fake.mean(0, keepdim=True), valid)\n",
        "      loss_fake = criterion_GAN(pred_fake - pred_real.mean(0, keepdim=True), fake)\n",
        "\n",
        "      # Total loss\n",
        "      loss_D = (loss_real + loss_fake) / 2\n",
        "\n",
        "      loss_D.backward()\n",
        "      optimizer_D.step()\n",
        "\n",
        "      # --------------\n",
        "      #  Log Progress\n",
        "      # --------------\n",
        "\n",
        "      print(\n",
        "            \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f, content: %f, adv: %f, pixel: %f]\"\n",
        "            % (\n",
        "                epoch,\n",
        "                opt.n_epochs,\n",
        "                i,\n",
        "                len(dataloader),\n",
        "                loss_D.item(),\n",
        "                loss_G.item(),\n",
        "                loss_content.item(),\n",
        "                loss_GAN.item(),\n",
        "                loss_pixel.item(),\n",
        "          )\n",
        "       )\n",
        "\n",
        "\n",
        "      if batches_done % opt.sample_interval == 0:\n",
        "          # Save image grid with upsampled inputs and ESRGAN outputs\n",
        "          imgs_lr = nn.functional.interpolate(imgs_lr, scale_factor=4)\n",
        "          img_grid = denormalize(torch.cat((imgs_lr, gen_hr), -1))\n",
        "          save_image(img_grid, \"./drive/MyDrive/ColabNotebooks/ESRGAN-pytorch/images/training/%d.png\" % batches_done, nrow=1, normalize=False)\n",
        "\n",
        "      if batches_done % opt.checkpoint_interval == 0:\n",
        "          # Save model checkpoints\n",
        "          torch.save(generator.state_dict(), \"./drive/MyDrive/ColabNotebooks/ESRGAN-pytorch/saved_models/generator_%d.pth\" % epoch)\n",
        "          print(\"saved generator %d successfully!\" %epoch)\n",
        "          torch.save(discriminator.state_dict(), \"./drive/MyDrive/ColabNotebooks/ESRGAN-pytorch/saved_models/discriminator_%d.pth\" %epoch)\n",
        "          print(\"saved discriminator %d successfully!\" %epoch)"
      ],
      "metadata": {
        "id": "9PsmwiSY7mwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Test"
      ],
      "metadata": {
        "id": "k0iPxEQGeGKb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/olivesgatech/dippykit.git"
      ],
      "metadata": {
        "id": "M7-wmpc8E2XC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Utils\n",
        "\n",
        "from dippykit.metrics import PSNR as psnr\n",
        "from dippykit.metrics import SSIM as ssim\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "from collections import OrderedDict\n",
        "\n",
        "import time\n",
        "class Timer():\n",
        "\n",
        "    def __init__(self):\n",
        "        self.v = time.time()\n",
        "\n",
        "    def s(self):\n",
        "        self.v = time.time()\n",
        "\n",
        "    def t(self):\n",
        "        return time.time() - self.v\n",
        "\n",
        "\n",
        "def time_text(t):\n",
        "    if t >= 3600:\n",
        "        return '{:.1f}h'.format(t / 3600)\n",
        "    elif t >= 60:\n",
        "        return '{:.1f}m'.format(t / 60)\n",
        "    else:\n",
        "        return '{:.1f}s'.format(t)\n",
        "def compute_psnr(im1, im2):\n",
        "    p = psnr(im1, im2)\n",
        "    return p\n",
        "\n",
        "\n",
        "def compute_ssim(im1, im2):\n",
        "    s = ssim(im1, im2, use_gaussian_window=True, sigma=1.5, auto_downsample=True)[0]\n",
        "    return s\n",
        "\n",
        "\n",
        "def shave(im, border):\n",
        "    border = [border, border]\n",
        "    im = im[border[0]:-border[0], border[1]:-border[1], ...]\n",
        "    return im\n",
        "\n",
        "\n",
        "def modcrop(im, modulo):\n",
        "    sz = im.shape\n",
        "    h = np.int32(sz[0] / modulo) * modulo\n",
        "    w = np.int32(sz[1] / modulo) * modulo\n",
        "    ims = im[0:h, 0:w, ...]\n",
        "    return ims\n",
        "\n",
        "\n",
        "def get_list(path, ext):\n",
        "    return [os.path.join(path, f) for f in os.listdir(path) if f.endswith(ext)]\n",
        "\n",
        "\n",
        "def convert_shape(img):\n",
        "    img = np.transpose((img * 255.0).round(), (1, 2, 0))\n",
        "    img = np.uint8(np.clip(img, 0, 255))\n",
        "    return img\n",
        "\n",
        "\n",
        "def quantize(img):\n",
        "    return img.clip(0, 255).round().astype(np.uint8)\n",
        "\n",
        "\n",
        "def tensor2np(tensor, out_type=np.uint8, min_max=(0, 1)):\n",
        "    tensor = tensor.float().cpu().clamp_(*min_max)\n",
        "    tensor = (tensor - min_max[0]) / (min_max[1] - min_max[0])  # to range [0, 1]\n",
        "    img_np = tensor.numpy()\n",
        "    img_np = np.transpose(img_np, (1, 2, 0))\n",
        "    if out_type == np.uint8:\n",
        "        img_np = (img_np * 255.0).round()\n",
        "\n",
        "    return img_np.astype(out_type)\n",
        "\n",
        "def convert2np(tensor):\n",
        "    return tensor.cpu().mul(255).clamp(0, 255).byte().squeeze().permute(1, 2, 0).numpy()\n",
        "\n",
        "\n",
        "def adjust_learning_rate(optimizer, epoch, step_size, lr_init, gamma):\n",
        "    factor = epoch // step_size\n",
        "    lr = lr_init * (gamma ** factor)\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "def load_state_dict(path):\n",
        "\n",
        "    state_dict = torch.load(path)\n",
        "    new_state_dcit = OrderedDict()\n",
        "    for k, v in state_dict.items():\n",
        "        if 'module' in k:\n",
        "            name = k[7:]\n",
        "        else:\n",
        "            name = k\n",
        "        new_state_dcit[name] = v\n",
        "    return new_state_dcit"
      ],
      "metadata": {
        "id": "Nv-CjUC0e1WZ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import torch\n",
        "import os\n",
        "import numpy as np\n",
        "import skimage.color as sc\n",
        "import cv2\n",
        "\n",
        "# Testing settings\n",
        "\n",
        "parser = argparse.ArgumentParser(description='ESRGAN')\n",
        "parser.add_argument(\"--test_hr_folder\", type=str, default='./drive/MyDrive/datasets/Urban100/', help='the folder of the target images')\n",
        "parser.add_argument(\"--test_lr_folder\", type=str, default='./drive/MyDrive/datasets/Urban100/LRbicx4/', help='the folder of the input images')\n",
        "parser.add_argument(\"--output_folder\", type=str, default='./drive/MyDrive/ColabNotebooks/ESRGAN-pytorch/results/Urban100/x4')\n",
        "parser.add_argument(\"--checkpoint\", type=str, default='./drive/MyDrive/ColabNotebooks/ESRGAN-pytorch/saved_models/generator_199.pth', help='checkpoint folder to use')\n",
        "parser.add_argument('--cuda', action='store_true', default=True, help='use cuda')\n",
        "parser.add_argument(\"--upscale_factor\", type=int, default=4, help='upscaling factor')\n",
        "parser.add_argument(\"--is_y\", action='store_true', default=True, help='evaluate on y channel, if False evaluate on RGB channels')\n",
        "opt = parser.parse_args(args=[])\n",
        "\n",
        "#print(opt)\n",
        "\n",
        "def forward_chop(model, x, shave=10, min_size=60000):\n",
        "  \n",
        "  scale = 4 #self.scale[self.idx_scale]\n",
        "  n_GPUs = 1 #min(self.n_GPUs, 4)\n",
        "  b, c, h, w = x.size()\n",
        "  h_half, w_half = h // 2, w // 2\n",
        "  h_size, w_size = h_half + shave, w_half + shave\n",
        "  lr_list = [\n",
        "      x[:, :, 0:h_size, 0:w_size],\n",
        "      x[:, :, 0:h_size, (w - w_size):w],\n",
        "      x[:, :, (h - h_size):h, 0:w_size],\n",
        "      x[:, :, (h - h_size):h, (w - w_size):w]]\n",
        "\n",
        "  if w_size * h_size < min_size:\n",
        "    sr_list = []\n",
        "    for i in range(0, 4, n_GPUs):\n",
        "\n",
        "      lr_batch = torch.cat(lr_list[i:(i + n_GPUs)], dim=0)\n",
        "      sr_batch = model(lr_batch)\n",
        "      sr_list.extend(sr_batch.chunk(n_GPUs, dim=0))\n",
        "  \n",
        "  else:\n",
        "    sr_list = [\n",
        "        forward_chop(model, patch, shave=shave, min_size=min_size) \\\n",
        "        for patch in lr_list\n",
        "    ]\n",
        "\n",
        "  h, w = scale * h, scale * w\n",
        "  h_half, w_half = scale * h_half, scale * w_half\n",
        "  h_size, w_size = scale * h_size, scale * w_size\n",
        "  shave *= scale\n",
        "\n",
        "  output = x.new(b, c, h, w)\n",
        "  output[:, :, 0:h_half, 0:w_half] \\\n",
        "      = sr_list[0][:, :, 0:h_half, 0:w_half]\n",
        "  output[:, :, 0:h_half, w_half:w] \\\n",
        "      = sr_list[1][:, :, 0:h_half, (w_size - w + w_half):w_size]\n",
        "  output[:, :, h_half:h, 0:w_half] \\\n",
        "      = sr_list[2][:, :, (h_size - h + h_half):h_size, 0:w_half]\n",
        "  output[:, :, h_half:h, w_half:w] \\\n",
        "      = sr_list[3][:, :, (h_size - h + h_half):h_size, (w_size - w + w_half):w_size]\n",
        "\n",
        "  return output\n",
        "\n",
        "cuda = opt.cuda\n",
        "device = torch.device('cuda' if cuda else 'cpu')\n",
        "\n",
        "filepath = opt.test_hr_folder\n",
        "\n",
        "if filepath.split('/')[-2] == 'Set5' or filepath.split('/')[-2] == 'Set14' or filepath.split('/')[-2] == 'BSDS100' or filepath.split('/')[-2] == 'Urban100':\n",
        "  ext = '.png'\n",
        "else:\n",
        "  ext = '.bmp'\n",
        "\n",
        "filelist = get_list(filepath, ext=ext)\n",
        "psnr_list = np.zeros(len(filelist))\n",
        "ssim_list = np.zeros(len(filelist))\n",
        "time_list = np.zeros(len(filelist))\n",
        "\n",
        "model = GeneratorRRDB(channels=3, filters=64, num_res_blocks=23).to(device)\n",
        "model_dict = load_state_dict(opt.checkpoint)\n",
        "model.load_state_dict(model_dict, strict=False)#True)\n",
        "\n",
        "i = 0\n",
        "start = torch.cuda.Event(enable_timing=True)\n",
        "end = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "for imname in filelist:\n",
        "\n",
        "  im_gt = cv2.imread(imname, cv2.IMREAD_COLOR)[:, :, [2, 1, 0]]  # BGR to RGB\n",
        "  im_gt = modcrop(im_gt, opt.upscale_factor)\n",
        "  im_l = cv2.imread(opt.test_lr_folder + imname.split('/')[-1].split('.')[0] + ext, cv2.IMREAD_COLOR)[:, :, [2, 1, 0]]  # BGR to RGB    #'x' + str(opt.upscale_factor) +\n",
        "  \n",
        "  #print(\"im_gt:\\n\", im_gt.shape)\n",
        "  #print(\"im_l:\\n\" , im_l.shape)\n",
        "  \n",
        "  if len(im_gt.shape) < 3:\n",
        "\n",
        "    im_gt = im_gt[..., np.newaxis]\n",
        "    im_gt = np.concatenate([im_gt] * 3, 2)\n",
        "    im_l = im_l[..., np.newaxis]\n",
        "    im_l = np.concatenate([im_l] * 3, 2)\n",
        "\n",
        "    #print(\"len(im_gt.shape) < 3 shape:\\n\", im_gt.shape)\n",
        "    #print(\"len(im_lr.shape) < 3 shape:\\n\" , im_l.shape)\n",
        "\n",
        "\n",
        "  im_input = im_l / 255.0\n",
        "  im_input = np.transpose(im_input, (2, 0, 1))\n",
        "  im_input = im_input[np.newaxis, ...]\n",
        "  im_input = torch.from_numpy(im_input).float()\n",
        "\n",
        "  if cuda:\n",
        "    model = model.to(device)\n",
        "    im_input = im_input.to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    start.record()\n",
        "    out = forward_chop(model, im_input) #model(im_input)\n",
        "    end.record()\n",
        "    torch.cuda.synchronize()\n",
        "    time_list[i] = start.elapsed_time(end)  # milliseconds\n",
        "\n",
        "  out_img = tensor2np(out.detach()[0])\n",
        "  #print(\"im_sr:\\n\" , out_img.shape)\n",
        "\n",
        "  crop_size = opt.upscale_factor\n",
        "  cropped_sr_img = shave(out_img, crop_size)\n",
        "  #print(\"im_sr_crop:\\n\" , cropped_sr_img.shape)\n",
        "  cropped_gt_img = shave(im_gt, crop_size)\n",
        "  #print(\"im_gt_crop:\\n\" , cropped_gt_img.shape)\n",
        "\n",
        "  if opt.is_y is True:\n",
        "    #print(\"if opt.is_y is True\\n\")\n",
        "    im_label = quantize(sc.rgb2ycbcr(cropped_gt_img)[:, :, 0])\n",
        "    im_pre = quantize(sc.rgb2ycbcr(cropped_sr_img)[:, :, 0])\n",
        "    #print(\"y im_pre:\\n\" , im_pre.shape)\n",
        "    #print(\"y im_label:\\n\", im_label.shape)\n",
        "    if(im_label.shape != im_pre.shape):\n",
        "      #print(\"im_label shape:\\n\" , im_label.shape)\n",
        "      im_pre = cv2.resize((im_pre) , (int(im_label.shape[1]) , int(im_label.shape[0])))\n",
        "      #print(\"im_pre resized shape:\\n\" , im_pre.shape)\n",
        "  else:\n",
        "    im_label = cropped_gt_img\n",
        "    im_pre = cropped_sr_img\n",
        "\n",
        "  psnr_list[i] = compute_psnr(im_pre, im_label)\n",
        "  ssim_list[i] = compute_ssim(im_pre, im_label)\n",
        "\n",
        "\n",
        "  output_folder = os.path.join(opt.output_folder, imname.split('/')[-1].split('.')[0] + 'x' + str(opt.upscale_factor) + '.png')\n",
        "\n",
        "  if not os.path.exists(opt.output_folder):\n",
        "    os.makedirs(opt.output_folder)\n",
        "\n",
        "  cv2.imwrite(output_folder, out_img[:, :, [2, 1, 0]])\n",
        "  i += 1\n",
        "\n",
        "print(\"Mean PSNR: {}, SSIM: {}, TIME: {} ms\".format(np.mean(psnr_list), np.mean(ssim_list), np.mean(time_list)))"
      ],
      "metadata": {
        "id": "lC7AqJHMeIw1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}