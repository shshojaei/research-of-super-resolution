{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shshojaei/research-of-super-resolution/blob/main/ESRGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_uXGAAiRgwg",
        "outputId": "30daf933-852c-48dc-f15a-f8c9ea091f0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch==1.8.1\n",
            "  Downloading torch-1.8.1-cp37-cp37m-manylinux1_x86_64.whl (804.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 804.1 MB 2.8 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1) (4.1.1)\n",
            "Installing collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.11.0\n",
            "    Uninstalling torch-1.11.0:\n",
            "      Successfully uninstalled torch-1.11.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.13.1+cu113 requires torch==1.12.1, but you have torch 1.8.1 which is incompatible.\n",
            "torchtext 0.13.1 requires torch==1.12.1, but you have torch 1.8.1 which is incompatible.\n",
            "torchaudio 0.12.1+cu113 requires torch==1.12.1, but you have torch 1.8.1 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.8.1\n"
          ]
        }
      ],
      "source": [
        "!pip install torch==1.8.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rMBYQkfnd1S",
        "outputId": "00622488-538c-480a-8f1e-3127ccfc1a06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import os,shutil\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TGr3Ib3AO-t"
      },
      "source": [
        "## **Prepare datasets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_BpPKKcqkAP",
        "outputId": "a0ab5579-83e3-4681-cf3c-b414d6adc05c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ls: cannot access 'drive/MyDrive': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!ls \"drive/MyDrive\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxZKDd_zASk-",
        "outputId": "c123984d-113c-4d5b-ed62-b51f3873b3a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive\n"
          ]
        }
      ],
      "source": [
        "#first change directory\n",
        "\n",
        "%cd 'drive/MyDrive/'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python ./ColabNotebooks/ESRGAN-pytorch/scripts/prepare_dataset.py --images_dir ./datasets/DIV2K/original/DIV2K_valid_HR --output_dir ./datasets/DIV2K/ESRGAN/valid --image_size 256 --step 256 --num_workers 16"
      ],
      "metadata": {
        "id": "_uptEEWlLZhz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9b94299-2c60-4356-e77e-d3146442effb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prepare split image: 100% 100/100 [00:43<00:00,  2.31image/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOsQ6Nsp5q0h",
        "outputId": "a39dfe4a-21b6-4bf1-b976-2491d26cdd4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prepare split image: 100% 800/800 [09:27<00:00,  1.41image/s]\n"
          ]
        }
      ],
      "source": [
        "!python ./ColabNotebooks/ESRGAN-pytorch/scripts/prepare_dataset.py --images_dir ./datasets/DIV2K/original/DIV2K_train_HR --output_dir ./datasets/DIV2K/ESRGAN/train --image_size 256 --step 256 --num_workers 16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBn32bJRJ-Mh"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ReTs6qQFfy_u"
      },
      "outputs": [],
      "source": [
        "#imgproc.py\n",
        "\n",
        "import math\n",
        "import random\n",
        "from typing import Any\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "from torchvision.transforms import functional as ttf\n",
        "\n",
        "from typing import Tuple\n",
        "\n",
        "__all__ = [\n",
        "    \"image2tensor\", \"tensor2image\",\n",
        "    \"image_resize\",\n",
        "    \"expand_y\", \"rgb2ycbcr\", \"bgr2ycbcr\", \"ycbcr2bgr\", \"ycbcr2rgb\",\n",
        "    \"rgb2ycbcr_torch\", \"bgr2ycbcr_torch\",\n",
        "    \"center_crop\", \"random_crop\", \"random_rotate\", \"random_vertically_flip\", \"random_horizontally_flip\",\n",
        "]\n",
        "\n",
        "\n",
        "def image2tensor(image: np.ndarray, range_norm: bool, half: bool) -> torch.Tensor:\n",
        "    \"\"\"Convert the image data type to the Tensor (NCWH) data type supported by PyTorch\n",
        "    Args:\n",
        "        image (np.ndarray): The image data read by ``OpenCV.imread``, the data range is [0,255] or [0, 1]\n",
        "        range_norm (bool): Scale [0, 1] data to between [-1, 1]\n",
        "        half (bool): Whether to convert torch.float32 similarly to torch.half type\n",
        "    Returns:\n",
        "        tensor (torch.Tensor): Data types supported by PyTorch\n",
        "    Examples:\n",
        "        >>> example_image = cv2.imread(\"lr_image.bmp\")\n",
        "        >>> example_tensor = image2tensor(example_image, range_norm=True, half=False)\n",
        "    \"\"\"\n",
        "    # Convert image data type to Tensor data type\n",
        "    tensor = ttf.to_tensor(image)\n",
        "\n",
        "    # Scale the image data from [0, 1] to [-1, 1]\n",
        "    if range_norm:\n",
        "        tensor = tensor.mul(2.0).sub(1.0)\n",
        "\n",
        "    # Convert torch.float32 image data type to torch.half image data type\n",
        "    if half:\n",
        "        tensor = tensor.half()\n",
        "\n",
        "    return tensor\n",
        "\n",
        "\n",
        "def tensor2image(tensor: torch.Tensor, range_norm: bool, half: bool) -> Any:\n",
        "    \"\"\"Convert the Tensor(NCWH) data type supported by PyTorch to the np.ndarray(WHC) image data type\n",
        "    Args:\n",
        "        tensor (torch.Tensor): Data types supported by PyTorch (NCHW), the data range is [0, 1]\n",
        "        range_norm (bool): Scale [-1, 1] data to between [0, 1]\n",
        "        half (bool): Whether to convert torch.float32 similarly to torch.half type.\n",
        "    Returns:\n",
        "        image (np.ndarray): Data types supported by PIL or OpenCV\n",
        "    Examples:\n",
        "        >>> example_image = cv2.imread(\"lr_image.bmp\")\n",
        "        >>> example_tensor = image2tensor(example_image, range_norm=False, half=False)\n",
        "    \"\"\"\n",
        "    if range_norm:\n",
        "        tensor = tensor.add(1.0).div(2.0)\n",
        "    if half:\n",
        "        tensor = tensor.half()\n",
        "\n",
        "    image = tensor.squeeze(0).permute(1, 2, 0).mul(255).clamp(0, 255).cpu().numpy().astype(\"uint8\")\n",
        "\n",
        "    return image\n",
        "\n",
        "\n",
        "# Code reference `https://github.com/xinntao/BasicSR/blob/master/basicsr/utils/matlab_functions.py`\n",
        "def _cubic(x: Any) -> Any:\n",
        "    \"\"\"Implementation of `cubic` function in Matlab under Python language.\n",
        "    Args:\n",
        "        x: Element vector.\n",
        "    Returns:\n",
        "        Bicubic interpolation\n",
        "    \"\"\"\n",
        "    absx = torch.abs(x)\n",
        "    absx2 = absx ** 2\n",
        "    absx3 = absx ** 3\n",
        "    return (1.5 * absx3 - 2.5 * absx2 + 1) * ((absx <= 1).type_as(absx)) + (\n",
        "            -0.5 * absx3 + 2.5 * absx2 - 4 * absx + 2) * (\n",
        "               ((absx > 1) * (absx <= 2)).type_as(absx))\n",
        "\n",
        "\n",
        "# Code reference `https://github.com/xinntao/BasicSR/blob/master/basicsr/utils/matlab_functions.py`\n",
        "def _calculate_weights_indices(in_length: int,\n",
        "                               out_length: int,\n",
        "                               scale: float,\n",
        "                               kernel_width: int,\n",
        "                               antialiasing: bool) -> Tuple[np.ndarray, np.ndarray, int, int]:\n",
        "    \"\"\"Implementation of `calculate_weights_indices` function in Matlab under Python language.\n",
        "    Args:\n",
        "        in_length (int): Input length.\n",
        "        out_length (int): Output length.\n",
        "        scale (float): Scale factor.\n",
        "        kernel_width (int): Kernel width.\n",
        "        antialiasing (bool): Whether to apply antialiasing when down-sampling operations.\n",
        "            Caution: Bicubic down-sampling in PIL uses antialiasing by default.\n",
        "    Returns:\n",
        "       weights, indices, sym_len_s, sym_len_e\n",
        "    \"\"\"\n",
        "    if (scale < 1) and antialiasing:\n",
        "        # Use a modified kernel (larger kernel width) to simultaneously\n",
        "        # interpolate and antialiasing\n",
        "        kernel_width = kernel_width / scale\n",
        "\n",
        "    # Output-space coordinates\n",
        "    x = torch.linspace(1, out_length, out_length)\n",
        "\n",
        "    # Input-space coordinates. Calculate the inverse mapping such that 0.5\n",
        "    # in output space maps to 0.5 in input space, and 0.5 + scale in output\n",
        "    # space maps to 1.5 in input space.\n",
        "    u = x / scale + 0.5 * (1 - 1 / scale)\n",
        "\n",
        "    # What is the left-most pixel that can be involved in the computation?\n",
        "    left = torch.floor(u - kernel_width / 2)\n",
        "\n",
        "    # What is the maximum number of pixels that can be involved in the\n",
        "    # computation?  Note: it's OK to use an extra pixel here; if the\n",
        "    # corresponding weights are all zero, it will be eliminated at the end\n",
        "    # of this function.\n",
        "    p = math.ceil(kernel_width) + 2\n",
        "\n",
        "    # The indices of the input pixels involved in computing the k-th output\n",
        "    # pixel are in row k of the indices matrix.\n",
        "    indices = left.view(out_length, 1).expand(out_length, p) + torch.linspace(0, p - 1, p).view(1, p).expand(\n",
        "        out_length, p)\n",
        "\n",
        "    # The weights used to compute the k-th output pixel are in row k of the\n",
        "    # weights matrix.\n",
        "    distance_to_center = u.view(out_length, 1).expand(out_length, p) - indices\n",
        "\n",
        "    # apply cubic kernel\n",
        "    if (scale < 1) and antialiasing:\n",
        "        weights = scale * _cubic(distance_to_center * scale)\n",
        "    else:\n",
        "        weights = _cubic(distance_to_center)\n",
        "\n",
        "    # Normalize the weights matrix so that each row sums to 1.\n",
        "    weights_sum = torch.sum(weights, 1).view(out_length, 1)\n",
        "    weights = weights / weights_sum.expand(out_length, p)\n",
        "\n",
        "    # If a column in weights is all zero, get rid of it. only consider the\n",
        "    # first and last column.\n",
        "    weights_zero_tmp = torch.sum((weights == 0), 0)\n",
        "    if not math.isclose(weights_zero_tmp[0], 0, rel_tol=1e-6):\n",
        "        indices = indices.narrow(1, 1, p - 2)\n",
        "        weights = weights.narrow(1, 1, p - 2)\n",
        "    if not math.isclose(weights_zero_tmp[-1], 0, rel_tol=1e-6):\n",
        "        indices = indices.narrow(1, 0, p - 2)\n",
        "        weights = weights.narrow(1, 0, p - 2)\n",
        "    weights = weights.contiguous()\n",
        "    indices = indices.contiguous()\n",
        "    sym_len_s = -indices.min() + 1\n",
        "    sym_len_e = indices.max() - in_length\n",
        "    indices = indices + sym_len_s - 1\n",
        "    return weights, indices, int(sym_len_s), int(sym_len_e)\n",
        "\n",
        "\n",
        "# Code reference `https://github.com/xinntao/BasicSR/blob/master/basicsr/utils/matlab_functions.py`\n",
        "def image_resize(image: Any, scale_factor: float, antialiasing: bool = True) -> Any:\n",
        "    \"\"\"Implementation of `imresize` function in Matlab under Python language.\n",
        "    Args:\n",
        "        image: The input image.\n",
        "        scale_factor (float): Scale factor. The same scale applies for both height and width.\n",
        "        antialiasing (bool): Whether to apply antialiasing when down-sampling operations.\n",
        "            Caution: Bicubic down-sampling in `PIL` uses antialiasing by default. Default: ``True``.\n",
        "    Returns:\n",
        "        out_2 (np.ndarray): Output image with shape (c, h, w), [0, 1] range, w/o round\n",
        "    \"\"\"\n",
        "    squeeze_flag = False\n",
        "    if type(image).__module__ == np.__name__:  # numpy type\n",
        "        numpy_type = True\n",
        "        if image.ndim == 2:\n",
        "            image = image[:, :, None]\n",
        "            squeeze_flag = True\n",
        "        image = torch.from_numpy(image.transpose(2, 0, 1)).float()\n",
        "    else:\n",
        "        numpy_type = False\n",
        "        if image.ndim == 2:\n",
        "            image = image.unsqueeze(0)\n",
        "            squeeze_flag = True\n",
        "\n",
        "    in_c, in_h, in_w = image.size()\n",
        "    out_h, out_w = math.ceil(in_h * scale_factor), math.ceil(in_w * scale_factor)\n",
        "    kernel_width = 4\n",
        "\n",
        "    # get weights and indices\n",
        "    weights_h, indices_h, sym_len_hs, sym_len_he = _calculate_weights_indices(in_h, out_h, scale_factor, kernel_width,\n",
        "                                                                              antialiasing)\n",
        "    weights_w, indices_w, sym_len_ws, sym_len_we = _calculate_weights_indices(in_w, out_w, scale_factor, kernel_width,\n",
        "                                                                              antialiasing)\n",
        "    # process H dimension\n",
        "    # symmetric copying\n",
        "    img_aug = torch.FloatTensor(in_c, in_h + sym_len_hs + sym_len_he, in_w)\n",
        "    img_aug.narrow(1, sym_len_hs, in_h).copy_(image)\n",
        "\n",
        "    sym_patch = image[:, :sym_len_hs, :]\n",
        "    inv_idx = torch.arange(sym_patch.size(1) - 1, -1, -1).long()\n",
        "    sym_patch_inv = sym_patch.index_select(1, inv_idx)\n",
        "    img_aug.narrow(1, 0, sym_len_hs).copy_(sym_patch_inv)\n",
        "\n",
        "    sym_patch = image[:, -sym_len_he:, :]\n",
        "    inv_idx = torch.arange(sym_patch.size(1) - 1, -1, -1).long()\n",
        "    sym_patch_inv = sym_patch.index_select(1, inv_idx)\n",
        "    img_aug.narrow(1, sym_len_hs + in_h, sym_len_he).copy_(sym_patch_inv)\n",
        "\n",
        "    out_1 = torch.FloatTensor(in_c, out_h, in_w)\n",
        "    kernel_width = weights_h.size(1)\n",
        "    for i in range(out_h):\n",
        "        idx = int(indices_h[i][0])\n",
        "        for j in range(in_c):\n",
        "            out_1[j, i, :] = img_aug[j, idx:idx + kernel_width, :].transpose(0, 1).mv(weights_h[i])\n",
        "\n",
        "    # process W dimension\n",
        "    # symmetric copying\n",
        "    out_1_aug = torch.FloatTensor(in_c, out_h, in_w + sym_len_ws + sym_len_we)\n",
        "    out_1_aug.narrow(2, sym_len_ws, in_w).copy_(out_1)\n",
        "\n",
        "    sym_patch = out_1[:, :, :sym_len_ws]\n",
        "    inv_idx = torch.arange(sym_patch.size(2) - 1, -1, -1).long()\n",
        "    sym_patch_inv = sym_patch.index_select(2, inv_idx)\n",
        "    out_1_aug.narrow(2, 0, sym_len_ws).copy_(sym_patch_inv)\n",
        "\n",
        "    sym_patch = out_1[:, :, -sym_len_we:]\n",
        "    inv_idx = torch.arange(sym_patch.size(2) - 1, -1, -1).long()\n",
        "    sym_patch_inv = sym_patch.index_select(2, inv_idx)\n",
        "    out_1_aug.narrow(2, sym_len_ws + in_w, sym_len_we).copy_(sym_patch_inv)\n",
        "\n",
        "    out_2 = torch.FloatTensor(in_c, out_h, out_w)\n",
        "    kernel_width = weights_w.size(1)\n",
        "    for i in range(out_w):\n",
        "        idx = int(indices_w[i][0])\n",
        "        for j in range(in_c):\n",
        "            out_2[j, :, i] = out_1_aug[j, :, idx:idx + kernel_width].mv(weights_w[i])\n",
        "\n",
        "    if squeeze_flag:\n",
        "        out_2 = out_2.squeeze(0)\n",
        "    if numpy_type:\n",
        "        out_2 = out_2.numpy()\n",
        "        if not squeeze_flag:\n",
        "            out_2 = out_2.transpose(1, 2, 0)\n",
        "\n",
        "    return out_2\n",
        "\n",
        "\n",
        "def expand_y(image: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Convert BGR channel to YCbCr format,\n",
        "    and expand Y channel data in YCbCr, from HW to HWC\n",
        "    Args:\n",
        "        image (np.ndarray): Y channel image data\n",
        "    Returns:\n",
        "        y_image (np.ndarray): Y-channel image data in HWC form\n",
        "    \"\"\"\n",
        "    # Normalize image data to [0, 1]\n",
        "    image = image.astype(np.float32) / 255.\n",
        "\n",
        "    # Convert BGR to YCbCr, and extract only Y channel\n",
        "    y_image = bgr2ycbcr(image, only_use_y_channel=True)\n",
        "\n",
        "    # Expand Y channel\n",
        "    y_image = y_image[..., None]\n",
        "\n",
        "    # Normalize the image data to [0, 255]\n",
        "    y_image = y_image.astype(np.float64) * 255.0\n",
        "\n",
        "    return y_image\n",
        "\n",
        "\n",
        "# Code reference `https://github.com/xinntao/BasicSR/blob/master/basicsr/utils/matlab_functions.py`\n",
        "def rgb2ycbcr(image: np.ndarray, only_use_y_channel: bool) -> np.ndarray:\n",
        "    \"\"\"Implementation of rgb2ycbcr function in Matlab under Python language\n",
        "    Args:\n",
        "        image (np.ndarray): Image input in RGB format.\n",
        "        only_use_y_channel (bool): Extract Y channel separately\n",
        "    Returns:\n",
        "        image (np.ndarray): YCbCr image array data\n",
        "    \"\"\"\n",
        "    if only_use_y_channel:\n",
        "        image = np.dot(image, [65.481, 128.553, 24.966]) + 16.0\n",
        "    else:\n",
        "        image = np.matmul(image, [[65.481, -37.797, 112.0], [128.553, -74.203, -93.786], [24.966, 112.0, -18.214]]) + [\n",
        "            16, 128, 128]\n",
        "\n",
        "    image /= 255.\n",
        "    image = image.astype(np.float32)\n",
        "\n",
        "    return image\n",
        "\n",
        "\n",
        "# Code reference `https://github.com/xinntao/BasicSR/blob/master/basicsr/utils/matlab_functions.py`\n",
        "def bgr2ycbcr(image: np.ndarray, only_use_y_channel: bool) -> np.ndarray:\n",
        "    \"\"\"Implementation of bgr2ycbcr function in Matlab under Python language.\n",
        "    Args:\n",
        "        image (np.ndarray): Image input in BGR format\n",
        "        only_use_y_channel (bool): Extract Y channel separately\n",
        "    Returns:\n",
        "        image (np.ndarray): YCbCr image array data\n",
        "    \"\"\"\n",
        "    if only_use_y_channel:\n",
        "        image = np.dot(image, [24.966, 128.553, 65.481]) + 16.0\n",
        "    else:\n",
        "        image = np.matmul(image, [[24.966, 112.0, -18.214], [128.553, -74.203, -93.786], [65.481, -37.797, 112.0]]) + [\n",
        "            16, 128, 128]\n",
        "\n",
        "    image /= 255.\n",
        "    image = image.astype(np.float32)\n",
        "\n",
        "    return image\n",
        "\n",
        "\n",
        "# Code reference `https://github.com/xinntao/BasicSR/blob/master/basicsr/utils/matlab_functions.py`\n",
        "def ycbcr2rgb(image: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Implementation of ycbcr2rgb function in Matlab under Python language.\n",
        "    Args:\n",
        "        image (np.ndarray): Image input in YCbCr format.\n",
        "    Returns:\n",
        "        image (np.ndarray): RGB image array data\n",
        "    \"\"\"\n",
        "    image_dtype = image.dtype\n",
        "    image *= 255.\n",
        "\n",
        "    image = np.matmul(image, [[0.00456621, 0.00456621, 0.00456621],\n",
        "                              [0, -0.00153632, 0.00791071],\n",
        "                              [0.00625893, -0.00318811, 0]]) * 255.0 + [-222.921, 135.576, -276.836]\n",
        "\n",
        "    image /= 255.\n",
        "    image = image.astype(image_dtype)\n",
        "\n",
        "    return image\n",
        "\n",
        "\n",
        "# Code reference `https://github.com/xinntao/BasicSR/blob/master/basicsr/utils/matlab_functions.py`\n",
        "def ycbcr2bgr(image: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Implementation of ycbcr2bgr function in Matlab under Python language.\n",
        "    Args:\n",
        "        image (np.ndarray): Image input in YCbCr format.\n",
        "    Returns:\n",
        "        image (np.ndarray): BGR image array data\n",
        "    \"\"\"\n",
        "    image_dtype = image.dtype\n",
        "    image *= 255.\n",
        "\n",
        "    image = np.matmul(image, [[0.00456621, 0.00456621, 0.00456621],\n",
        "                              [0.00791071, -0.00153632, 0],\n",
        "                              [0, -0.00318811, 0.00625893]]) * 255.0 + [-276.836, 135.576, -222.921]\n",
        "\n",
        "    image /= 255.\n",
        "    image = image.astype(image_dtype)\n",
        "\n",
        "    return image\n",
        "\n",
        "\n",
        "def rgb2ycbcr_torch(tensor: torch.Tensor, only_use_y_channel: bool) -> torch.Tensor:\n",
        "    \"\"\"Implementation of rgb2ycbcr function in Matlab under PyTorch\n",
        "    References from：`https://en.wikipedia.org/wiki/YCbCr#ITU-R_BT.601_conversion`\n",
        "    Args:\n",
        "        tensor (torch.Tensor): Image data in PyTorch format\n",
        "        only_use_y_channel (bool): Extract only Y channel\n",
        "    Returns:\n",
        "        tensor (torch.Tensor): YCbCr image data in PyTorch format\n",
        "    \"\"\"\n",
        "    if only_use_y_channel:\n",
        "        weight = torch.Tensor([[65.481], [128.553], [24.966]]).to(tensor)\n",
        "        tensor = torch.matmul(tensor.permute(0, 2, 3, 1), weight).permute(0, 3, 1, 2) + 16.0\n",
        "    else:\n",
        "        weight = torch.Tensor([[65.481, -37.797, 112.0],\n",
        "                               [128.553, -74.203, -93.786],\n",
        "                               [24.966, 112.0, -18.214]]).to(tensor)\n",
        "        bias = torch.Tensor([16, 128, 128]).view(1, 3, 1, 1).to(tensor)\n",
        "        tensor = torch.matmul(tensor.permute(0, 2, 3, 1), weight).permute(0, 3, 1, 2) + bias\n",
        "\n",
        "    tensor /= 255.\n",
        "\n",
        "    return tensor\n",
        "\n",
        "\n",
        "def bgr2ycbcr_torch(tensor: torch.Tensor, only_use_y_channel: bool) -> torch.Tensor:\n",
        "    \"\"\"Implementation of bgr2ycbcr function in Matlab under PyTorch\n",
        "    References from：`https://en.wikipedia.org/wiki/YCbCr#ITU-R_BT.601_conversion`\n",
        "    Args:\n",
        "        tensor (torch.Tensor): Image data in PyTorch format\n",
        "        only_use_y_channel (bool): Extract only Y channel\n",
        "    Returns:\n",
        "        tensor (torch.Tensor): YCbCr image data in PyTorch format\n",
        "    \"\"\"\n",
        "    if only_use_y_channel:\n",
        "        weight = torch.Tensor([[24.966], [128.553], [65.481]]).to(tensor)\n",
        "        tensor = torch.matmul(tensor.permute(0, 2, 3, 1), weight).permute(0, 3, 1, 2) + 16.0\n",
        "    else:\n",
        "        weight = torch.Tensor([[24.966, 112.0, -18.214],\n",
        "                               [128.553, -74.203, -93.786],\n",
        "                               [65.481, -37.797, 112.0]]).to(tensor)\n",
        "        bias = torch.Tensor([16, 128, 128]).view(1, 3, 1, 1).to(tensor)\n",
        "        tensor = torch.matmul(tensor.permute(0, 2, 3, 1), weight).permute(0, 3, 1, 2) + bias\n",
        "\n",
        "    tensor /= 255.\n",
        "\n",
        "    return tensor\n",
        "\n",
        "\n",
        "def center_crop(image: np.ndarray, image_size: int) -> np.ndarray:\n",
        "    \"\"\"Crop small image patches from one image center area.\n",
        "    Args:\n",
        "        image (np.ndarray): The input image for `OpenCV.imread`.\n",
        "        image_size (int): The size of the captured image area.\n",
        "    Returns:\n",
        "        patch_image (np.ndarray): Small patch image\n",
        "    \"\"\"\n",
        "    image_height, image_width = image.shape[:2]\n",
        "\n",
        "    # Just need to find the top and left coordinates of the image\n",
        "    top = (image_height - image_size) // 2\n",
        "    left = (image_width - image_size) // 2\n",
        "\n",
        "    # Crop image patch\n",
        "    patch_image = image[top:top + image_size, left:left + image_size, ...]\n",
        "\n",
        "    return patch_image\n",
        "\n",
        "\n",
        "def random_crop(image: np.ndarray, image_size: int) -> np.ndarray:\n",
        "    \"\"\"Crop small image patches from one image.\n",
        "    Args:\n",
        "        image (np.ndarray): The input image for `OpenCV.imread`.\n",
        "        image_size (int): The size of the captured image area.\n",
        "    Returns:\n",
        "        patch_image (np.ndarray): Small patch image\n",
        "    \"\"\"\n",
        "    image_height, image_width = image.shape[:2]\n",
        "\n",
        "    # Just need to find the top and left coordinates of the image\n",
        "    top = random.randint(0, image_height - image_size)\n",
        "    left = random.randint(0, image_width - image_size)\n",
        "\n",
        "    # Crop image patch\n",
        "    patch_image = image[top:top + image_size, left:left + image_size, ...]\n",
        "\n",
        "    return patch_image\n",
        "\n",
        "\n",
        "def random_rotate(image,\n",
        "                  angles: list,\n",
        "                  center: tuple = None,\n",
        "                  scale_factor: float = 1.0) -> np.ndarray:\n",
        "    \"\"\"Rotate an image by a random angle\n",
        "    Args:\n",
        "        image (np.ndarray): Image read with OpenCV\n",
        "        angles (list): Rotation angle range\n",
        "        center (optional, tuple): High resolution image selection center point. Default: ``None``\n",
        "        scale_factor (optional, float): scaling factor. Default: 1.0\n",
        "    Returns:\n",
        "        rotated_image (np.ndarray): image after rotation\n",
        "    \"\"\"\n",
        "    image_height, image_width = image.shape[:2]\n",
        "\n",
        "    if center is None:\n",
        "        center = (image_width // 2, image_height // 2)\n",
        "\n",
        "    # Random select specific angle\n",
        "    angle = random.choice(angles)\n",
        "    matrix = cv2.getRotationMatrix2D(center, angle, scale_factor)\n",
        "    rotated_image = cv2.warpAffine(image, matrix, (image_width, image_height))\n",
        "\n",
        "    return rotated_image\n",
        "\n",
        "\n",
        "def random_horizontally_flip(image: np.ndarray, p: float = 0.5) -> np.ndarray:\n",
        "    \"\"\"Flip the image upside down randomly\n",
        "    Args:\n",
        "        image (np.ndarray): Image read with OpenCV\n",
        "        p (optional, float): Horizontally flip probability. Default: 0.5\n",
        "    Returns:\n",
        "        horizontally_flip_image (np.ndarray): image after horizontally flip\n",
        "    \"\"\"\n",
        "    if random.random() < p:\n",
        "        horizontally_flip_image = cv2.flip(image, 1)\n",
        "    else:\n",
        "        horizontally_flip_image = image\n",
        "\n",
        "    return horizontally_flip_image\n",
        "\n",
        "\n",
        "def random_vertically_flip(image: np.ndarray, p: float = 0.5) -> np.ndarray:\n",
        "    \"\"\"Flip an image horizontally randomly\n",
        "    Args:\n",
        "        image (np.ndarray): Image read with OpenCV\n",
        "        p (optional, float): Vertically flip probability. Default: 0.5\n",
        "    Returns:\n",
        "        vertically_flip_image (np.ndarray): image after vertically flip\n",
        "    \"\"\"\n",
        "    if random.random() < p:\n",
        "        vertically_flip_image = cv2.flip(image, 0)\n",
        "    else:\n",
        "        vertically_flip_image = image\n",
        "\n",
        "    return vertically_flip_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "OTBYX6b-JLgi"
      },
      "outputs": [],
      "source": [
        "#dataset.py\n",
        "\n",
        "\"\"\"Realize the function of dataset preparation.\"\"\"\n",
        "import os\n",
        "import queue\n",
        "import threading\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "__all__ = [\n",
        "    \"TrainValidImageDataset\", \"TestImageDataset\",\n",
        "    \"PrefetchGenerator\", \"PrefetchDataLoader\", \"CPUPrefetcher\", \"CUDAPrefetcher\",\n",
        "]\n",
        "\n",
        "\n",
        "class TrainValidImageDataset(Dataset):\n",
        "    \"\"\"Define training/valid dataset loading methods.\n",
        "    Args:\n",
        "        image_dir (str): Train/Valid dataset address.\n",
        "        image_size (int): High resolution image size.\n",
        "        upscale_factor (int): Image up scale factor.\n",
        "        mode (str): Data set loading method, the training data set is for data enhancement, and the\n",
        "            verification dataset is not for data enhancement.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, image_dir: str, image_size: int, upscale_factor: int, mode: str) -> None:\n",
        "        super(TrainValidImageDataset, self).__init__()\n",
        "        # Get all image file names in folder\n",
        "        self.image_file_names = [os.path.join(image_dir, image_file_name) for image_file_name in os.listdir(image_dir)]\n",
        "        # Specify the high-resolution image size, with equal length and width\n",
        "        self.image_size = image_size\n",
        "        # How many times the high-resolution image is the low-resolution image\n",
        "        self.upscale_factor = upscale_factor\n",
        "        # Load training dataset or test dataset\n",
        "        self.mode = mode\n",
        "\n",
        "    def __getitem__(self, batch_index: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        # Read a batch of image data\n",
        "        image = cv2.imread(self.image_file_names[batch_index], cv2.IMREAD_UNCHANGED).astype(np.float32) / 255.\n",
        "\n",
        "        # Image processing operations\n",
        "        if self.mode == \"Train\":\n",
        "            hr_image = random_crop(image, self.image_size)\n",
        "            hr_image = random_rotate(hr_image, angles=[0, 90, 180, 270])\n",
        "            hr_image = random_horizontally_flip(hr_image, p=0.5)\n",
        "            hr_image = random_vertically_flip(hr_image, p=0.5)\n",
        "        elif self.mode == \"Valid\":\n",
        "            hr_image = center_crop(image, self.image_size)\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported data processing model, please use `Train` or `Valid`.\")\n",
        "\n",
        "        lr_image = image_resize(hr_image, 1 / self.upscale_factor)\n",
        "\n",
        "        # BGR convert to RGB\n",
        "        lr_image = cv2.cvtColor(lr_image, cv2.COLOR_BGR2RGB)\n",
        "        hr_image = cv2.cvtColor(hr_image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Convert image data into Tensor stream format (PyTorch).\n",
        "        # Note: The range of input and output is between [0, 1]\n",
        "        lr_tensor = image2tensor(lr_image, range_norm=False, half=False)\n",
        "        hr_tensor = image2tensor(hr_image, range_norm=False, half=False)\n",
        "\n",
        "        return {\"lr\": lr_tensor, \"hr\": hr_tensor}\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.image_file_names)\n",
        "\n",
        "\n",
        "class TestImageDataset(Dataset):\n",
        "    \"\"\"Define Test dataset loading methods.\n",
        "    Args:\n",
        "        test_lr_image_dir (str): Test dataset address for low resolution image dir.\n",
        "        test_hr_image_dir (str): Test dataset address for high resolution image dir.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, test_lr_image_dir: str, test_hr_image_dir: str) -> None:\n",
        "        super(TestImageDataset, self).__init__()\n",
        "        # Get all image file names in folder\n",
        "        self.lr_image_file_names = [os.path.join(test_lr_image_dir, x) for x in os.listdir(test_lr_image_dir)]\n",
        "        self.hr_image_file_names = [os.path.join(test_hr_image_dir, x) for x in os.listdir(test_hr_image_dir)]\n",
        "\n",
        "    def __getitem__(self, batch_index: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        # Read a batch of image data\n",
        "        lr_image = cv2.imread(self.lr_image_file_names[batch_index], cv2.IMREAD_UNCHANGED).astype(np.float32) / 255.\n",
        "        hr_image = cv2.imread(self.hr_image_file_names[batch_index], cv2.IMREAD_UNCHANGED).astype(np.float32) / 255.\n",
        "\n",
        "        # BGR convert to RGB\n",
        "        lr_image = cv2.cvtColor(lr_image, cv2.COLOR_BGR2RGB)\n",
        "        hr_image = cv2.cvtColor(hr_image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Convert image data into Tensor stream format (PyTorch).\n",
        "        # Note: The range of input and output is between [0, 1]\n",
        "        lr_tensor = image2tensor(lr_image, range_norm=False, half=False)\n",
        "        hr_tensor = image2tensor(hr_image, range_norm=False, half=False)\n",
        "\n",
        "        return {\"lr\": lr_tensor, \"hr\": hr_tensor}\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.lr_image_file_names)\n",
        "\n",
        "\n",
        "class PrefetchGenerator(threading.Thread):\n",
        "    \"\"\"A fast data prefetch generator.\n",
        "    Args:\n",
        "        generator: Data generator.\n",
        "        num_data_prefetch_queue (int): How many early data load queues.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, generator, num_data_prefetch_queue: int) -> None:\n",
        "        threading.Thread.__init__(self)\n",
        "        self.queue = queue.Queue(num_data_prefetch_queue)\n",
        "        self.generator = generator\n",
        "        self.daemon = True\n",
        "        self.start()\n",
        "\n",
        "    def run(self) -> None:\n",
        "        for item in self.generator:\n",
        "            self.queue.put(item)\n",
        "        self.queue.put(None)\n",
        "\n",
        "    def __next__(self):\n",
        "        next_item = self.queue.get()\n",
        "        if next_item is None:\n",
        "            raise StopIteration\n",
        "        return next_item\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "\n",
        "class PrefetchDataLoader(DataLoader):\n",
        "    \"\"\"A fast data prefetch dataloader.\n",
        "    Args:\n",
        "        num_data_prefetch_queue (int): How many early data load queues.\n",
        "        kwargs (dict): Other extended parameters.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_data_prefetch_queue: int, **kwargs) -> None:\n",
        "        self.num_data_prefetch_queue = num_data_prefetch_queue\n",
        "        super(PrefetchDataLoader, self).__init__(**kwargs)\n",
        "\n",
        "    def __iter__(self):\n",
        "        return PrefetchGenerator(super().__iter__(), self.num_data_prefetch_queue)\n",
        "\n",
        "\n",
        "class CPUPrefetcher:\n",
        "    \"\"\"Use the CPU side to accelerate data reading.\n",
        "    Args:\n",
        "        dataloader (DataLoader): Data loader. Combines a dataset and a sampler, and provides an iterable over the given dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataloader: DataLoader) -> None:\n",
        "        self.original_dataloader = dataloader\n",
        "        self.data = iter(dataloader)\n",
        "\n",
        "    def next(self):\n",
        "        try:\n",
        "            return next(self.data)\n",
        "        except StopIteration:\n",
        "            return None\n",
        "\n",
        "    def reset(self):\n",
        "        self.data = iter(self.original_dataloader)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.original_dataloader)\n",
        "\n",
        "\n",
        "class CUDAPrefetcher:\n",
        "    \"\"\"Use the CUDA side to accelerate data reading.\n",
        "    Args:\n",
        "        dataloader (DataLoader): Data loader. Combines a dataset and a sampler, and provides an iterable over the given dataset.\n",
        "        device (torch.device): Specify running device.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataloader: DataLoader, device: torch.device):\n",
        "        self.batch_data = None\n",
        "        self.original_dataloader = dataloader\n",
        "        self.device = device\n",
        "\n",
        "        self.data = iter(dataloader)\n",
        "        self.stream = torch.cuda.Stream()\n",
        "        self.preload()\n",
        "\n",
        "    def preload(self):\n",
        "        try:\n",
        "            self.batch_data = next(self.data)\n",
        "        except StopIteration:\n",
        "            self.batch_data = None\n",
        "            return None\n",
        "\n",
        "        with torch.cuda.stream(self.stream):\n",
        "            for k, v in self.batch_data.items():\n",
        "                if torch.is_tensor(v):\n",
        "                    self.batch_data[k] = self.batch_data[k].to(self.device, non_blocking=True)\n",
        "\n",
        "    def next(self):\n",
        "        torch.cuda.current_stream().wait_stream(self.stream)\n",
        "        batch_data = self.batch_data\n",
        "        self.preload()\n",
        "        return batch_data\n",
        "\n",
        "    def reset(self):\n",
        "        self.data = iter(self.original_dataloader)\n",
        "        self.preload()\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.original_dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34CxZ5M3g2AX"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "4aHFtZJcg36I"
      },
      "outputs": [],
      "source": [
        "#model.py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "from torchvision.models.feature_extraction import create_feature_extractor\n",
        "from torchvision import transforms\n",
        "\n",
        "__all__ = [\n",
        "    \"ResidualDenseBlock\", \"ResidualResidualDenseBlock\",\n",
        "    \"Discriminator\", \"Generator\",\n",
        "    \"ContentLoss\"\n",
        "]\n",
        "\n",
        "\n",
        "class ResidualDenseBlock(nn.Module):\n",
        "    \"\"\"Achieves densely connected convolutional layers.\n",
        "    `Densely Connected Convolutional Networks <https://arxiv.org/pdf/1608.06993v5.pdf>` paper.\n",
        "    Args:\n",
        "        channels (int): The number of channels in the input image.\n",
        "        growth_channels (int): The number of channels that increase in each layer of convolution.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, channels: int, growth_channels: int) -> None:\n",
        "        super(ResidualDenseBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(channels + growth_channels * 0, growth_channels, (3, 3), (1, 1), (1, 1))\n",
        "        self.conv2 = nn.Conv2d(channels + growth_channels * 1, growth_channels, (3, 3), (1, 1), (1, 1))\n",
        "        self.conv3 = nn.Conv2d(channels + growth_channels * 2, growth_channels, (3, 3), (1, 1), (1, 1))\n",
        "        self.conv4 = nn.Conv2d(channels + growth_channels * 3, growth_channels, (3, 3), (1, 1), (1, 1))\n",
        "        self.conv5 = nn.Conv2d(channels + growth_channels * 4, channels, (3, 3), (1, 1), (1, 1))\n",
        "\n",
        "        self.leaky_relu = nn.LeakyReLU(0.2, True)\n",
        "        self.identity = nn.Identity()\n",
        "\n",
        "        # Initialize model weights\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        identity = x\n",
        "\n",
        "        out1 = self.leaky_relu(self.conv1(x))\n",
        "        out2 = self.leaky_relu(self.conv2(torch.cat([x, out1], 1)))\n",
        "        out3 = self.leaky_relu(self.conv3(torch.cat([x, out1, out2], 1)))\n",
        "        out4 = self.leaky_relu(self.conv4(torch.cat([x, out1, out2, out3], 1)))\n",
        "        out5 = self.identity(self.conv5(torch.cat([x, out1, out2, out3, out4], 1)))\n",
        "        out = torch.mul(out5, 0.2)\n",
        "        out = torch.add(out, identity)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def _initialize_weights(self) -> None:\n",
        "        for module in self.modules():\n",
        "            if isinstance(module, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(module.weight)\n",
        "                module.weight.data *= 0.1\n",
        "                if module.bias is not None:\n",
        "                    nn.init.constant_(module.bias, 0)\n",
        "\n",
        "\n",
        "class ResidualResidualDenseBlock(nn.Module):\n",
        "    \"\"\"Multi-layer residual dense convolution block.\n",
        "    Args:\n",
        "        channels (int): The number of channels in the input image.\n",
        "        growth_channels (int): The number of channels that increase in each layer of convolution.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, channels: int, growth_channels: int) -> None:\n",
        "        super(ResidualResidualDenseBlock, self).__init__()\n",
        "        self.rdb1 = ResidualDenseBlock(channels, growth_channels)\n",
        "        self.rdb2 = ResidualDenseBlock(channels, growth_channels)\n",
        "        self.rdb3 = ResidualDenseBlock(channels, growth_channels)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        identity = x\n",
        "\n",
        "        out = self.rdb1(x)\n",
        "        out = self.rdb2(out)\n",
        "        out = self.rdb3(out)\n",
        "        out = torch.mul(out, 0.2)\n",
        "        out = torch.add(out, identity)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            # input size. (3) x 128 x 128\n",
        "            nn.Conv2d(3, 64, (3, 3), (1, 1), (1, 1), bias=True),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            # state size. (64) x 64 x 64\n",
        "            nn.Conv2d(64, 64, (4, 4), (2, 2), (1, 1), bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            nn.Conv2d(64, 128, (3, 3), (1, 1), (1, 1), bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            # state size. (128) x 32 x 32\n",
        "            nn.Conv2d(128, 128, (4, 4), (2, 2), (1, 1), bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            nn.Conv2d(128, 256, (3, 3), (1, 1), (1, 1), bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            # state size. (256) x 16 x 16\n",
        "            nn.Conv2d(256, 256, (4, 4), (2, 2), (1, 1), bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            nn.Conv2d(256, 512, (3, 3), (1, 1), (1, 1), bias=False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            # state size. (512) x 8 x 8\n",
        "            nn.Conv2d(512, 512, (4, 4), (2, 2), (1, 1), bias=False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            nn.Conv2d(512, 512, (3, 3), (1, 1), (1, 1), bias=False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            # state size. (512) x 4 x 4\n",
        "            nn.Conv2d(512, 512, (4, 4), (2, 2), (1, 1), bias=False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.2, True)\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512 * 4 * 4, 100),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            nn.Linear(100, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        out = self.features(x)\n",
        "        out = torch.flatten(out, 1)\n",
        "        out = self.classifier(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super(Generator, self).__init__()\n",
        "        # The first layer of convolutional layer.\n",
        "        self.conv1 = nn.Conv2d(3, 64, (3, 3), (1, 1), (1, 1))\n",
        "\n",
        "        # Feature extraction backbone network.\n",
        "        trunk = []\n",
        "        for _ in range(23):\n",
        "            trunk.append(ResidualResidualDenseBlock(64, 32))\n",
        "        self.trunk = nn.Sequential(*trunk)\n",
        "\n",
        "        # After the feature extraction network, reconnect a layer of convolutional blocks.\n",
        "        self.conv2 = nn.Conv2d(64, 64, (3, 3), (1, 1), (1, 1))\n",
        "\n",
        "        # Upsampling convolutional layer.\n",
        "        self.upsampling1 = nn.Sequential(\n",
        "            nn.Conv2d(64, 64, (3, 3), (1, 1), (1, 1)),\n",
        "            nn.LeakyReLU(0.2, True)\n",
        "        )\n",
        "        self.upsampling2 = nn.Sequential(\n",
        "            nn.Conv2d(64, 64, (3, 3), (1, 1), (1, 1)),\n",
        "            nn.LeakyReLU(0.2, True)\n",
        "        )\n",
        "\n",
        "        # Reconnect a layer of convolution block after upsampling.\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(64, 64, (3, 3), (1, 1), (1, 1)),\n",
        "            nn.LeakyReLU(0.2, True)\n",
        "        )\n",
        "\n",
        "        # Output layer.\n",
        "        self.conv4 = nn.Conv2d(64, 3, (3, 3), (1, 1), (1, 1))\n",
        "\n",
        "    # The model should be defined in the Torch.script method.\n",
        "    def _forward_impl(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        out1 = self.conv1(x)\n",
        "        out = self.trunk(out1)\n",
        "        out2 = self.conv2(out)\n",
        "        out = torch.add(out1, out2)\n",
        "\n",
        "        out = self.upsampling1(F.interpolate(out, scale_factor=2, mode=\"nearest\"))\n",
        "        out = self.upsampling2(F.interpolate(out, scale_factor=2, mode=\"nearest\"))\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.conv4(out)\n",
        "\n",
        "        out = torch.clamp_(out, 0.0, 1.0)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self._forward_impl(x)\n",
        "\n",
        "\n",
        "class ContentLoss(nn.Module):\n",
        "    \"\"\"Constructs a content loss function based on the VGG19 network.\n",
        "    Using high-level feature mapping layers from the latter layers will focus more on the texture content of the image.\n",
        "    Paper reference list:\n",
        "        -`Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network <https://arxiv.org/pdf/1609.04802.pdf>` paper.\n",
        "        -`ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks                    <https://arxiv.org/pdf/1809.00219.pdf>` paper.\n",
        "        -`Perceptual Extreme Super Resolution Network with Receptive Field Block               <https://arxiv.org/pdf/2005.12597.pdf>` paper.\n",
        "     \"\"\"\n",
        "\n",
        "    def __init__(self, feature_model_extractor_node: str,\n",
        "                 feature_model_normalize_mean: list,\n",
        "                 feature_model_normalize_std: list) -> None:\n",
        "        super(ContentLoss, self).__init__()\n",
        "        # Get the name of the specified feature extraction node\n",
        "        self.feature_model_extractor_node = feature_model_extractor_node\n",
        "        # Load the VGG19 model trained on the ImageNet dataset.\n",
        "        model = models.vgg19(True)\n",
        "        # Extract the thirty-fifth layer output in the VGG19 model as the content loss.\n",
        "        self.feature_extractor = create_feature_extractor(model, [feature_model_extractor_node])\n",
        "        # set to validation mode\n",
        "        self.feature_extractor.eval()\n",
        "\n",
        "        # The preprocessing method of the input data.\n",
        "        # This is the VGG model preprocessing method of the ImageNet dataset\n",
        "        self.normalize = transforms.Normalize(feature_model_normalize_mean, feature_model_normalize_std)\n",
        "\n",
        "        # Freeze model parameters.\n",
        "        for model_parameters in self.feature_extractor.parameters():\n",
        "            model_parameters.requires_grad = False\n",
        "\n",
        "    def forward(self, sr_tensor: torch.Tensor, hr_tensor: torch.Tensor) -> torch.Tensor:\n",
        "        # Standardized operations\n",
        "        sr_tensor = self.normalize(sr_tensor)\n",
        "        hr_tensor = self.normalize(hr_tensor)\n",
        "\n",
        "        sr_feature = self.feature_extractor(sr_tensor)[self.feature_model_extractor_node]\n",
        "        hr_feature = self.feature_extractor(hr_tensor)[self.feature_model_extractor_node]\n",
        "\n",
        "        # Find the feature map difference between the two images\n",
        "        content_loss = F.l1_loss(sr_feature, hr_feature)\n",
        "\n",
        "        return content_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzANhORni6JB"
      },
      "source": [
        "# **Image quality assesment**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "q3XQneEPg5lI"
      },
      "outputs": [],
      "source": [
        "#image_quality_assessment.py\n",
        "\n",
        "import warnings\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "__all__ = [\n",
        "    \"psnr\", \"ssim\",\n",
        "    \"PSNR\", \"SSIM\",\n",
        "]\n",
        "\n",
        "\n",
        "# The following is the implementation of IQA method in Python, using CPU as processing device\n",
        "def _check_image(raw_image: np.ndarray, dst_image: np.ndarray):\n",
        "    \"\"\"Check whether the size and type of the two images are the same\n",
        "    Args:\n",
        "        raw_image (np.ndarray): image data to be compared, BGR format, data range [0, 255]\n",
        "        dst_image (np.ndarray): reference image data, BGR format, data range [0, 255]\n",
        "    \"\"\"\n",
        "    # check image scale\n",
        "    assert raw_image.shape == dst_image.shape, \\\n",
        "        f\"Supplied images have different sizes {str(raw_image.shape)} and {str(dst_image.shape)}\"\n",
        "\n",
        "    # check image type\n",
        "    if raw_image.dtype != dst_image.dtype:\n",
        "        warnings.warn(f\"Supplied images have different dtypes{str(raw_image.shape)} and {str(dst_image.shape)}\")\n",
        "\n",
        "\n",
        "def psnr(raw_image: np.ndarray, dst_image: np.ndarray, crop_border: int, only_test_y_channel: bool) -> float:\n",
        "    \"\"\"Python implements PSNR (Peak Signal-to-Noise Ratio, peak signal-to-noise ratio) function\n",
        "    Args:\n",
        "        raw_image (np.ndarray): image data to be compared, BGR format, data range [0, 255]\n",
        "        dst_image (np.ndarray): reference image data, BGR format, data range [0, 255]\n",
        "        crop_border (int): crop border a few pixels\n",
        "        only_test_y_channel (bool): Whether to test only the Y channel of the image.\n",
        "    Returns:\n",
        "        psnr_metrics (np.float64): PSNR metrics\n",
        "    \"\"\"\n",
        "    # Check if two images are similar in scale and type\n",
        "    _check_image(raw_image, dst_image)\n",
        "\n",
        "    # crop border pixels\n",
        "    if crop_border > 0:\n",
        "        raw_image = raw_image[crop_border:-crop_border, crop_border:-crop_border, ...]\n",
        "        dst_image = dst_image[crop_border:-crop_border, crop_border:-crop_border, ...]\n",
        "\n",
        "    # If you only test the Y channel, you need to extract the Y channel data of the YCbCr channel data separately\n",
        "    if only_test_y_channel:\n",
        "        raw_image = expand_y(raw_image)\n",
        "        dst_image = expand_y(dst_image)\n",
        "\n",
        "    # Convert data type to numpy.float64 bit\n",
        "    raw_image = raw_image.astype(np.float64)\n",
        "    dst_image = dst_image.astype(np.float64)\n",
        "\n",
        "    psnr_metrics = 10 * np.log10((255.0 ** 2) / np.mean((raw_image - dst_image) ** 2) + 1e-8)\n",
        "\n",
        "    return psnr_metrics\n",
        "\n",
        "\n",
        "def _ssim(raw_image: np.ndarray, dst_image: np.ndarray) -> float:\n",
        "    \"\"\"Python implements the SSIM (Structural Similarity) function, which only calculates single-channel data\n",
        "    Args:\n",
        "        raw_image (np.ndarray): The image data to be compared, in BGR format, the data range is [0, 255]\n",
        "        dst_image (np.ndarray): reference image data, BGR format, data range is [0, 255]\n",
        "    Returns:\n",
        "        ssim_metrics (float): SSIM metrics for single channel\n",
        "    \"\"\"\n",
        "    c1 = (0.01 * 255.0) ** 2\n",
        "    c2 = (0.03 * 255.0) ** 2\n",
        "\n",
        "    kernel = cv2.getGaussianKernel(11, 1.5)\n",
        "    kernel_window = np.outer(kernel, kernel.transpose())\n",
        "\n",
        "    raw_mean = cv2.filter2D(raw_image, -1, kernel_window)[5:-5, 5:-5]\n",
        "    dst_mean = cv2.filter2D(dst_image, -1, kernel_window)[5:-5, 5:-5]\n",
        "    raw_mean_square = raw_mean ** 2\n",
        "    dst_mean_square = dst_mean ** 2\n",
        "    raw_dst_mean = raw_mean * dst_mean\n",
        "    raw_variance = cv2.filter2D(raw_image ** 2, -1, kernel_window)[5:-5, 5:-5] - raw_mean_square\n",
        "    dst_variance = cv2.filter2D(dst_image ** 2, -1, kernel_window)[5:-5, 5:-5] - dst_mean_square\n",
        "    raw_dst_covariance = cv2.filter2D(raw_image * dst_image, -1, kernel_window)[5:-5, 5:-5] - raw_dst_mean\n",
        "\n",
        "    ssim_molecular = (2 * raw_dst_mean + c1) * (2 * raw_dst_covariance + c2)\n",
        "    ssim_denominator = (raw_mean_square + dst_mean_square + c1) * (raw_variance + dst_variance + c2)\n",
        "\n",
        "    ssim_metrics = ssim_molecular / ssim_denominator\n",
        "    ssim_metrics = np.mean(ssim_metrics)\n",
        "\n",
        "    return ssim_metrics\n",
        "\n",
        "\n",
        "def ssim(raw_image: np.ndarray, dst_image: np.ndarray, crop_border: int, only_test_y_channel: bool) -> float:\n",
        "    \"\"\"Python implements the SSIM (Structural Similarity) function, which calculates single/multi-channel data\n",
        "    Args:\n",
        "        raw_image (np.ndarray): The image data to be compared, in BGR format, the data range is [0, 255]\n",
        "        dst_image (np.ndarray): reference image data, BGR format, data range is [0, 255]\n",
        "        crop_border (int): crop border a few pixels\n",
        "        only_test_y_channel (bool): Whether to test only the Y channel of the image\n",
        "    Returns:\n",
        "        ssim_metrics (float): SSIM metrics for single channel\n",
        "    \"\"\"\n",
        "    # Check if two images are similar in scale and type\n",
        "    _check_image(raw_image, dst_image)\n",
        "\n",
        "    # crop border pixels\n",
        "    if crop_border > 0:\n",
        "        raw_image = raw_image[crop_border:-crop_border, crop_border:-crop_border, ...]\n",
        "        dst_image = dst_image[crop_border:-crop_border, crop_border:-crop_border, ...]\n",
        "\n",
        "    # If you only test the Y channel, you need to extract the Y channel data of the YCbCr channel data separately\n",
        "    if only_test_y_channel:\n",
        "        raw_image = expand_y(raw_image)\n",
        "        dst_image = expand_y(dst_image)\n",
        "\n",
        "    # Convert data type to numpy.float64 bit\n",
        "    raw_image = raw_image.astype(np.float64)\n",
        "    dst_image = dst_image.astype(np.float64)\n",
        "\n",
        "    channels_ssim_metrics = []\n",
        "    for channel in range(raw_image.shape[2]):\n",
        "        ssim_metrics = _ssim(raw_image[..., channel], dst_image[..., channel])\n",
        "        channels_ssim_metrics.append(ssim_metrics)\n",
        "    ssim_metrics = np.mean(np.asarray(channels_ssim_metrics))\n",
        "\n",
        "    return ssim_metrics\n",
        "\n",
        "\n",
        "# The following is the IQA method implemented by PyTorch, using CUDA as the processing device\n",
        "def _check_tensor_shape(raw_tensor: torch.Tensor, dst_tensor: torch.Tensor):\n",
        "    \"\"\"Check if the dimensions of the two tensors are the same\n",
        "    Args:\n",
        "        raw_tensor (np.ndarray or torch.Tensor): image tensor flow to be compared, RGB format, data range [0, 1]\n",
        "        dst_tensor (np.ndarray or torch.Tensor): reference image tensorflow, RGB format, data range [0, 1]\n",
        "    \"\"\"\n",
        "    # Check if tensor scales are consistent\n",
        "    assert raw_tensor.shape == dst_tensor.shape, \\\n",
        "        f\"Supplied images have different sizes {str(raw_tensor.shape)} and {str(dst_tensor.shape)}\"\n",
        "\n",
        "\n",
        "def _psnr_torch(raw_tensor: torch.Tensor, dst_tensor: torch.Tensor, crop_border: int,\n",
        "                only_test_y_channel: bool) -> float:\n",
        "    \"\"\"PyTorch implements PSNR (Peak Signal-to-Noise Ratio, peak signal-to-noise ratio) function\n",
        "    Args:\n",
        "        raw_tensor (torch.Tensor): image tensor flow to be compared, RGB format, data range [0, 1]\n",
        "        dst_tensor (torch.Tensor): reference image tensorflow, RGB format, data range [0, 1]\n",
        "        crop_border (int): crop border a few pixels\n",
        "        only_test_y_channel (bool): Whether to test only the Y channel of the image\n",
        "    Returns:\n",
        "        psnr_metrics (torch.Tensor): PSNR metrics\n",
        "    \"\"\"\n",
        "    # Check if two tensor scales are similar\n",
        "    _check_tensor_shape(raw_tensor, dst_tensor)\n",
        "\n",
        "    # crop border pixels\n",
        "    if crop_border > 0:\n",
        "        raw_tensor = raw_tensor[:, :, crop_border:-crop_border, crop_border:-crop_border]\n",
        "        dst_tensor = dst_tensor[:, :, crop_border:-crop_border, crop_border:-crop_border]\n",
        "\n",
        "    # Convert RGB tensor data to YCbCr tensor, and extract only Y channel data\n",
        "    if only_test_y_channel:\n",
        "        raw_tensor = rgb2ycbcr_torch(raw_tensor, only_use_y_channel=True)\n",
        "        dst_tensor = rgb2ycbcr_torch(dst_tensor, only_use_y_channel=True)\n",
        "\n",
        "    # Convert data type to torch.float64 bit\n",
        "    raw_tensor = raw_tensor.to(torch.float64)\n",
        "    dst_tensor = dst_tensor.to(torch.float64)\n",
        "\n",
        "    mse_value = torch.mean((raw_tensor * 255.0 - dst_tensor * 255.0) ** 2 + 1e-8, dim=[1, 2, 3])\n",
        "    psnr_metrics = 10 * torch.log10_(255.0 ** 2 / mse_value)\n",
        "\n",
        "    return psnr_metrics\n",
        "\n",
        "\n",
        "class PSNR(nn.Module):\n",
        "    \"\"\"PyTorch implements PSNR (Peak Signal-to-Noise Ratio, peak signal-to-noise ratio) function\n",
        "    Attributes:\n",
        "        crop_border (int): crop border a few pixels\n",
        "        only_test_y_channel (bool): Whether to test only the Y channel of the image\n",
        "    Returns:\n",
        "        psnr_metrics (torch.Tensor): PSNR metrics\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, crop_border: int, only_test_y_channel: bool) -> None:\n",
        "        super().__init__()\n",
        "        self.crop_border = crop_border\n",
        "        self.only_test_y_channel = only_test_y_channel\n",
        "\n",
        "    def forward(self, raw_tensor: torch.Tensor, dst_tensor: torch.Tensor) -> torch.Tensor:\n",
        "        psnr_metrics = _psnr_torch(raw_tensor, dst_tensor, self.crop_border, self.only_test_y_channel)\n",
        "\n",
        "        return psnr_metrics\n",
        "\n",
        "\n",
        "def _ssim_torch(raw_tensor: torch.Tensor,\n",
        "                dst_tensor: torch.Tensor,\n",
        "                window_size: int,\n",
        "                gaussian_kernel_window: np.ndarray) -> float:\n",
        "    \"\"\"PyTorch implements the SSIM (Structural Similarity) function, which only calculates single-channel data\n",
        "    Args:\n",
        "        raw_tensor (torch.Tensor): image tensor flow to be compared, RGB format, data range [0, 255]\n",
        "        dst_tensor (torch.Tensor): reference image tensorflow, RGB format, data range [0, 255]\n",
        "        window_size (int): Gaussian filter size\n",
        "        gaussian_kernel_window (np.ndarray): Gaussian filter\n",
        "    Returns:\n",
        "        ssim_metrics (torch.Tensor): SSIM metrics\n",
        "    \"\"\"\n",
        "    c1 = (0.01 * 255.0) ** 2\n",
        "    c2 = (0.03 * 255.0) ** 2\n",
        "\n",
        "    gaussian_kernel_window = torch.from_numpy(gaussian_kernel_window).view(1, 1, window_size, window_size)\n",
        "    gaussian_kernel_window = gaussian_kernel_window.expand(raw_tensor.size(1), 1, window_size, window_size)\n",
        "    gaussian_kernel_window = gaussian_kernel_window.to(device=raw_tensor.device, dtype=raw_tensor.dtype)\n",
        "\n",
        "    raw_mean = F.conv2d(raw_tensor, gaussian_kernel_window, stride=(1, 1), padding=(0, 0), groups=raw_tensor.shape[1])\n",
        "    dst_mean = F.conv2d(dst_tensor, gaussian_kernel_window, stride=(1, 1), padding=(0, 0), groups=dst_tensor.shape[1])\n",
        "    raw_mean_square = raw_mean ** 2\n",
        "    dst_mean_square = dst_mean ** 2\n",
        "    raw_dst_mean = raw_mean * dst_mean\n",
        "    raw_variance = F.conv2d(raw_tensor * raw_tensor, gaussian_kernel_window, stride=(1, 1), padding=(0, 0),\n",
        "                            groups=raw_tensor.shape[1]) - raw_mean_square\n",
        "    dst_variance = F.conv2d(dst_tensor * dst_tensor, gaussian_kernel_window, stride=(1, 1), padding=(0, 0),\n",
        "                            groups=raw_tensor.shape[1]) - dst_mean_square\n",
        "    raw_dst_covariance = F.conv2d(raw_tensor * dst_tensor, gaussian_kernel_window, stride=1, padding=(0, 0),\n",
        "                                  groups=raw_tensor.shape[1]) - raw_dst_mean\n",
        "\n",
        "    ssim_molecular = (2 * raw_dst_mean + c1) * (2 * raw_dst_covariance + c2)\n",
        "    ssim_denominator = (raw_mean_square + dst_mean_square + c1) * (raw_variance + dst_variance + c2)\n",
        "\n",
        "    ssim_metrics = ssim_molecular / ssim_denominator\n",
        "    ssim_metrics = torch.mean(ssim_metrics, [1, 2, 3])\n",
        "\n",
        "    return ssim_metrics\n",
        "\n",
        "\n",
        "def _ssim_single_torch(raw_tensor: torch.Tensor,\n",
        "                       dst_tensor: torch.Tensor,\n",
        "                       crop_border: int,\n",
        "                       only_test_y_channel: bool,\n",
        "                       window_size: int,\n",
        "                       gaussian_kernel_window: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"PyTorch implements the SSIM (Structural Similarity) function, which only calculates single-channel data\n",
        "    Args:\n",
        "        raw_tensor (torch.Tensor): image tensor flow to be compared, RGB format, data range [0, 1]\n",
        "        dst_tensor (torch.Tensor): reference image tensorflow, RGB format, data range [0, 1]\n",
        "        crop_border (int): crop border a few pixels\n",
        "        only_test_y_channel (bool): Whether to test only the Y channel of the image\n",
        "        window_size (int): Gaussian filter size\n",
        "        gaussian_kernel_window (torch.Tensor): Gaussian filter\n",
        "    Returns:\n",
        "        ssim_metrics (torch.Tensor): SSIM metrics\n",
        "    \"\"\"\n",
        "    # Check if two tensor scales are similar\n",
        "    _check_tensor_shape(raw_tensor, dst_tensor)\n",
        "\n",
        "    # crop border pixels\n",
        "    if crop_border > 0:\n",
        "        raw_tensor = raw_tensor[:, :, crop_border:-crop_border, crop_border:-crop_border]\n",
        "        dst_tensor = dst_tensor[:, :, crop_border:-crop_border, crop_border:-crop_border]\n",
        "\n",
        "    # Convert RGB tensor data to YCbCr tensor, and extract only Y channel data\n",
        "    if only_test_y_channel:\n",
        "        raw_tensor = rgb2ycbcr_torch(raw_tensor, only_use_y_channel=True)\n",
        "        dst_tensor = rgb2ycbcr_torch(dst_tensor, only_use_y_channel=True)\n",
        "\n",
        "    # Convert data type to torch.float64 bit\n",
        "    raw_tensor = raw_tensor.to(torch.float64)\n",
        "    dst_tensor = dst_tensor.to(torch.float64)\n",
        "\n",
        "    ssim_metrics = _ssim_torch(raw_tensor * 255.0, dst_tensor * 255.0, window_size, gaussian_kernel_window)\n",
        "\n",
        "    return ssim_metrics\n",
        "\n",
        "\n",
        "class SSIM(nn.Module):\n",
        "    \"\"\"PyTorch implements the SSIM (Structural Similarity) function, which only calculates single-channel data\n",
        "    Args:\n",
        "        crop_border (int): crop border a few pixels\n",
        "        only_only_test_y_channel (bool): Whether to test only the Y channel of the image\n",
        "        window_size (int): Gaussian filter size\n",
        "        gaussian_sigma (float): sigma parameter in Gaussian filter\n",
        "    Returns:\n",
        "        ssim_metrics (torch.Tensor): SSIM metrics\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, crop_border: int,\n",
        "                 only_only_test_y_channel: bool,\n",
        "                 window_size: int = 11,\n",
        "                 gaussian_sigma: float = 1.5) -> None:\n",
        "        super().__init__()\n",
        "        self.crop_border = crop_border\n",
        "        self.only_test_y_channel = only_only_test_y_channel\n",
        "        self.window_size = window_size\n",
        "\n",
        "        gaussian_kernel = cv2.getGaussianKernel(window_size, gaussian_sigma)\n",
        "        self.gaussian_kernel_window = np.outer(gaussian_kernel, gaussian_kernel.transpose())\n",
        "\n",
        "    def forward(self, raw_tensor: torch.Tensor, dst_tensor: torch.Tensor) -> torch.Tensor:\n",
        "        ssim_metrics = _ssim_single_torch(raw_tensor,\n",
        "                                          dst_tensor,\n",
        "                                          self.crop_border,\n",
        "                                          self.only_test_y_channel,\n",
        "                                          self.window_size,\n",
        "                                          self.gaussian_kernel_window)\n",
        "\n",
        "        return ssim_metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8uNl0XtMwNZ"
      },
      "source": [
        "# **config for train RRDBNet**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "G36ite8UM0kd"
      },
      "outputs": [],
      "source": [
        "#config.py\n",
        "\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.backends import cudnn\n",
        "\n",
        "# Random seed to maintain reproducible results\n",
        "random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "# Use GPU for training by default\n",
        "device = torch.device(\"cuda\", 0)\n",
        "# Turning on when the image size does not change during training can speed up training\n",
        "cudnn.benchmark = True\n",
        "# When evaluating the performance of the SR model, whether to verify only the Y channel image data\n",
        "only_test_y_channel = True\n",
        "# Image magnification factor\n",
        "upscale_factor = 4\n",
        "# Current configuration parameter method\n",
        "mode = \"train_rrdbnet\"\n",
        "# Experiment name, easy to save weights and log files\n",
        "exp_name = \"RRDBNet_baseline\"\n",
        "\n",
        "if mode == \"train_rrdbnet\":\n",
        "    # Dataset address\n",
        "    train_image_dir = \"./datasets/DIV2K/ESRGAN/train\"\n",
        "    valid_image_dir = \"./datasets/DIV2K/ESRGAN/valid\"\n",
        "    test_lr_image_dir = f\"./datasets/Set5/LRbicx{upscale_factor}\"\n",
        "    test_hr_image_dir = \"./datasets/Set5/GTmod12\"\n",
        "\n",
        "    #image_size = 128\n",
        "    gt_image_size = int(upscale_factor*64)\n",
        "    #batch_size = 16\n",
        "    batch_size = 8\n",
        "    num_workers = 2\n",
        "\n",
        "    # The address to load the pretrained model\n",
        "    pretrained_model_path = \"./ColabNotebooks/ESRGAN-pytorch/results/pretrained_models/RRDBNet_x4-DFO2K-2e2a91f4.pth.tar\"\n",
        "\n",
        "    # Incremental training and migration training\n",
        "    resume = f\"\"\n",
        "\n",
        "    # Total num epochs\n",
        "    epochs = 108\n",
        "\n",
        "    # Optimizer parameter\n",
        "    model_lr = 2e-4\n",
        "    model_betas = (0.9, 0.99)\n",
        "\n",
        "    # Dynamically adjust the learning rate policy\n",
        "    lr_scheduler_step_size = epochs // 5\n",
        "    lr_scheduler_gamma = 0.5\n",
        "\n",
        "    # How many iterations to print the training result\n",
        "    print_frequency = 200\n",
        "\n",
        "if mode == \"train_esrgan\":\n",
        "    # Dataset address\n",
        "    train_image_dir = \"./datasets/DIV2K/ESRGAN/train\"\n",
        "    valid_image_dir = \"./datasets/DIV2K/ESRGAN/valid\"\n",
        "    test_lr_image_dir = f\"./datasets/Set5/LRbicx{upscale_factor}\"\n",
        "    test_hr_image_dir = \"./datasets/Set5/GTmod12\"\n",
        "\n",
        "    #image_size = 128\n",
        "    image_size = 128\n",
        "    #batch_size = 16\n",
        "    batch_size = 8\n",
        "    num_workers = 2\n",
        "\n",
        "    # The address to load the pretrained model\n",
        "    pretrained_d_model_path = \"\"\n",
        "    pretrained_g_model_path = \"./ColabNotebooks/ESRGAN-pytorch/results/RRDBNet_baseline/g_last.pth.tar\"\n",
        "\n",
        "    # Incremental training and migration training\n",
        "    resume_d = \"\"\n",
        "    resume_g = \"\"\n",
        "\n",
        "    # Total num epochs\n",
        "    epochs = 44\n",
        "\n",
        "    # Feature extraction layer parameter configuration\n",
        "    feature_model_extractor_node = \"features.34\"\n",
        "    feature_model_normalize_mean = [0.485, 0.456, 0.406]\n",
        "    feature_model_normalize_std = [0.229, 0.224, 0.225]\n",
        "\n",
        "    # Loss function weight\n",
        "    pixel_weight = 0.01\n",
        "    content_weight = 1.0\n",
        "    adversarial_weight = 0.005\n",
        "\n",
        "    # Adam optimizer parameter\n",
        "    model_lr = 1e-4\n",
        "    model_betas = (0.9, 0.99)\n",
        "\n",
        "    # Dynamically adjust the learning rate policy\n",
        "    lr_scheduler_milestones = [int(epochs * 0.125), int(epochs * 0.250), int(epochs * 0.500), int(epochs * 0.750)]\n",
        "    lr_scheduler_gamma = 0.5\n",
        "\n",
        "    # How many iterations to print the training result\n",
        "    print_frequency = 200\n",
        "\n",
        "if mode == \"test\":\n",
        "    # Test data address\n",
        "    lr_dir = f\"./datasets/Set5/LRbicx{upscale_factor}\"\n",
        "    sr_dir = f\"./ColabNotebooks/ESRGAN-pytorch/results/test/{exp_name}\"\n",
        "    hr_dir = \"./datasets/Set5/GTmod12\"\n",
        "\n",
        "    model_path = \"./ColabNotebooks/ESRGAN-pytorch/results/pretrained_models/RRDBNet_x4-DFO2K-2e2a91f4.pth.tar\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KF1YChejDVE"
      },
      "source": [
        "# **train RRDBNet**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "QWBhHAIHLOUW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 973
        },
        "outputId": "1d1b9115-746b-4300-baae-9cb6c7ab7d19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Load all datasets successfully.\n",
            "Build RRDBNet model successfully.\n",
            "Define all loss functions successfully.\n",
            "Define all optimizer functions successfully.\n",
            "Define all optimizer scheduler successfully.\n",
            "Check whether to load pretrained model weights...\n",
            "Loaded `./ColabNotebooks/ESRGAN-pytorch/results/pretrained_models/RRDBNet_x4-DFO2K-2e2a91f4.pth.tar` pretrained model weights successfully.\n",
            "Check whether the pretrained model is restored...\n",
            "Epoch: [1][   0/3494]\tTime 10.667 (10.667)\tData  0.000 ( 0.000)\tLoss 0.019024 (0.019024)\n",
            "Epoch: [1][ 200/3494]\tTime  4.424 ( 3.641)\tData  2.932 ( 2.100)\tLoss 0.020263 (0.022606)\n",
            "Epoch: [1][ 400/3494]\tTime  4.675 ( 3.644)\tData  3.182 ( 2.126)\tLoss 0.021115 (0.022747)\n",
            "Epoch: [1][ 600/3494]\tTime  3.501 ( 3.630)\tData  2.007 ( 2.120)\tLoss 0.013760 (0.022955)\n",
            "Epoch: [1][ 800/3494]\tTime  5.400 ( 3.641)\tData  3.908 ( 2.134)\tLoss 0.024497 (0.023146)\n",
            "Epoch: [1][1000/3494]\tTime  1.494 ( 3.647)\tData  0.000 ( 2.143)\tLoss 0.026159 (0.023246)\n",
            "Epoch: [1][1200/3494]\tTime  3.317 ( 3.651)\tData  1.821 ( 2.148)\tLoss 0.026954 (0.023200)\n",
            "Epoch: [1][1400/3494]\tTime  1.494 ( 3.660)\tData  0.000 ( 2.159)\tLoss 0.030006 (0.023216)\n",
            "Epoch: [1][1600/3494]\tTime  1.498 ( 3.654)\tData  0.001 ( 2.153)\tLoss 0.027462 (0.023152)\n",
            "Epoch: [1][1800/3494]\tTime  1.491 ( 3.645)\tData  0.000 ( 2.145)\tLoss 0.024971 (0.023079)\n",
            "Epoch: [1][2000/3494]\tTime  1.492 ( 3.657)\tData  0.000 ( 2.157)\tLoss 0.016345 (0.023072)\n",
            "Epoch: [1][2200/3494]\tTime  4.930 ( 3.662)\tData  3.438 ( 2.163)\tLoss 0.017348 (0.023061)\n",
            "Epoch: [1][2400/3494]\tTime  8.664 ( 3.665)\tData  7.171 ( 2.166)\tLoss 0.032039 (0.023079)\n",
            "Epoch: [1][2600/3494]\tTime  1.493 ( 3.661)\tData  0.000 ( 2.163)\tLoss 0.020824 (0.023081)\n",
            "Epoch: [1][2800/3494]\tTime  5.182 ( 3.656)\tData  3.689 ( 2.158)\tLoss 0.015132 (0.023097)\n",
            "Epoch: [1][3000/3494]\tTime  1.493 ( 3.655)\tData  0.000 ( 2.157)\tLoss 0.027982 (0.023094)\n",
            "Epoch: [1][3200/3494]\tTime  4.939 ( 3.656)\tData  3.447 ( 2.159)\tLoss 0.028026 (0.023038)\n",
            "Epoch: [1][3400/3494]\tTime  6.592 ( 3.658)\tData  5.098 ( 2.160)\tLoss 0.017028 (0.023051)\n",
            "Valid: [   0/3598]\tTime  0.347 ( 0.347)\tPSNR 27.52 (27.52)\tSSIM 0.8345 (0.8345)\n",
            "Valid: [ 719/3598]\tTime  0.778 ( 0.849)\tPSNR 40.29 (33.11)\tSSIM 0.9720 (0.8445)\n",
            "Valid: [1438/3598]\tTime  0.779 ( 0.871)\tPSNR 34.49 (32.39)\tSSIM 0.8716 (0.8345)\n",
            "Valid: [2157/3598]\tTime  0.863 ( 0.875)\tPSNR 43.19 (32.66)\tSSIM 0.9612 (0.8357)\n",
            "Valid: [2876/3598]\tTime  1.207 ( 0.889)\tPSNR 28.05 (33.06)\tSSIM 0.7635 (0.8386)\n",
            "Valid: [3595/3598]\tTime  0.818 ( 0.892)\tPSNR 34.72 (33.08)\tSSIM 0.9269 (0.8417)\n",
            " * Time 0.89 PSNR 33.08 SSIM 0.84\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-49d1d03ac826>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[0;31m#if __name__ == \"__main__\":\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-49d1d03ac826>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_prefetcher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpixel_criterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_prefetcher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpsnr_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mssim_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Valid\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m         \u001b[0mpsnr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mssim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_prefetcher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpsnr_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mssim_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-49d1d03ac826>\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(model, data_prefetcher, epoch, writer, psnr_model, ssim_model, mode)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m             \u001b[0;31m# Statistical loss value for terminal data output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m             \u001b[0mpsnr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpsnr_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m             \u001b[0mssim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mssim_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m             \u001b[0mpsnres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpsnr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-b5de6baa9f18>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, raw_tensor, dst_tensor)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_tensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst_tensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m         \u001b[0mpsnr_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_psnr_torch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrop_border\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monly_test_y_channel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpsnr_metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-b5de6baa9f18>\u001b[0m in \u001b[0;36m_psnr_torch\u001b[0;34m(raw_tensor, dst_tensor, crop_border, only_test_y_channel)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \"\"\"\n\u001b[1;32m    156\u001b[0m     \u001b[0;31m# Check if two tensor scales are similar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m     \u001b[0m_check_tensor_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;31m# crop border pixels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-b5de6baa9f18>\u001b[0m in \u001b[0;36m_check_tensor_shape\u001b[0;34m(raw_tensor, dst_tensor)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;31m# Check if tensor scales are consistent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mraw_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdst_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0;34mf\"Supplied images have different sizes {str(raw_tensor.shape)} and {str(dst_tensor.shape)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: Supplied images have different sizes torch.Size([1, 3, 276, 276]) and torch.Size([1, 3, 504, 504])"
          ]
        }
      ],
      "source": [
        "#train_rrdbnet.py\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import time\n",
        "from enum import Enum\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.cuda import amp\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "from typing import Tuple\n",
        "\n",
        "def load_dataset() -> Tuple[CUDAPrefetcher, CUDAPrefetcher, CUDAPrefetcher]:\n",
        "    # Load train, test and valid datasets\n",
        "    train_datasets = TrainValidImageDataset(train_image_dir, gt_image_size, upscale_factor, \"Train\")\n",
        "    valid_datasets = TrainValidImageDataset(valid_image_dir, gt_image_size, upscale_factor, \"Valid\")\n",
        "    test_datasets = TestImageDataset(test_lr_image_dir, test_hr_image_dir)\n",
        "\n",
        "    # Generator all dataloader\n",
        "    train_dataloader = DataLoader(train_datasets,\n",
        "                                  batch_size=batch_size,\n",
        "                                  shuffle=True,\n",
        "                                  num_workers=num_workers,\n",
        "                                  pin_memory=True,\n",
        "                                  drop_last=True,\n",
        "                                  persistent_workers=True)\n",
        "    valid_dataloader = DataLoader(valid_datasets,\n",
        "                                  batch_size=1,\n",
        "                                  shuffle=False,\n",
        "                                  num_workers=1,\n",
        "                                  pin_memory=True,\n",
        "                                  drop_last=False,\n",
        "                                  persistent_workers=True)\n",
        "    test_dataloader = DataLoader(test_datasets,\n",
        "                                 batch_size=1,\n",
        "                                 shuffle=False,\n",
        "                                 num_workers=1,\n",
        "                                 pin_memory=True,\n",
        "                                 drop_last=False,\n",
        "                                 persistent_workers=True)\n",
        "\n",
        "    # Place all data on the preprocessing data loader\n",
        "    train_prefetcher = CUDAPrefetcher(train_dataloader, device)\n",
        "    valid_prefetcher = CUDAPrefetcher(valid_dataloader, device)\n",
        "    test_prefetcher = CUDAPrefetcher(test_dataloader, device)\n",
        "\n",
        "    return train_prefetcher, valid_prefetcher, test_prefetcher\n",
        "\n",
        "def main():\n",
        "    # Initialize the number of training epochs\n",
        "    start_epoch = 0\n",
        "\n",
        "    # Initialize training to generate network evaluation indicators\n",
        "    best_psnr = 0.0\n",
        "    best_ssim = 0.0\n",
        "\n",
        "    train_prefetcher, valid_prefetcher, test_prefetcher = load_dataset()\n",
        "    print(\"Load all datasets successfully.\")\n",
        "\n",
        "    model = build_model()\n",
        "    print(\"Build RRDBNet model successfully.\")\n",
        "\n",
        "    pixel_criterion = define_loss()\n",
        "    print(\"Define all loss functions successfully.\")\n",
        "\n",
        "    optimizer = define_optimizer(model)\n",
        "    print(\"Define all optimizer functions successfully.\")\n",
        "\n",
        "    scheduler = define_scheduler(optimizer)\n",
        "    print(\"Define all optimizer scheduler successfully.\")\n",
        "\n",
        "    print(\"Check whether to load pretrained model weights...\")\n",
        "    if pretrained_model_path:\n",
        "        # Load checkpoint model\n",
        "        checkpoint = torch.load(pretrained_model_path, map_location=lambda storage, loc: storage)\n",
        "        # Load model state dict. Extract the fitted model weights\n",
        "        model_state_dict = model.state_dict()\n",
        "        state_dict = {k: v for k, v in checkpoint[\"state_dict\"].items() if\n",
        "                      k in model_state_dict.keys() and v.size() == model_state_dict[k].size()}\n",
        "        # Overwrite the model weights to the current model\n",
        "        model_state_dict.update(state_dict)\n",
        "        model.load_state_dict(model_state_dict)\n",
        "        print(f\"Loaded `{pretrained_model_path}` pretrained model weights successfully.\")\n",
        "    else:\n",
        "        print(\"Pretrained model weights not found.\")\n",
        "\n",
        "    print(\"Check whether the pretrained model is restored...\")\n",
        "    if resume:\n",
        "        # Load checkpoint model\n",
        "        checkpoint = torch.load(resume, map_location=lambda storage, loc: storage)\n",
        "        # Restore the parameters in the training node to this point\n",
        "        start_epoch = checkpoint[\"epoch\"]\n",
        "        best_psnr = checkpoint[\"best_psnr\"]\n",
        "        best_ssim = checkpoint[\"best_ssim\"]\n",
        "        # Load checkpoint state dict. Extract the fitted model weights\n",
        "        model_state_dict = model.state_dict()\n",
        "        new_state_dict = {k: v for k, v in checkpoint[\"state_dict\"].items() if\n",
        "                      k in model_state_dict.keys() and v.size() == model_state_dict[k].size()}\n",
        "        # Overwrite the pretrained model weights to the current model\n",
        "        model_state_dict.update(new_state_dict)\n",
        "        model.load_state_dict(model_state_dict)\n",
        "        # Load the optimizer model\n",
        "        optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
        "        # Load the optimizer scheduler\n",
        "        scheduler.load_state_dict(checkpoint[\"scheduler\"])\n",
        "        print(\"Loaded pretrained model weights.\")\n",
        "\n",
        "    # Create a folder of super-resolution experiment results\n",
        "    samples_dir = os.path.join(\"samples\", exp_name)\n",
        "    results_dir = os.path.join(\"results\", exp_name)\n",
        "    if not os.path.exists(samples_dir):\n",
        "        os.makedirs(samples_dir)\n",
        "    if not os.path.exists(results_dir):\n",
        "        os.makedirs(results_dir)\n",
        "\n",
        "    # Create training process log file\n",
        "    writer = SummaryWriter(os.path.join(\"samples\", \"logs\", exp_name))\n",
        "\n",
        "    # Initialize the gradient scaler\n",
        "    scaler = amp.GradScaler()\n",
        "\n",
        "    # Create an IQA evaluation model\n",
        "    psnr_model = PSNR(upscale_factor, only_test_y_channel)\n",
        "    ssim_model = SSIM(upscale_factor, only_test_y_channel)\n",
        "\n",
        "    # Transfer the IQA model to the specified device\n",
        "    psnr_model = psnr_model.to(device=device, memory_format=torch.channels_last, non_blocking=True)\n",
        "    ssim_model = ssim_model.to(device=device, memory_format=torch.channels_last, non_blocking=True)\n",
        "\n",
        "    for epoch in range(start_epoch, epochs):\n",
        "\n",
        "        train(model, train_prefetcher, pixel_criterion, optimizer, epoch, scaler, writer)\n",
        "        _, _ = validate(model, valid_prefetcher, epoch, writer, psnr_model, ssim_model, \"Valid\")\n",
        "        psnr, ssim = validate(model, test_prefetcher, epoch, writer, psnr_model, ssim_model, \"Test\")\n",
        "        print(\"\\n\")\n",
        "\n",
        "        # Update LR\n",
        "        scheduler.step()\n",
        "\n",
        "        # Automatically save the model with the highest index\n",
        "        is_best = psnr > best_psnr and ssim > best_ssim\n",
        "        best_psnr = max(psnr, best_psnr)\n",
        "        best_ssim = max(ssim, best_ssim)\n",
        "        torch.save({\"epoch\": epoch + 1,\n",
        "                    \"best_psnr\": best_psnr,\n",
        "                    \"best_ssim\": best_ssim,\n",
        "                    \"state_dict\": model.state_dict(),\n",
        "                    \"optimizer\": optimizer.state_dict(),\n",
        "                    \"scheduler\": scheduler.state_dict()},\n",
        "                   os.path.join(samples_dir, f\"g_epoch_{epoch + 1}.pth.tar\"))\n",
        "        if is_best:\n",
        "            shutil.copyfile(os.path.join(samples_dir, f\"g_epoch_{epoch + 1}.pth.tar\"),\n",
        "                            os.path.join(results_dir, \"g_best.pth.tar\"))\n",
        "        if (epoch + 1) == epochs:\n",
        "            shutil.copyfile(os.path.join(samples_dir, f\"g_epoch_{epoch + 1}.pth.tar\"),\n",
        "                            os.path.join(results_dir, \"g_last.pth.tar\"))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def build_model() -> nn.Module:\n",
        "    model = Generator()\n",
        "    model = model.to(device=device, memory_format=torch.channels_last)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def define_loss() -> nn.L1Loss:\n",
        "    pixel_criterion = nn.L1Loss()\n",
        "    pixel_criterion = pixel_criterion.to(device=device, memory_format=torch.channels_last)\n",
        "\n",
        "    return pixel_criterion\n",
        "\n",
        "\n",
        "def define_optimizer(model) -> optim.Adam:\n",
        "    optimizer = optim.Adam(model.parameters(), model_lr, model_betas)\n",
        "\n",
        "    return optimizer\n",
        "\n",
        "\n",
        "def define_scheduler(optimizer) -> lr_scheduler.StepLR:\n",
        "    scheduler = lr_scheduler.StepLR(optimizer, lr_scheduler_step_size, lr_scheduler_gamma)\n",
        "\n",
        "    return scheduler\n",
        "\n",
        "\n",
        "def train(model: nn.Module,\n",
        "          train_prefetcher: CUDAPrefetcher,\n",
        "          pixel_criterion: nn.L1Loss,\n",
        "          optimizer: optim.Adam,\n",
        "          epoch: int,\n",
        "          scaler: amp.GradScaler,\n",
        "          writer: SummaryWriter) -> None:\n",
        "    \"\"\"Training main program\n",
        "    Args:\n",
        "        model (nn.Module): the generator model in the generative network\n",
        "        train_prefetcher (CUDAPrefetcher): training dataset iterator\n",
        "        pixel_criterion (nn.L1Loss): Calculate the pixel difference between real and fake samples\n",
        "        optimizer (optim.Adam): optimizer for optimizing generator models in generative networks\n",
        "        epoch (int): number of training epochs during training the generative network\n",
        "        scaler (amp.GradScaler): Mixed precision training function\n",
        "        writer (SummaryWrite): log file management function\n",
        "    \"\"\"\n",
        "    # Calculate how many batches of data are in each Epoch\n",
        "    batches = len(train_prefetcher)\n",
        "    # Print information of progress bar during training\n",
        "    batch_time = AverageMeter(\"Time\", \":6.3f\")\n",
        "    data_time = AverageMeter(\"Data\", \":6.3f\")\n",
        "    losses = AverageMeter(\"Loss\", \":6.6f\")\n",
        "    progress = ProgressMeter(batches, [batch_time, data_time, losses], prefix=f\"Epoch: [{epoch + 1}]\")\n",
        "\n",
        "    # Put the generative network model in training mode\n",
        "    model.train()\n",
        "\n",
        "    # Initialize the number of data batches to print logs on the terminal\n",
        "    batch_index = 0\n",
        "\n",
        "    # Initialize the data loader and load the first batch of data\n",
        "    train_prefetcher.reset()\n",
        "    batch_data = train_prefetcher.next()\n",
        "\n",
        "    # Get the initialization training time\n",
        "    end = time.time()\n",
        "\n",
        "    while batch_data is not None:\n",
        "        # Calculate the time it takes to load a batch of data\n",
        "        data_time.update(time.time() - end)\n",
        "\n",
        "        # Transfer in-memory data to CUDA devices to speed up training\n",
        "        lr = batch_data[\"lr\"].to(device=device, memory_format=torch.channels_last, non_blocking=True)\n",
        "        hr = batch_data[\"hr\"].to(device=device, memory_format=torch.channels_last, non_blocking=True)\n",
        "\n",
        "        # Initialize generator gradients\n",
        "        model.zero_grad(set_to_none=True)\n",
        "\n",
        "        # Mixed precision training\n",
        "        with amp.autocast():\n",
        "            sr = model(lr)\n",
        "            loss = pixel_criterion(sr, hr)\n",
        "\n",
        "        # Backpropagation\n",
        "        scaler.scale(loss).backward()\n",
        "        # update generator weights\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        # Statistical loss value for terminal data output\n",
        "        losses.update(loss.item(), lr.size(0))\n",
        "\n",
        "        # Calculate the time it takes to fully train a batch of data\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "        # Write the data during training to the training log file\n",
        "        if batch_index % print_frequency == 0:\n",
        "            # Record loss during training and output to file\n",
        "            writer.add_scalar(\"Train/Loss\", loss.item(), batch_index + epoch * batches + 1)\n",
        "            progress.display(batch_index)\n",
        "\n",
        "        # Preload the next batch of data\n",
        "        batch_data = train_prefetcher.next()\n",
        "\n",
        "        # After training a batch of data, add 1 to the number of data batches to ensure that the terminal prints data normally\n",
        "        batch_index += 1\n",
        "\n",
        "\n",
        "def validate(model: nn.Module,\n",
        "             data_prefetcher: CUDAPrefetcher,\n",
        "             epoch: int,\n",
        "             writer: SummaryWriter,\n",
        "             psnr_model: nn.Module,\n",
        "             ssim_model: nn.Module,\n",
        "             mode: str) -> Tuple[float, float]:\n",
        "    \"\"\"Test main program\n",
        "    Args:\n",
        "        model (nn.Module): generator model in adversarial networks\n",
        "        data_prefetcher (CUDAPrefetcher): test dataset iterator\n",
        "        epoch (int): number of test epochs during training of the adversarial network\n",
        "        writer (SummaryWriter): log file management function\n",
        "        psnr_model (nn.Module): The model used to calculate the PSNR function\n",
        "        ssim_model (nn.Module): The model used to compute the SSIM function\n",
        "        mode (str): test validation dataset accuracy or test dataset accuracy\n",
        "    \"\"\"\n",
        "    # Calculate how many batches of data are in each Epoch\n",
        "    batches = len(data_prefetcher)\n",
        "    batch_time = AverageMeter(\"Time\", \":6.3f\")\n",
        "    psnres = AverageMeter(\"PSNR\", \":4.2f\")\n",
        "    ssimes = AverageMeter(\"SSIM\", \":4.4f\")\n",
        "    progress = ProgressMeter(len(data_prefetcher), [batch_time, psnres, ssimes], prefix=f\"{mode}: \")\n",
        "\n",
        "    # Put the adversarial network model in validation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Initialize the number of data batches to print logs on the terminal\n",
        "    batch_index = 0\n",
        "\n",
        "    # Initialize the data loader and load the first batch of data\n",
        "    data_prefetcher.reset()\n",
        "    batch_data = data_prefetcher.next()\n",
        "\n",
        "    # Get the initialization test time\n",
        "    end = time.time()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        while batch_data is not None:\n",
        "            # Transfer the in-memory data to the CUDA device to speed up the test\n",
        "            lr = batch_data[\"lr\"].to(device=device, memory_format=torch.channels_last, non_blocking=True)\n",
        "            hr = batch_data[\"hr\"].to(device=device, memory_format=torch.channels_last, non_blocking=True)\n",
        "\n",
        "            # Use the generator model to generate a fake sample\n",
        "            with amp.autocast():\n",
        "                sr = model(lr)\n",
        "\n",
        "            # Statistical loss value for terminal data output\n",
        "            psnr = psnr_model(sr, hr)\n",
        "            ssim = ssim_model(sr, hr)\n",
        "            psnres.update(psnr.item(), lr.size(0))\n",
        "            ssimes.update(ssim.item(), lr.size(0))\n",
        "\n",
        "            # Calculate the time it takes to fully test a batch of data\n",
        "            batch_time.update(time.time() - end)\n",
        "            end = time.time()\n",
        "\n",
        "            # Record training log information\n",
        "            if batch_index % (batches // 5) == 0:\n",
        "                progress.display(batch_index)\n",
        "\n",
        "            # Preload the next batch of data\n",
        "            batch_data = data_prefetcher.next()\n",
        "\n",
        "            # After training a batch of data, add 1 to the number of data batches to ensure that the\n",
        "            # terminal print data normally\n",
        "            batch_index += 1\n",
        "\n",
        "    # print metrics\n",
        "    progress.display_summary()\n",
        "\n",
        "    if mode == \"Valid\" or mode == \"Test\":\n",
        "        writer.add_scalar(f\"{mode}/PSNR\", psnres.avg, epoch + 1)\n",
        "        writer.add_scalar(f\"{mode}/SSIM\", ssimes.avg, epoch + 1)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported mode, please use `Valid` or `Test`.\")\n",
        "\n",
        "    return psnres.avg, ssimes.avg\n",
        "\n",
        "\n",
        "class Summary(Enum):\n",
        "    NONE = 0\n",
        "    AVERAGE = 1\n",
        "    SUM = 2\n",
        "    COUNT = 3\n",
        "\n",
        "\n",
        "class AverageMeter(object):\n",
        "    def __init__(self, name, fmt=\":f\", summary_type=Summary.AVERAGE):\n",
        "        self.name = name\n",
        "        self.fmt = fmt\n",
        "        self.summary_type = summary_type\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "    def __str__(self):\n",
        "        fmtstr = \"{name} {val\" + self.fmt + \"} ({avg\" + self.fmt + \"})\"\n",
        "        return fmtstr.format(**self.__dict__)\n",
        "\n",
        "    def summary(self):\n",
        "        if self.summary_type is Summary.NONE:\n",
        "            fmtstr = \"\"\n",
        "        elif self.summary_type is Summary.AVERAGE:\n",
        "            fmtstr = \"{name} {avg:.2f}\"\n",
        "        elif self.summary_type is Summary.SUM:\n",
        "            fmtstr = \"{name} {sum:.2f}\"\n",
        "        elif self.summary_type is Summary.COUNT:\n",
        "            fmtstr = \"{name} {count:.2f}\"\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid summary type {self.summary_type}\")\n",
        "\n",
        "        return fmtstr.format(**self.__dict__)\n",
        "\n",
        "\n",
        "class ProgressMeter(object):\n",
        "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
        "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
        "        self.meters = meters\n",
        "        self.prefix = prefix\n",
        "\n",
        "    def display(self, batch):\n",
        "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
        "        entries += [str(meter) for meter in self.meters]\n",
        "        print(\"\\t\".join(entries))\n",
        "\n",
        "    def display_summary(self):\n",
        "        entries = [\" *\"]\n",
        "        entries += [meter.summary() for meter in self.meters]\n",
        "        print(\" \".join(entries))\n",
        "\n",
        "    def _get_batch_fmtstr(self, num_batches):\n",
        "        num_digits = len(str(num_batches // 1))\n",
        "        fmt = \"{:\" + str(num_digits) + \"d}\"\n",
        "        return \"[\" + fmt + \"/\" + fmt.format(num_batches) + \"]\"\n",
        "\n",
        "\n",
        "#if __name__ == \"__main__\":\n",
        "main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
