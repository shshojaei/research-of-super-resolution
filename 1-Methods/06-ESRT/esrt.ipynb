{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "source: https://github.com/luissen/ESRT"
      ],
      "metadata": {
        "id": "oNuDYaWzNG-V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UoUIthheHKGp",
        "outputId": "e020b7d7-00e9-40fd-cd29-748e85379d09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import os,shutil\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#png2npy.py scrips"
      ],
      "metadata": {
        "id": "tozH2hYjLvg-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#prepare dataset \n",
        "#run this cell for each folder of images in training (both HR and LR images)\n",
        "import os\n",
        "import argparse\n",
        "import skimage.io as sio\n",
        "import numpy as np\n",
        "import pdb\n",
        "\n",
        "parser = argparse.ArgumentParser(description='Pre-processing .png images')\n",
        "parser.add_argument('--pathFrom', default='./drive/MyDrive/datasets/DIV2K/DIV2K_train_HR/',\n",
        "                    help='directory of images to convert')\n",
        "parser.add_argument('--pathTo', default='./drive/MyDrive/datasets/DIV2K/DIV2K_decoded/DIV2K_train_HR/',\n",
        "                    help='directory of images to save')\n",
        "parser.add_argument('--split', default=True,\n",
        "                    help='save individual images')\n",
        "parser.add_argument('--select', default='',\n",
        "                    help='select certain path')\n",
        "\n",
        "args = parser.parse_args(args=[])\n",
        "\n",
        "for (path, dirs, files) in os.walk(args.pathFrom):\n",
        "    print(path)\n",
        "    targetDir = os.path.join(args.pathTo, path[len(args.pathFrom):])\n",
        "    # pdb.set_trace()\n",
        "    if len(args.select) > 0 and path.find(args.select) == -1:\n",
        "        continue\n",
        "\n",
        "    if not os.path.exists(targetDir):\n",
        "        os.mkdir(targetDir)\n",
        "\n",
        "    if len(dirs) == 0:\n",
        "        pack = {}\n",
        "        n = 0\n",
        "        for fileName in files:\n",
        "            (idx, ext) = os.path.splitext(fileName)\n",
        "            # pdb.set_trace()\n",
        "            if ext == '.png':\n",
        "                image = sio.imread(os.path.join(path, fileName))\n",
        "                if args.split:\n",
        "                    np.save(os.path.join(targetDir, idx + '.npy'), image)\n",
        "                n += 1\n",
        "                if n % 100 == 0:\n",
        "                    print('Converted ' + str(n) + ' images.')"
      ],
      "metadata": {
        "id": "_apaI1HcLzp3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12e1f2d4-7ea0-49cb-f965-6d22332ffed1"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./drive/MyDrive/datasets/DIV2K/DIV2K_train_LR_bicubic/X4/\n",
            "Converted 100 images.\n",
            "Converted 200 images.\n",
            "Converted 300 images.\n",
            "Converted 400 images.\n",
            "Converted 500 images.\n",
            "Converted 600 images.\n",
            "Converted 700 images.\n",
            "Converted 800 images.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#every time you must check the length of HR and LR images, must be equal\n",
        "from glob import glob\n",
        "files = glob('./drive/MyDrive/datasets/DIV2K/DIV2K_decoded/DIV2K_train_LR_bicubic/X4/*.npy')\n",
        "print(len(files))"
      ],
      "metadata": {
        "id": "PL47DUpniZ33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check duplicated images by name \n",
        "sorted(os.listdir('./drive/MyDrive/datasets/DIV2K/DIV2K_decoded/DIV2K_train_LR_bicubic/X4/'))"
      ],
      "metadata": {
        "id": "NzQ8i8JCpa0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#common.py data"
      ],
      "metadata": {
        "id": "VBmuhsp-IfGS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "import skimage.color as sc\n",
        "\n",
        "\n",
        "def get_patch(*args, patch_size, scale):\n",
        "    ih, iw = args[0].shape[:2]\n",
        "\n",
        "    tp = patch_size  # target patch (HR)\n",
        "    ip = tp // scale  # input patch (LR)\n",
        "\n",
        "    ix = random.randrange(0, iw - ip + 1)\n",
        "    iy = random.randrange(0, ih - ip + 1)\n",
        "    tx, ty = scale * ix, scale * iy\n",
        "\n",
        "    ret = [\n",
        "        args[0][iy:iy + ip, ix:ix + ip, :],\n",
        "        *[a[ty:ty + tp, tx:tx + tp, :] for a in args[1:]]\n",
        "    ]  # results\n",
        "    return ret\n",
        "\n",
        "\n",
        "def set_channel(*args, n_channels=3):\n",
        "    def _set_channel(img):\n",
        "        if img.ndim == 2:\n",
        "            img = np.expand_dims(img, axis=2)\n",
        "\n",
        "        c = img.shape[2]\n",
        "        if n_channels == 1 and c == 3:\n",
        "            img = np.expand_dims(sc.rgb2ycbcr(img)[:, :, 0], 2)\n",
        "        elif n_channels == 3 and c == 1:\n",
        "            img = np.concatenate([img] * n_channels, 2)\n",
        "\n",
        "        return img\n",
        "\n",
        "    return [_set_channel(a) for a in args]\n",
        "\n",
        "\n",
        "def np2Tensor(*args, rgb_range):\n",
        "    def _np2Tensor(img):\n",
        "        np_transpose = np.ascontiguousarray(img.transpose((2, 0, 1)))\n",
        "        tensor = torch.from_numpy(np_transpose).float()\n",
        "        tensor.mul_(rgb_range / 255)\n",
        "\n",
        "        return tensor\n",
        "\n",
        "    return [_np2Tensor(a) for a in args]\n",
        "\n",
        "\n",
        "def augment(*args, hflip=True, rot=True):\n",
        "    hflip = hflip and random.random() < 0.5\n",
        "    vflip = rot and random.random() < 0.5\n",
        "    rot90 = rot and random.random() < 0.5\n",
        "\n",
        "    def _augment(img):\n",
        "        if hflip: img = img[:, ::-1, :]\n",
        "        if vflip: img = img[::-1, :, :]\n",
        "        if rot90: img = img.transpose(1, 0, 2)\n",
        "\n",
        "        return img\n",
        "\n",
        "    return [_augment(a) for a in args]"
      ],
      "metadata": {
        "id": "pqH1B_ZfIhwA"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#image_flder.py data"
      ],
      "metadata": {
        "id": "2N5jgmQ1_SGZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###############################################################################\n",
        "# Code from\n",
        "# https://github.com/pytorch/vision/blob/master/torchvision/datasets/folder.py\n",
        "# Modified the original code so that it also loads images from the current\n",
        "# directory as well as the subdirectories\n",
        "###############################################################################\n",
        "\n",
        "import torch.utils.data as data\n",
        "\n",
        "from PIL import Image\n",
        "import os\n",
        "import os.path\n",
        "\n",
        "def default_flist_reader(flist):\n",
        "    imlist = []\n",
        "    with open(flist, 'r') as rf:\n",
        "        for line in rf.readlines():\n",
        "            impath = line.strip()\n",
        "            imlist.append(impath)\n",
        "\n",
        "    return imlist\n",
        "\n",
        "\n",
        "IMG_EXTENSIONS = [\n",
        "    '.jpg', '.JPG', '.jpeg', '.JPEG',\n",
        "    '.png', '.PNG', '.ppm', '.PPM', '.bmp', '.BMP', '.npy'\n",
        "]\n",
        "\n",
        "\n",
        "def is_image_file(filename):\n",
        "    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)\n",
        "\n",
        "\n",
        "def make_dataset(dir):\n",
        "    images = []\n",
        "    assert os.path.isdir(dir), '%s is not a valid directory' % dir\n",
        "\n",
        "    for root, _, fnames in sorted(os.walk(dir)):\n",
        "        for fname in fnames:\n",
        "            if is_image_file(fname):\n",
        "                path = os.path.join(root, fname)\n",
        "                images.append(path)\n",
        "\n",
        "    return images\n",
        "\n",
        "\n",
        "def default_loader(path):\n",
        "    return Image.open(path).convert('RGB')\n",
        "\n",
        "\n",
        "class ImageFolder(data.Dataset):\n",
        "\n",
        "    def __init__(self, root, transform=None, return_paths=False,\n",
        "                 loader=default_loader):\n",
        "        imgs = make_dataset(root)\n",
        "        if len(imgs) == 0:\n",
        "            raise(RuntimeError(\"Found 0 images in: \" + root + \"\\n\"\n",
        "                               \"Supported image extensions are: \" +\n",
        "                               \",\".join(IMG_EXTENSIONS)))\n",
        "\n",
        "        self.root = root\n",
        "        self.imgs = imgs\n",
        "        self.transform = transform\n",
        "        self.return_paths = return_paths\n",
        "        self.loader = loader\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        path = self.imgs[index]\n",
        "        img = self.loader(path)\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "        if self.return_paths:\n",
        "            return img, path\n",
        "        else:\n",
        "            return img\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)"
      ],
      "metadata": {
        "id": "vimJdrpb_Wzp"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#div2k.py data"
      ],
      "metadata": {
        "id": "2Fl6lxi0ILO7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.utils.data as data\n",
        "import os.path\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def default_loader(path):\n",
        "    return cv2.imread(path, cv2.IMREAD_UNCHANGED)[:, :, [2, 1, 0]]\n",
        "\n",
        "def npy_loader(path):\n",
        "    return np.load(path)\n",
        "\n",
        "IMG_EXTENSIONS = [\n",
        "    '.png', '.npy',\n",
        "]\n",
        "\n",
        "def is_image_file(filename):\n",
        "    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)\n",
        "\n",
        "def make_dataset(dir):\n",
        "    images = []\n",
        "    assert os.path.isdir(dir), '%s is not a valid directory' % dir\n",
        "\n",
        "    for root, _, fnames in sorted(os.walk(dir)):\n",
        "        for fname in fnames:\n",
        "            if is_image_file(fname):\n",
        "                path = os.path.join(root, fname)\n",
        "                images.append(path)\n",
        "    return images\n",
        "\n",
        "\n",
        "class div2k(data.Dataset):\n",
        "    def __init__(self, opt):\n",
        "        self.opt = opt\n",
        "        self.scale = self.opt.scale\n",
        "        self.root = self.opt.root\n",
        "        self.ext = self.opt.ext   # '.png' or '.npy'(default)\n",
        "        self.train = True if self.opt.phase == 'train' else False\n",
        "        self.repeat = 10 #self.opt.test_every // (self.opt.n_train // self.opt.batch_size)\n",
        "        self._set_filesystem(self.root)\n",
        "        self.images_hr, self.images_lr = self._scan()\n",
        "\n",
        "    def _set_filesystem(self, dir_data):\n",
        "        self.root = dir_data + '/DIV2K_decoded'\n",
        "        self.dir_hr = os.path.join(self.root, 'DIV2K_train_HR')\n",
        "        self.dir_lr = os.path.join(self.root, 'DIV2K_train_LR_bicubic/X' + str(self.scale))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        lr, hr = self._load_file(idx)\n",
        "        lr, hr = self._get_patch(lr, hr)\n",
        "        lr, hr = set_channel(lr, hr, n_channels=self.opt.n_colors)\n",
        "        lr_tensor, hr_tensor = np2Tensor(lr, hr, rgb_range=self.opt.rgb_range)\n",
        "        return lr_tensor, hr_tensor\n",
        "\n",
        "    def __len__(self):\n",
        "        if self.train:\n",
        "            return self.opt.n_train * self.repeat\n",
        "\n",
        "    def _get_index(self, idx):\n",
        "        if self.train:\n",
        "            return idx % self.opt.n_train\n",
        "        else:\n",
        "          return idx\n",
        "\n",
        "    def _get_patch(self, img_in, img_tar):\n",
        "        patch_size = self.opt.patch_size\n",
        "        scale = self.scale\n",
        "        if self.train:\n",
        "            img_in, img_tar = get_patch(\n",
        "                img_in, img_tar, patch_size=patch_size, scale=scale)\n",
        "            img_in, img_tar = augment(img_in, img_tar)\n",
        "        else:\n",
        "            ih, iw = img_in.shape[:2]\n",
        "            img_tar = img_tar[0:ih * scale, 0:iw * scale, :]\n",
        "        return img_in, img_tar\n",
        "\n",
        "    def _scan(self):\n",
        "        list_hr = sorted(make_dataset(self.dir_hr))\n",
        "        list_lr = sorted(make_dataset(self.dir_lr))\n",
        "        return list_hr, list_lr\n",
        "\n",
        "    def _load_file(self, idx):\n",
        "\n",
        "      idx = self._get_index(idx)\n",
        "      if self.ext == '.npy':\n",
        "        lr = npy_loader(self.images_lr[idx])\n",
        "        hr = npy_loader(self.images_hr[idx])\n",
        "      else:\n",
        "        lr = default_loader(self.images_lr[idx])\n",
        "        hr = default_loader(self.images_hr[idx])\n",
        "      return lr, hr"
      ],
      "metadata": {
        "id": "u67KRdMHIPRE"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#set5.py data"
      ],
      "metadata": {
        "id": "2lER9FDbIU84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.utils.data as data\n",
        "from os.path import join\n",
        "from os import listdir\n",
        "from torchvision.transforms import Compose, ToTensor\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def img_modcrop(image, modulo):\n",
        "    sz = image.size\n",
        "    w = np.int32(sz[0] / modulo) * modulo\n",
        "    h = np.int32(sz[1] / modulo) * modulo\n",
        "    out = image.crop((0, 0, w, h))\n",
        "    return out\n",
        "\n",
        "\n",
        "def np2tensor():\n",
        "    return Compose([\n",
        "        ToTensor(),\n",
        "    ])\n",
        "\n",
        "\n",
        "def is_image_file(filename):\n",
        "    return any(filename.endswith(extension) for extension in [\".bmp\", \".png\", \".jpg\"])\n",
        "\n",
        "\n",
        "def load_image(filepath):\n",
        "    return Image.open(filepath).convert('RGB')\n",
        "\n",
        "\n",
        "class DatasetFromFolderVal(data.Dataset):\n",
        "    def __init__(self, hr_dir, lr_dir, upscale):\n",
        "        super(DatasetFromFolderVal, self).__init__()\n",
        "        self.hr_filenames = sorted([join(hr_dir, x) for x in listdir(hr_dir) if is_image_file(x)])\n",
        "        self.lr_filenames = sorted([join(lr_dir, x) for x in listdir(lr_dir) if is_image_file(x)])\n",
        "        self.upscale = upscale\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        input = load_image(self.lr_filenames[index])\n",
        "        target = load_image(self.hr_filenames[index])\n",
        "        input = np2tensor()(input)\n",
        "        target = np2tensor()(img_modcrop(target, self.upscale))\n",
        "\n",
        "        return input, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.lr_filenames)"
      ],
      "metadata": {
        "id": "g7_iAdsdIYIS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#common.py model"
      ],
      "metadata": {
        "id": "XkeSoRvtJJ8t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.autograd import Variable\n",
        "\n",
        "def default_conv(in_channels, out_channels, kernel_size, bias=True, groups = 1):\n",
        "    wn = lambda x:torch.nn.utils.weight_norm(x)\n",
        "    return nn.Conv2d(\n",
        "        in_channels, out_channels, kernel_size,\n",
        "        padding=(kernel_size//2), bias=bias, groups = groups)\n",
        "class Scale(nn.Module):\n",
        "\n",
        "    def __init__(self, init_value=1e-3):\n",
        "        super().__init__()\n",
        "        self.scale = nn.Parameter(torch.FloatTensor([init_value]))\n",
        "\n",
        "    def forward(self, input):\n",
        "        return input * self.scale\n",
        "        \n",
        "class MeanShift(nn.Conv2d):\n",
        "    def __init__(self, rgb_range, rgb_mean, rgb_std, sign=-1):\n",
        "        super(MeanShift, self).__init__(3, 3, kernel_size=1)\n",
        "        std = torch.Tensor(rgb_std)\n",
        "        self.weight.data = torch.eye(3).view(3, 3, 1, 1)\n",
        "        self.weight.data.div_(std.view(3, 1, 1, 1))\n",
        "        self.bias.data = sign * rgb_range * torch.Tensor(rgb_mean)\n",
        "        self.bias.data.div_(std)\n",
        "        self.requires_grad = False\n",
        "\n",
        "class BasicBlock(nn.Sequential):\n",
        "    def __init__(\n",
        "        self, in_channels, out_channels, kernel_size, stride=1, bias=False,\n",
        "        bn=True, act=nn.ReLU(True)):\n",
        "\n",
        "        m = [nn.Conv2d(\n",
        "            in_channels, out_channels, kernel_size,\n",
        "            padding=(kernel_size//2), stride=stride, bias=bias)\n",
        "        ]\n",
        "        if bn: m.append(nn.BatchNorm2d(out_channels))\n",
        "        if act is not None: m.append(act)\n",
        "        super(BasicBlock, self).__init__(*m)\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self, conv, n_feats, kernel_size,\n",
        "        bias=True, bn=False, act=nn.ReLU(True), res_scale=1):\n",
        "\n",
        "        super(ResBlock, self).__init__()\n",
        "        m = []\n",
        "        for i in range(2):\n",
        "            m.append(conv(n_feats, n_feats, kernel_size, bias=bias))\n",
        "            if bn: m.append(nn.BatchNorm2d(n_feats))\n",
        "            if i == 0: m.append(act)\n",
        "\n",
        "        self.body = nn.Sequential(*m)\n",
        "        self.res_scale = res_scale\n",
        "\n",
        "    def forward(self, x):\n",
        "        res = self.body(x).mul(self.res_scale)\n",
        "        res += x\n",
        "\n",
        "        return res\n",
        "\n",
        "class LuConv(nn.Module):\n",
        "    def __init__(\n",
        "        self, conv, n_feats, kernel_size,\n",
        "        bias=True, bn=False, act=nn.LeakyReLU(0.05), res_scale=1):\n",
        "        super(LuConv, self).__init__()\n",
        "        #self.scale1 = Scale(1)\n",
        "        #self.scale2 = Scale(1)\n",
        "        m = []\n",
        "        for i in range(2):\n",
        "            m.append(conv(n_feats, n_feats, kernel_size, bias=bias))\n",
        "            if bn: m.append(nn.BatchNorm2d(n_feats))\n",
        "            if i == 0: m.append(act)\n",
        "\n",
        "        self.body = nn.Sequential(*m)\n",
        "        self.res_scale = res_scale\n",
        "\n",
        "    def forward(self, x):\n",
        "        res = self.body(x)\n",
        "        return res\n",
        "        \n",
        "class Upsampler(nn.Sequential):\n",
        "    def __init__(self, conv, scale, n_feats, bn=False, act=False, bias=True):\n",
        "\n",
        "        m = []\n",
        "        if (scale & (scale - 1)) == 0:    # Is scale = 2^n?\n",
        "            for _ in range(int(math.log(scale, 2))):\n",
        "                m.append(conv(n_feats, 4 * n_feats, 3, bias))\n",
        "                m.append(nn.PixelShuffle(2))\n",
        "                if bn: m.append(nn.BatchNorm2d(n_feats))\n",
        "\n",
        "                if act == 'relu':\n",
        "                    m.append(nn.ReLU(True))\n",
        "                elif act == 'prelu':\n",
        "                    m.append(nn.PReLU(n_feats))\n",
        "\n",
        "        elif scale == 3:\n",
        "            m.append(conv(n_feats, 9 * n_feats, 3, bias))\n",
        "            m.append(nn.PixelShuffle(3))\n",
        "            if bn: m.append(nn.BatchNorm2d(n_feats))\n",
        "\n",
        "            if act == 'relu':\n",
        "                m.append(nn.ReLU(True))\n",
        "            elif act == 'prelu':\n",
        "                m.append(nn.PReLU(n_feats))\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "        super(Upsampler, self).__init__(*m)"
      ],
      "metadata": {
        "id": "0AWgWyX1JMbe"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#non.py util"
      ],
      "metadata": {
        "id": "iX3Phb5OKzFZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "class _NonLocalBlockND(nn.Module):\n",
        "    def __init__(self, in_channels, inter_channels=None, dimension=3, sub_sample=True, bn_layer=True):\n",
        "        super(_NonLocalBlockND, self).__init__()\n",
        "\n",
        "        assert dimension in [1, 2, 3]\n",
        "\n",
        "        self.dimension = dimension\n",
        "        self.sub_sample = sub_sample\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.inter_channels = inter_channels\n",
        "\n",
        "        if self.inter_channels is None:\n",
        "            self.inter_channels = in_channels // 2\n",
        "            if self.inter_channels == 0:\n",
        "                self.inter_channels = 1\n",
        "\n",
        "        if dimension == 3:\n",
        "            conv_nd = nn.Conv3d\n",
        "            max_pool_layer = nn.MaxPool3d(kernel_size=(1, 2, 2))\n",
        "            bn = nn.BatchNorm3d\n",
        "        elif dimension == 2:\n",
        "            conv_nd = nn.Conv2d\n",
        "            max_pool_layer = nn.MaxPool2d(kernel_size=(2, 2))\n",
        "            bn = nn.BatchNorm2d\n",
        "        else:\n",
        "            conv_nd = nn.Conv1d\n",
        "            max_pool_layer = nn.MaxPool1d(kernel_size=(2))\n",
        "            bn = nn.BatchNorm1d\n",
        "\n",
        "        self.g = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n",
        "                         kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "        if bn_layer:\n",
        "            self.W = nn.Sequential(\n",
        "                conv_nd(in_channels=self.inter_channels, out_channels=self.in_channels,\n",
        "                        kernel_size=1, stride=1, padding=0),\n",
        "                bn(self.in_channels)\n",
        "            )\n",
        "            nn.init.constant_(self.W[1].weight, 0)\n",
        "            nn.init.constant_(self.W[1].bias, 0)\n",
        "        else:\n",
        "            self.W = conv_nd(in_channels=self.inter_channels, out_channels=self.in_channels,\n",
        "                             kernel_size=1, stride=1, padding=0)\n",
        "            nn.init.constant_(self.W.weight, 0)\n",
        "            nn.init.constant_(self.W.bias, 0)\n",
        "\n",
        "        self.theta = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n",
        "                             kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "        self.phi = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n",
        "                           kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "        if sub_sample:\n",
        "            self.g = nn.Sequential(self.g, max_pool_layer)\n",
        "            self.phi = nn.Sequential(self.phi, max_pool_layer)\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        :param x: (b, c, t, h, w)\n",
        "        :return:\n",
        "        '''\n",
        "\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        g_x = self.g(x).view(batch_size, self.inter_channels, -1)\n",
        "        g_x = g_x.permute(0, 2, 1)\n",
        "\n",
        "        theta_x = self.theta(x).view(batch_size, self.inter_channels, -1)\n",
        "        theta_x = theta_x.permute(0, 2, 1)\n",
        "        phi_x = self.phi(x).view(batch_size, self.inter_channels, -1)\n",
        "        f = torch.matmul(theta_x, phi_x)\n",
        "        N = f.size(-1)\n",
        "        f_div_C = f / N\n",
        "\n",
        "        y = torch.matmul(f_div_C, g_x)\n",
        "        y = y.permute(0, 2, 1).contiguous()\n",
        "        y = y.view(batch_size, self.inter_channels, *x.size()[2:])\n",
        "        W_y = self.W(y)\n",
        "        z = W_y + x\n",
        "\n",
        "        return z\n",
        "\n",
        "\n",
        "class NONLocalBlock1D(_NonLocalBlockND):\n",
        "    def __init__(self, in_channels, inter_channels=None, sub_sample=True, bn_layer=True):\n",
        "        super(NONLocalBlock1D, self).__init__(in_channels,\n",
        "                                              inter_channels=inter_channels,\n",
        "                                              dimension=1, sub_sample=sub_sample,\n",
        "                                              bn_layer=bn_layer)\n",
        "\n",
        "\n",
        "class NONLocalBlock2D(_NonLocalBlockND):\n",
        "    def __init__(self, in_channels, inter_channels=None, sub_sample=True, bn_layer=True):\n",
        "        super(NONLocalBlock2D, self).__init__(in_channels,\n",
        "                                              inter_channels=inter_channels,\n",
        "                                              dimension=2, sub_sample=sub_sample,\n",
        "                                              bn_layer=bn_layer)\n",
        "\n",
        "\n",
        "class NONLocalBlock3D(_NonLocalBlockND):\n",
        "    def __init__(self, in_channels, inter_channels=None, sub_sample=True, bn_layer=True):\n",
        "        super(NONLocalBlock3D, self).__init__(in_channels,\n",
        "                                              inter_channels=inter_channels,\n",
        "                                              dimension=3, sub_sample=sub_sample,\n",
        "                                              bn_layer=bn_layer)"
      ],
      "metadata": {
        "id": "KICtzL_XK26Q"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#tools.py util"
      ],
      "metadata": {
        "id": "m_h0IktRK_CU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def normalize(x):\n",
        "    return x.mul_(2).add_(-1)\n",
        "\n",
        "def same_padding(images, ksizes, strides, rates):\n",
        "    assert len(images.size()) == 4\n",
        "    batch_size, channel, rows, cols = images.size()\n",
        "    out_rows = (rows + strides[0] - 1) // strides[0]\n",
        "    out_cols = (cols + strides[1] - 1) // strides[1]\n",
        "    effective_k_row = (ksizes[0] - 1) * rates[0] + 1\n",
        "    effective_k_col = (ksizes[1] - 1) * rates[1] + 1\n",
        "    padding_rows = max(0, (out_rows-1)*strides[0]+effective_k_row-rows)\n",
        "    padding_cols = max(0, (out_cols-1)*strides[1]+effective_k_col-cols)\n",
        "    # Pad the input\n",
        "    padding_top = int(padding_rows / 2.)\n",
        "    padding_left = int(padding_cols / 2.)\n",
        "    padding_bottom = padding_rows - padding_top\n",
        "    padding_right = padding_cols - padding_left\n",
        "    paddings = (padding_left, padding_right, padding_top, padding_bottom)\n",
        "    images = torch.nn.ZeroPad2d(paddings)(images)\n",
        "    return images\n",
        "\n",
        "\n",
        "def extract_image_patches(images, ksizes, strides, rates, padding='same'):\n",
        "    \"\"\"\n",
        "    Extract patches from images and put them in the C output dimension.\n",
        "    :param padding:\n",
        "    :param images: [batch, channels, in_rows, in_cols]. A 4-D Tensor with shape\n",
        "    :param ksizes: [ksize_rows, ksize_cols]. The size of the sliding window for\n",
        "     each dimension of images\n",
        "    :param strides: [stride_rows, stride_cols]\n",
        "    :param rates: [dilation_rows, dilation_cols]\n",
        "    :return: A Tensor\n",
        "    \"\"\"\n",
        "    assert len(images.size()) == 4\n",
        "    assert padding in ['same', 'valid']\n",
        "    batch_size, channel, height, width = images.size()\n",
        "    \n",
        "    if padding == 'same':\n",
        "        images = same_padding(images, ksizes, strides, rates)\n",
        "    elif padding == 'valid':\n",
        "        pass\n",
        "    else:\n",
        "        raise NotImplementedError('Unsupported padding type: {}.\\\n",
        "                Only \"same\" or \"valid\" are supported.'.format(padding))\n",
        "\n",
        "    unfold = torch.nn.Unfold(kernel_size=ksizes,\n",
        "                             dilation=rates,\n",
        "                             padding=0,\n",
        "                             stride=strides)\n",
        "    patches = unfold(images)\n",
        "    return patches  # [N, C*k*k, L], L is the total number of such blocks\n",
        "def reverse_patches(images, out_size, ksizes, strides, padding):\n",
        "    \"\"\"\n",
        "    Extract patches from images and put them in the C output dimension.\n",
        "    :param padding:\n",
        "    :param images: [batch, channels, in_rows, in_cols]. A 4-D Tensor with shape\n",
        "    :param ksizes: [ksize_rows, ksize_cols]. The size of the sliding window for\n",
        "     each dimension of images\n",
        "    :param strides: [stride_rows, stride_cols]\n",
        "    :param rates: [dilation_rows, dilation_cols]\n",
        "    :return: A Tensor\n",
        "    \"\"\"\n",
        "    unfold = torch.nn.Fold(output_size = out_size, \n",
        "                            kernel_size=ksizes, \n",
        "                            dilation=1, \n",
        "                            padding=padding, \n",
        "                            stride=strides)\n",
        "    patches = unfold(images)\n",
        "    return patches  # [N, C*k*k, L], L is the total number of such blocks\n",
        "def reduce_mean(x, axis=None, keepdim=False):\n",
        "    if not axis:\n",
        "        axis = range(len(x.shape))\n",
        "    for i in sorted(axis, reverse=True):\n",
        "        x = torch.mean(x, dim=i, keepdim=keepdim)\n",
        "    return x\n",
        "\n",
        "\n",
        "def reduce_std(x, axis=None, keepdim=False):\n",
        "    if not axis:\n",
        "        axis = range(len(x.shape))\n",
        "    for i in sorted(axis, reverse=True):\n",
        "        x = torch.std(x, dim=i, keepdim=keepdim)\n",
        "    return x\n",
        "\n",
        "\n",
        "def reduce_sum(x, axis=None, keepdim=False):\n",
        "    if not axis:\n",
        "        axis = range(len(x.shape))\n",
        "    for i in sorted(axis, reverse=True):\n",
        "        x = torch.sum(x, dim=i, keepdim=keepdim)\n",
        "    return x"
      ],
      "metadata": {
        "id": "YgSPyZNuLCSJ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#position.py util"
      ],
      "metadata": {
        "id": "n4ced3GbLQWm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# from util.misc import NestedTensor\n",
        "\n",
        "\n",
        "class PositionEmbeddingSine(nn.Module):\n",
        "    \"\"\"\n",
        "    This is a more standard version of the position embedding, very similar to the one\n",
        "    used by the Attention is all you need paper, generalized to work on images.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_pos_feats=64, temperature=10000, normalize=False, scale=None):\n",
        "        super().__init__()\n",
        "        self.num_pos_feats = num_pos_feats\n",
        "        self.temperature = temperature\n",
        "        self.normalize = normalize\n",
        "        if scale is not None and normalize is False:\n",
        "            raise ValueError(\"normalize should be True if scale is passed\")\n",
        "        if scale is None:\n",
        "            scale = 2 * math.pi\n",
        "        self.scale = scale\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = tensor_list.tensors\n",
        "        # mask = tensor_list.mask\n",
        "        # assert mask is not None\n",
        "        # not_mask = ~mask\n",
        "        y_embed = x.cumsum(1, dtype=torch.float32)\n",
        "        x_embed = x.cumsum(2, dtype=torch.float32)\n",
        "        if self.normalize:\n",
        "            eps = 1e-6\n",
        "            y_embed = y_embed / (y_embed[:, -1:, :] + eps) * self.scale\n",
        "            x_embed = x_embed / (x_embed[:, :, -1:] + eps) * self.scale\n",
        "\n",
        "        dim_t = torch.arange(self.num_pos_feats, dtype=torch.float32, device=x.device)\n",
        "        dim_t = self.temperature ** (2 * (dim_t // 2) / self.num_pos_feats)\n",
        "\n",
        "        pos_x = x_embed[:, :, :, None] / dim_t\n",
        "        pos_y = y_embed[:, :, :, None] / dim_t\n",
        "        pos_x = torch.stack((pos_x[:, :, :, 0::2].sin(), pos_x[:, :, :, 1::2].cos()), dim=4).flatten(3)\n",
        "        pos_y = torch.stack((pos_y[:, :, :, 0::2].sin(), pos_y[:, :, :, 1::2].cos()), dim=4).flatten(3)\n",
        "        pos = torch.cat((pos_y, pos_x), dim=3).permute(0, 3, 1, 2)\n",
        "        return pos\n",
        "\n",
        "\n",
        "class PositionEmbeddingLearned(nn.Module):\n",
        "    \"\"\"\n",
        "    Absolute pos embedding, learned.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_pos_feats=16):\n",
        "        super().__init__()\n",
        "        self.row_embed = nn.Embedding(1000, num_pos_feats)\n",
        "        self.col_embed = nn.Embedding(1000, num_pos_feats)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        nn.init.uniform_(self.row_embed.weight)\n",
        "        nn.init.uniform_(self.col_embed.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = tensor_list.tensors\n",
        "        h, w = x.shape[-2:]\n",
        "        i = torch.arange(w, device=x.device)\n",
        "        j = torch.arange(h, device=x.device)\n",
        "        x_emb = self.col_embed(i)\n",
        "        y_emb = self.row_embed(j)\n",
        "        pos = x_emb.unsqueeze(0).repeat(h, 1, 1) + y_emb.unsqueeze(1).repeat(1, w, 1)\n",
        "        pos = pos.permute(2, 0, 1).unsqueeze(0).repeat(x.shape[0], 1, 1, 1)\n",
        "        # pos = (x_emb.unsqueeze(0).repeat(h, 1, 1) + y_emb.unsqueeze(1).repeat(1, w, 1)).unsqueeze(0).repeat(x.shape[0], 1, 1, 1)\n",
        "        # pos = torch.cat([\n",
        "        #     x_emb.unsqueeze(0).repeat(h, 1, 1),\n",
        "        #     y_emb.unsqueeze(1).repeat(1, w, 1),\n",
        "        # ], dim=-1).permute(2, 0, 1).unsqueeze(0).repeat(x.shape[0], 1, 1, 1)\n",
        "        return pos\n",
        "\n",
        "\n",
        "def build_position_encoding(args):\n",
        "    N_steps = args.hidden_dim // 2\n",
        "    if args.position_embedding in ('v2', 'sine'):\n",
        "        # TODO find a better way of exposing other arguments\n",
        "        position_embedding = PositionEmbeddingSine(N_steps, normalize=True)\n",
        "    elif args.position_embedding in ('v3', 'learned'):\n",
        "        position_embedding = PositionEmbeddingLearned(N_steps)\n",
        "    else:\n",
        "        raise ValueError(f\"not supported {args.position_embedding}\")\n",
        "\n",
        "    return position_embedding"
      ],
      "metadata": {
        "id": "F_KYFyPeLV2P"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#count_hooks.py FLOPs"
      ],
      "metadata": {
        "id": "DtorprGYPHwK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "multiply_adds = 1\n",
        "\n",
        "\n",
        "def count_convNd(m, x, y):\n",
        "    x = x[0]\n",
        "    cin = m.in_channels\n",
        "    batch_size = x.size(0)\n",
        "\n",
        "    kernel_ops = m.weight.size()[2:].numel()\n",
        "    bias_ops = 1 if m.bias is not None else 0\n",
        "    ops_per_element = kernel_ops + bias_ops\n",
        "    output_elements = y.nelement()\n",
        "\n",
        "    # cout x oW x oH\n",
        "    total_ops = batch_size * cin * output_elements * ops_per_element // m.groups\n",
        "    # total_ops = batch_size * output_elements * (cin * kernel_ops // m.groups + bias_ops)\n",
        "    m.total_ops = torch.Tensor([int(total_ops)])\n",
        "\n",
        "\n",
        "def count_conv2d(m, x, y):\n",
        "    x = x[0]\n",
        "\n",
        "    cin = m.in_channels\n",
        "    cout = m.out_channels\n",
        "    kh, kw = m.kernel_size\n",
        "    batch_size = x.size()[0]\n",
        "\n",
        "    out_h = y.size(2)\n",
        "    out_w = y.size(3)\n",
        "\n",
        "    # ops per output element\n",
        "    # kernel_mul = kh * kw * cin\n",
        "    # kernel_add = kh * kw * cin - 1\n",
        "    kernel_ops = multiply_adds * kh * kw\n",
        "    bias_ops = 1 if m.bias is not None else 0\n",
        "    ops_per_element = kernel_ops + bias_ops\n",
        "\n",
        "    # total ops\n",
        "    # num_out_elements = y.numel()\n",
        "    output_elements = batch_size * out_w * out_h * cout\n",
        "    total_ops = output_elements * ops_per_element * cin // m.groups\n",
        "\n",
        "    m.total_ops = torch.Tensor([int(total_ops)])\n",
        "\n",
        "\n",
        "def count_convtranspose2d(m, x, y):\n",
        "    x = x[0]\n",
        "\n",
        "    cin = m.in_channels\n",
        "    cout = m.out_channels\n",
        "    kh, kw = m.kernel_size\n",
        "    batch_size = x.size()[0]\n",
        "\n",
        "    out_h = y.size(2)\n",
        "    out_w = y.size(3)\n",
        "\n",
        "    # ops per output element\n",
        "    # kernel_mul = kh * kw * cin\n",
        "    # kernel_add = kh * kw * cin - 1\n",
        "    kernel_ops = multiply_adds * kh * kw * cin // m.groups\n",
        "    bias_ops = 1 if m.bias is not None else 0\n",
        "    ops_per_element = kernel_ops + bias_ops\n",
        "\n",
        "    # total ops\n",
        "    # num_out_elements = y.numel()\n",
        "    # output_elements = batch_size * out_w * out_h * cout\n",
        "    ops_per_element = m.weight.nelement()\n",
        "    output_elements = y.nelement()\n",
        "    total_ops = output_elements * ops_per_element\n",
        "\n",
        "    m.total_ops = torch.Tensor([int(total_ops)])\n",
        "\n",
        "\n",
        "def count_bn(m, x, y):\n",
        "    x = x[0]\n",
        "\n",
        "    nelements = x.numel()\n",
        "    # subtract, divide, gamma, beta\n",
        "    total_ops = 4 * nelements\n",
        "\n",
        "    m.total_ops = torch.Tensor([int(total_ops)])\n",
        "\n",
        "\n",
        "def count_relu(m, x, y):\n",
        "    x = x[0]\n",
        "\n",
        "    nelements = x.numel()\n",
        "    total_ops = nelements\n",
        "\n",
        "    m.total_ops = torch.Tensor([int(total_ops)])\n",
        "\n",
        "\n",
        "def count_sigmoid(m, x, y):\n",
        "    x = x[0]\n",
        "    nelements = x.numel()\n",
        "\n",
        "    total_exp = nelements\n",
        "    total_add = nelements\n",
        "    total_div = nelements\n",
        "\n",
        "    total_ops = total_exp + total_add + total_div\n",
        "    m.total_ops = torch.Tensor([int(total_ops)])\n",
        "\n",
        "def count_pixelshuffle(m, x, y):\n",
        "    x = x[0]\n",
        "    nelements = x.numel()\n",
        "    total_ops = nelements\n",
        "    m.total_ops = torch.Tensor([int(total_ops)])\n",
        "\n",
        "\n",
        "def count_softmax(m, x, y):\n",
        "    x = x[0]\n",
        "\n",
        "    batch_size, nfeatures = x.size()\n",
        "\n",
        "    total_exp = nfeatures\n",
        "    total_add = nfeatures - 1\n",
        "    total_div = nfeatures\n",
        "    total_ops = batch_size * (total_exp + total_add + total_div)\n",
        "\n",
        "    m.total_ops = torch.Tensor([int(total_ops)])\n",
        "\n",
        "\n",
        "def count_maxpool(m, x, y):\n",
        "    kernel_ops = torch.prod(torch.Tensor([m.kernel_size]))\n",
        "    num_elements = y.numel()\n",
        "    total_ops = kernel_ops * num_elements\n",
        "\n",
        "    m.total_ops = torch.Tensor([int(total_ops)])\n",
        "\n",
        "\n",
        "def count_adap_maxpool(m, x, y):\n",
        "    kernel = torch.Tensor([*(x[0].shape[2:])]) // torch.Tensor(list((m.output_size,))).squeeze()\n",
        "    kernel_ops = torch.prod(kernel)\n",
        "    num_elements = y.numel()\n",
        "    total_ops = kernel_ops * num_elements\n",
        "\n",
        "    m.total_ops = torch.Tensor([int(total_ops)])\n",
        "\n",
        "\n",
        "def count_avgpool(m, x, y):\n",
        "    total_add = torch.prod(torch.Tensor([m.kernel_size]))\n",
        "    total_div = 1\n",
        "    kernel_ops = total_add + total_div\n",
        "    num_elements = y.numel()\n",
        "    total_ops = kernel_ops * num_elements\n",
        "\n",
        "    m.total_ops = torch.Tensor([int(total_ops)])\n",
        "\n",
        "\n",
        "def count_adap_avgpool(m, x, y):\n",
        "    kernel = torch.Tensor([*(x[0].shape[2:])]) // torch.Tensor(list((m.output_size,))).squeeze()\n",
        "    total_add = torch.prod(kernel)\n",
        "    total_div = 1\n",
        "    kernel_ops = total_add + total_div\n",
        "    num_elements = y.numel()\n",
        "    total_ops = kernel_ops * num_elements\n",
        "\n",
        "    m.total_ops = torch.Tensor([int(total_ops)])\n",
        "\n",
        "\n",
        "def count_linear(m, x, y):\n",
        "    # per output element\n",
        "    total_mul = m.in_features\n",
        "    total_add = m.in_features - 1\n",
        "    num_elements = y.numel()\n",
        "    total_ops = (total_mul + total_add) * num_elements\n",
        "\n",
        "    m.total_ops = torch.Tensor([int(total_ops)])"
      ],
      "metadata": {
        "id": "XbYrCFETPL27"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#profile.py FLOPs"
      ],
      "metadata": {
        "id": "IKMk7y_1O5u6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.modules.conv import _ConvNd\n",
        "\n",
        "register_hooks = {\n",
        "    nn.Conv1d: count_convNd,\n",
        "    nn.Conv2d: count_convNd,\n",
        "    nn.Conv3d: count_convNd,\n",
        "    nn.ConvTranspose2d: count_convtranspose2d,\n",
        "\n",
        "    nn.BatchNorm1d: count_bn,\n",
        "    nn.BatchNorm2d: count_bn,\n",
        "    nn.BatchNorm3d: count_bn,\n",
        "\n",
        "    nn.ReLU: count_relu,\n",
        "    nn.ReLU6: count_relu,\n",
        "    nn.LeakyReLU: count_relu,\n",
        "    nn.PReLU: count_relu,\n",
        "\n",
        "    nn.MaxPool1d: count_maxpool,\n",
        "    nn.MaxPool2d: count_maxpool,\n",
        "    nn.MaxPool3d: count_maxpool,\n",
        "    nn.AdaptiveMaxPool1d: count_adap_maxpool,\n",
        "    nn.AdaptiveMaxPool2d: count_adap_maxpool,\n",
        "    nn.AdaptiveMaxPool3d: count_adap_maxpool,\n",
        "\n",
        "    nn.AvgPool1d: count_avgpool,\n",
        "    nn.AvgPool2d: count_avgpool,\n",
        "    nn.AvgPool3d: count_avgpool,\n",
        "\n",
        "    nn.AdaptiveAvgPool1d: count_adap_avgpool,\n",
        "    nn.AdaptiveAvgPool2d: count_adap_avgpool,\n",
        "    nn.AdaptiveAvgPool3d: count_adap_avgpool,\n",
        "    nn.Linear: count_linear,\n",
        "    nn.Dropout: None,\n",
        "    nn.PixelShuffle: count_pixelshuffle,\n",
        "    nn.Sigmoid: count_sigmoid,\n",
        "}\n",
        "\n",
        "\n",
        "def profile(model, input_size, custom_ops={}, device=\"cpu\"):\n",
        "    handler_collection = []\n",
        "\n",
        "    def add_hooks(m):\n",
        "        if len(list(m.children())) > 0:\n",
        "            return\n",
        "\n",
        "        m.register_buffer('total_ops', torch.zeros(1))\n",
        "        m.register_buffer('total_params', torch.zeros(1))\n",
        "\n",
        "        for p in m.parameters():\n",
        "            m.total_params += torch.Tensor([p.numel()])\n",
        "\n",
        "        m_type = type(m)\n",
        "        fn = None\n",
        "\n",
        "        if m_type in custom_ops:\n",
        "            fn = custom_ops[m_type]\n",
        "        elif m_type in register_hooks:\n",
        "            fn = register_hooks[m_type]\n",
        "        else:\n",
        "            print(\"Not implemented for \", m)\n",
        "\n",
        "        if fn is not None:\n",
        "            #print(\"Register FLOP counter for module %s\" % str(m))\n",
        "            handler = m.register_forward_hook(fn)\n",
        "            handler_collection.append(handler)\n",
        "\n",
        "    original_device = model.parameters().__next__().device\n",
        "    training = model.training\n",
        "\n",
        "    model.eval().to(device)\n",
        "    model.apply(add_hooks)\n",
        "\n",
        "    x = torch.zeros(input_size).to(device)\n",
        "    with torch.no_grad():\n",
        "        model(x)\n",
        "\n",
        "    total_ops = 0\n",
        "    total_params = 0\n",
        "    for m in model.modules():\n",
        "        if len(list(m.children())) > 0:  # skip for non-leaf module\n",
        "            continue\n",
        "        total_ops += m.total_ops\n",
        "        total_params += m.total_params\n",
        "\n",
        "    total_ops = total_ops.item()\n",
        "    total_params = total_params.item()\n",
        "\n",
        "    model.train(training).to(original_device)\n",
        "    for handler in handler_collection:\n",
        "        handler.remove()\n",
        "\n",
        "    return total_ops, total_params"
      ],
      "metadata": {
        "id": "ngmNt76JO-kj"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#transformer.py util"
      ],
      "metadata": {
        "id": "0A0jVMzrLMid"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pdb\n",
        "import math\n",
        "\n",
        "class DropPath(nn.Module):\n",
        "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
        "    \"\"\"\n",
        "    def __init__(self, drop_prob=None):\n",
        "        super(DropPath, self).__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "\n",
        "    def forward(self, x):\n",
        "        return drop_path(x, self.drop_prob, self.training)\n",
        "\n",
        "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
        "    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n",
        "    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,\n",
        "    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n",
        "    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for\n",
        "    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use\n",
        "    'survival rate' as the argument.\n",
        "    \"\"\"\n",
        "    if drop_prob == 0. or not training:\n",
        "        return x\n",
        "    keep_prob = 1 - drop_prob\n",
        "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
        "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
        "    random_tensor.floor_()  # binarize\n",
        "    output = x.div(keep_prob) * random_tensor\n",
        "    return output\n",
        "class PatchEmbed(nn.Module):\n",
        "    \"\"\" Image to Patch Embedding\n",
        "    \"\"\"\n",
        "    def __init__(self, img_size=48, patch_size=2, in_chans=64, embed_dim=768):\n",
        "        super().__init__()\n",
        "        img_size = tuple((img_size,img_size))\n",
        "        patch_size = tuple((patch_size,patch_size))\n",
        "        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = num_patches\n",
        "\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape #16*64*48*48\n",
        "        # FIXME look at relaxing size constraints\n",
        "        assert H == self.img_size[0] and W == self.img_size[1], \\\n",
        "            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
        "        # pdb.set_trace()\n",
        "        x = self.proj(x).flatten(2).transpose(1, 2)#64*48*48->768*6*6->768*36->36*768\n",
        "        return x\n",
        "class Mlp(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.ReLU, drop=0.):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features//4\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = act_layer()\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class EffAttention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "\n",
        "        self.reduce = nn.Linear(dim, dim//2, bias=qkv_bias)\n",
        "        self.qkv = nn.Linear(dim//2, dim//2 * 3, bias=qkv_bias)\n",
        "        self.proj = nn.Linear(dim//2, dim)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        print('scale in EffAttention:', self.scale)\n",
        "        print('dim in EffAttention:', dim)\n",
        "        print('head_dim in EffAttention:', dim//num_heads)\n",
        "        # self.proj = nn.Linear(dim, dim)\n",
        "        # self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.reduce(x)\n",
        "        B, N, C = x.shape\n",
        "        # pdb.set_trace()\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        # q = x.reshape(B, N, 1, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        # k = x.reshape(B, N, 1, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        # v = x.reshape(B, N, 1, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        # qkv: 3*16*8*37*96\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)\n",
        "        # pdb.set_trace()\n",
        "        \n",
        "        q_all = torch.split(q, math.ceil(N//4), dim=-2)\n",
        "        k_all = torch.split(k, math.ceil(N//4), dim=-2)\n",
        "        v_all = torch.split(v, math.ceil(N//4), dim=-2)        \n",
        "\n",
        "        output = []\n",
        "        for q,k,v in zip(q_all, k_all, v_all):\n",
        "            attn = (q @ k.transpose(-2, -1)) * self.scale   #16*8*37*37\n",
        "            attn = attn.softmax(dim=-1)\n",
        "            attn = self.attn_drop(attn)\n",
        "            \n",
        "            trans_x = (attn @ v).transpose(1, 2)#.reshape(B, N, C)\n",
        "\n",
        "            output.append(trans_x)\n",
        "        # pdb.set_trace()\n",
        "        # attn = torch.cat(att, dim=-2)\n",
        "        # x = (attn @ v).transpose(1, 2).reshape(B, N, C) #16*37*768\n",
        "        x = torch.cat(output,dim=1)\n",
        "        x = x.reshape(B,N,C)\n",
        "        # pdb.set_trace()\n",
        "        x = self.proj(x)\n",
        "        # x = self.proj_drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Base block\n",
        "class MLABlock(nn.Module):\n",
        "    def __init__(\n",
        "        self, n_feat = 64,dim=768, num_heads=8, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
        "                 drop_path=0., act_layer=nn.ReLU, norm_layer=nn.LayerNorm):\n",
        "        super(MLABlock, self).__init__()\n",
        "        self.dim = dim\n",
        "        self.atten = EffAttention(self.dim, num_heads=8, qkv_bias=False, qk_scale=None, \\\n",
        "                             attn_drop=0., proj_drop=0.)\n",
        "        self.norm1 = nn.LayerNorm(self.dim)\n",
        "        # self.posi = PositionEmbeddingLearned(n_feat)\n",
        "        self.mlp = Mlp(in_features=dim, hidden_features=dim//4, act_layer=act_layer, drop=drop)\n",
        "        self.norm2 = nn.LayerNorm(self.dim)\n",
        "    def forward(self, x):\n",
        "        # pdb.set_trace()\n",
        "        B = x.shape[0]\n",
        "        # posi = self.posi(x)\n",
        "        # x = posi + x\n",
        "        # x = self.patch_embed(x) # 16*36*768\n",
        "        # print(x.shape)\n",
        "        x = extract_image_patches(x, ksizes=[3, 3],\n",
        "                                      strides=[1,1],\n",
        "                                      rates=[1, 1],\n",
        "                                      padding='same')#   16*2304*576\n",
        "        x = x.permute(0,2,1)\n",
        "\n",
        "        x = x + self.atten(self.norm1(x))\n",
        "        x = x + self.mlp(self.norm2(x))#self.drop_path(self.mlp(self.norm2(x)))\n",
        "        return x"
      ],
      "metadata": {
        "id": "U4wBjDjOLRFU"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#esrt.py model"
      ],
      "metadata": {
        "id": "4yKlvjNJJAg4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pdb\n",
        "import math\n",
        "\n",
        "def make_model(upscale=4):\n",
        "    # inpu = torch.randn(1, 3, 320, 180).cpu()\n",
        "    # flops, params = profile(RTC(upscale).cpu(), inputs=(inpu,))\n",
        "    # print(params)\n",
        "    # print(flops)\n",
        "    return ESRT(upscale=upscale)\n",
        "\n",
        "\n",
        "## Channel Attention (CA) Layer\n",
        "class CALayer(nn.Module):\n",
        "    def __init__(self, channel, reduction=16):\n",
        "        super(CALayer, self).__init__()\n",
        "        # global average pooling: feature --> point\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        # feature channel downscale and upscale --> channel weight\n",
        "        self.conv_du = nn.Sequential(\n",
        "                nn.Conv2d(channel, channel // reduction, 1, padding=0, bias=True),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Conv2d(channel // reduction, channel, 1, padding=0, bias=True),\n",
        "                nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.avg_pool(x)\n",
        "        y = self.conv_du(y)\n",
        "        return x * y\n",
        "\n",
        "\n",
        "class one_conv(nn.Module):\n",
        "    def __init__(self,inchanels,growth_rate,kernel_size = 3, relu = True):\n",
        "        super(one_conv,self).__init__()\n",
        "        wn = lambda x:torch.nn.utils.weight_norm(x)\n",
        "        self.conv = nn.Conv2d(inchanels,growth_rate,kernel_size=kernel_size,padding = kernel_size>>1,stride= 1)\n",
        "        self.flag = relu\n",
        "        self.conv1 = nn.Conv2d(growth_rate,inchanels,kernel_size=kernel_size,padding = kernel_size>>1,stride= 1)\n",
        "        if relu:\n",
        "            self.relu = nn.PReLU(growth_rate)\n",
        "        self.weight1 = Scale(1)\n",
        "        self.weight2 = Scale(1)\n",
        "    def forward(self,x):\n",
        "        if self.flag == False:\n",
        "            output = self.weight1(x) + self.weight2(self.conv1(self.conv(x)))\n",
        "        else:\n",
        "            output = self.weight1(x) + self.weight2(self.conv1(self.relu(self.conv(x))))\n",
        "        return output#torch.cat((x,output),1)\n",
        "        \n",
        "class BasicConv(nn.Module):\n",
        "    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=1, dilation=1, groups=1, relu=True,\n",
        "                 bn=False, bias=False, up_size=0,fan=False):\n",
        "        super(BasicConv, self).__init__()\n",
        "        wn = lambda x:torch.nn.utils.weight_norm(x)\n",
        "        self.out_channels = out_planes\n",
        "        self.in_channels = in_planes\n",
        "        if fan:\n",
        "            self.conv = nn.ConvTranspose2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding,\n",
        "                              dilation=dilation, groups=groups, bias=bias)\n",
        "        else:\n",
        "            self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding,\n",
        "                              dilation=dilation, groups=groups, bias=bias)\n",
        "        self.bn = nn.BatchNorm2d(out_planes, eps=1e-5, momentum=0.01, affine=True) if bn else None\n",
        "        self.relu = nn.ReLU(inplace=True) if relu else None\n",
        "        self.up_size = up_size\n",
        "        self.up_sample = nn.Upsample(size=(up_size, up_size), mode='bilinear') if up_size != 0 else None\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        if self.bn is not None:\n",
        "            x = self.bn(x)\n",
        "        if self.relu is not None:\n",
        "            x = self.relu(x)\n",
        "        if self.up_size > 0:\n",
        "            x = self.up_sample(x)\n",
        "        return x\n",
        "\n",
        "class one_module(nn.Module):\n",
        "    def __init__(self, n_feats):\n",
        "        super(one_module, self).__init__()\n",
        "        self.layer1 = one_conv(n_feats, n_feats//2,3)\n",
        "        self.layer2 = one_conv(n_feats, n_feats//2,3)\n",
        "        # self.layer3 = one_conv(n_feats, n_feats//2,3)\n",
        "        self.layer4 = BasicConv(n_feats, n_feats, 3,1,1)\n",
        "        self.alise = BasicConv(2*n_feats, n_feats, 1,1,0)\n",
        "        self.atten = CALayer(n_feats)\n",
        "        self.weight1 = Scale(1)\n",
        "        self.weight2 = Scale(1)\n",
        "        self.weight3 = Scale(1)\n",
        "        self.weight4 = Scale(1)\n",
        "        self.weight5 = Scale(1)\n",
        "    def forward(self, x):\n",
        "\n",
        "        x1 = self.layer1(x)\n",
        "        x2 = self.layer2(x1)\n",
        "        # x3 = self.layer3(x2)\n",
        "        # pdb.set_trace()\n",
        "        x4 = self.layer4(self.atten(self.alise(torch.cat([self.weight2(x2),self.weight3(x1)],1))))\n",
        "        return self.weight4(x)+self.weight5(x4)\n",
        "\n",
        "class Updownblock(nn.Module):\n",
        "    def __init__(self, n_feats):\n",
        "        super(Updownblock, self).__init__()\n",
        "        self.encoder = one_module(n_feats)\n",
        "        self.decoder_low = one_module(n_feats) #nn.Sequential(one_module(n_feats),\n",
        "        #                     one_module(n_feats),\n",
        "        #                     one_module(n_feats))\n",
        "        self.decoder_high = one_module(n_feats)\n",
        "        self.alise = one_module(n_feats)\n",
        "        self.alise2 = BasicConv(2*n_feats, n_feats, 1,1,0) #one_module(n_feats)\n",
        "        self.down = nn.AvgPool2d(kernel_size=2)\n",
        "        self.att = CALayer(n_feats)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.encoder(x)\n",
        "        x2 = self.down(x1)\n",
        "        high = x1 - F.interpolate(x2, size = x.size()[-2:], mode='bilinear', align_corners=True)\n",
        "        for i in range(5):\n",
        "            x2 = self.decoder_low(x2)\n",
        "        x3 = x2\n",
        "        # x3 = self.decoder_low(x2)\n",
        "        high1 = self.decoder_high(high)\n",
        "        x4 = F.interpolate(x3, size = x.size()[-2:], mode='bilinear', align_corners=True)\n",
        "        return self.alise(self.att(self.alise2(torch.cat([x4,high1],dim=1))))+ x\n",
        "\n",
        "class Un(nn.Module):\n",
        "    def __init__(self,n_feats, wn):\n",
        "        super(Un, self).__init__()\n",
        "        self.encoder1 = Updownblock(n_feats)\n",
        "        self.encoder2 = Updownblock(n_feats)\n",
        "        self.encoder3 = Updownblock(n_feats)\n",
        "        self.reduce = default_conv(3*n_feats, n_feats, 3)\n",
        "        self.weight2 = Scale(1)\n",
        "        self.weight1 = Scale(1)\n",
        "        self.attention = MLABlock(n_feat=n_feats, dim=288) \n",
        "        self.alise = default_conv(n_feats, n_feats, 3)\n",
        "\n",
        "    def forward(self,x):\n",
        "        # out = self.encoder3(self.encoder2(self.encoder1(x)))\n",
        "        x1 = self.encoder1(x)\n",
        "        x2 = self.encoder2(x1)\n",
        "        x3 = self.encoder3(x2)\n",
        "        out = x3\n",
        "        b,c,h,w = x3.shape\n",
        "        out = self.attention(self.reduce(torch.cat([x1,x2,x3],dim=1)))\n",
        "        out = out.permute(0,2,1)\n",
        "        out = reverse_patches(out, (h,w), (3,3), 1, 1)\n",
        "        out = self.alise(out)\n",
        "\n",
        "        return self.weight1(x) + self.weight2(out)\n",
        "        \n",
        "class ESRT(nn.Module):\n",
        "    def __init__(self, upscale=4, conv=default_conv):\n",
        "        super(ESRT, self).__init__()\n",
        "        wn = lambda x:torch.nn.utils.weight_norm(x)\n",
        "        n_feats = 32\n",
        "        n_blocks = 1\n",
        "        kernel_size = 3\n",
        "        scale = upscale#args.scale[0] #gaile\n",
        "        act = nn.ReLU(True)\n",
        "        #self.up_sample = F.interpolate(scale_factor=2, mode='nearest')\n",
        "        self.n_blocks = n_blocks\n",
        "        \n",
        "        # RGB mean for DIV2K\n",
        "        rgb_mean = (0.4488, 0.4371, 0.4040)\n",
        "        rgb_std = (1.0, 1.0, 1.0)\n",
        "        # self.sub_mean = common.MeanShift(args.rgb_range, rgb_mean, rgb_std)\n",
        "        \n",
        "        # define head module\n",
        "        modules_head = [conv(3, n_feats, kernel_size)]\n",
        "        \n",
        "        # define body module\n",
        "        modules_body = nn.ModuleList()\n",
        "        for i in range(n_blocks):\n",
        "            modules_body.append(\n",
        "                Un(n_feats=n_feats, wn = wn))\n",
        "\n",
        "        # define tail module\n",
        "        modules_tail = [\n",
        "\n",
        "            Upsampler(conv, scale, n_feats, act=False),\n",
        "            conv(n_feats, 3, kernel_size)]\n",
        "\n",
        "\n",
        "        self.up = nn.Sequential(Upsampler(conv,scale,n_feats,act=False),\n",
        "                          BasicConv(n_feats, 3,3,1,1))\n",
        "        self.head = nn.Sequential(*modules_head)\n",
        "        self.body = nn.Sequential(*modules_body)\n",
        "        self.tail = nn.Sequential(*modules_tail)\n",
        "        self.reduce = conv(n_blocks*n_feats, n_feats, kernel_size)\n",
        "\n",
        "\n",
        "    def forward(self, x1,x2 = None, test=False):\n",
        "        # x1 = self.sub_mean(x1)\n",
        "        x1 = self.head(x1)\n",
        "        res2 = x1\n",
        "        #res2 = x2\n",
        "        body_out = []\n",
        "        for i in range(self.n_blocks):\n",
        "            x1 = self.body[i](x1)\n",
        "            body_out.append(x1)\n",
        "        res1 = torch.cat(body_out,1)\n",
        "        res1 = self.reduce(res1)\n",
        "\n",
        "        \n",
        "        x1 = self.tail(res1)\n",
        "        x1 = self.up(res2) + x1\n",
        "        # x1 = self.add_mean(x1)\n",
        "        #x2 = self.tail(res2)\n",
        "        return x1\n",
        "\n",
        "    def load_state_dict(self, state_dict, strict=False):\n",
        "        own_state = self.state_dict()\n",
        "        for name, param in state_dict.items():\n",
        "            if name in own_state:\n",
        "                if isinstance(param, nn.Parameter):\n",
        "                    param = param.data\n",
        "                try:\n",
        "                    own_state[name].copy_(param)\n",
        "                except Exception:\n",
        "                    if name.find('tail') >= 0:\n",
        "                        print('Replace pre-trained upsampler to new one...')\n",
        "                    else:\n",
        "                        raise RuntimeError('While copying the parameter named {}, '\n",
        "                                           'whose dimensions in the model are {} and '\n",
        "                                           'whose dimensions in the checkpoint are {}.'\n",
        "                                           .format(name, own_state[name].size(), param.size()))\n",
        "            elif strict:\n",
        "                if name.find('tail') == -1:\n",
        "                    raise KeyError('unexpected key \"{}\" in state_dict'\n",
        "                                   .format(name))\n",
        "\n",
        "        if strict:\n",
        "            missing = set(own_state.keys()) - set(state_dict.keys())\n",
        "            if len(missing) > 0:\n",
        "                raise KeyError('missing keys in state_dict: \"{}\"'.format(missing))\n",
        "        #MSRB_out = []from model import common"
      ],
      "metadata": {
        "id": "VUckwOggJDuz"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#utils.py"
      ],
      "metadata": {
        "id": "q-A8zczwJ8by"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "from collections import OrderedDict\n",
        "\n",
        "import time\n",
        "class Timer():\n",
        "\n",
        "    def __init__(self):\n",
        "        self.v = time.time()\n",
        "\n",
        "    def s(self):\n",
        "        self.v = time.time()\n",
        "\n",
        "    def t(self):\n",
        "        return time.time() - self.v\n",
        "\n",
        "\n",
        "def time_text(t):\n",
        "    if t >= 3600:\n",
        "        return '{:.1f}h'.format(t / 3600)\n",
        "    elif t >= 60:\n",
        "        return '{:.1f}m'.format(t / 60)\n",
        "    else:\n",
        "        return '{:.1f}s'.format(t)\n",
        "def compute_psnr(im1, im2):\n",
        "    p = psnr(im1, im2)\n",
        "    return p\n",
        "\n",
        "\n",
        "def compute_ssim(im1, im2):\n",
        "    isRGB = len(im1.shape) == 3 and im1.shape[-1] == 3\n",
        "    s = ssim(im1, im2, K1=0.01, K2=0.03, gaussian_weights=True, sigma=1.5, use_sample_covariance=False, multichannel=isRGB)\n",
        "    return s\n",
        "\n",
        "\n",
        "def shave(im, border):\n",
        "    border = [border, border]\n",
        "    im = im[border[0]:-border[0], border[1]:-border[1], ...]\n",
        "    return im\n",
        "\n",
        "\n",
        "def modcrop(im, modulo):\n",
        "    sz = im.shape\n",
        "    h = np.int32(sz[0] / modulo) * modulo\n",
        "    w = np.int32(sz[1] / modulo) * modulo\n",
        "    ims = im[0:h, 0:w, ...]\n",
        "    return ims\n",
        "\n",
        "\n",
        "def get_list(path, ext):\n",
        "    return [os.path.join(path, f) for f in os.listdir(path) if f.endswith(ext)]\n",
        "\n",
        "\n",
        "def convert_shape(img):\n",
        "    img = np.transpose((img * 255.0).round(), (1, 2, 0))\n",
        "    img = np.uint8(np.clip(img, 0, 255))\n",
        "    return img\n",
        "\n",
        "\n",
        "def quantize(img):\n",
        "    return img.clip(0, 255).round().astype(np.uint8)\n",
        "\n",
        "\n",
        "def tensor2np(tensor, out_type=np.uint8, min_max=(0, 1)):\n",
        "    tensor = tensor.float().cpu().clamp_(*min_max)\n",
        "    tensor = (tensor - min_max[0]) / (min_max[1] - min_max[0])  # to range [0, 1]\n",
        "    img_np = tensor.numpy()\n",
        "    img_np = np.transpose(img_np, (1, 2, 0))\n",
        "    if out_type == np.uint8:\n",
        "        img_np = (img_np * 255.0).round()\n",
        "\n",
        "    return img_np.astype(out_type)\n",
        "\n",
        "def convert2np(tensor):\n",
        "    return tensor.cpu().mul(255).clamp(0, 255).byte().squeeze().permute(1, 2, 0).numpy()\n",
        "\n",
        "\n",
        "def adjust_learning_rate(optimizer, epoch, step_size, lr_init, gamma):\n",
        "    factor = epoch // step_size\n",
        "    lr = lr_init * (gamma ** factor)\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "def load_state_dict(path):\n",
        "\n",
        "    state_dict = torch.load(path)\n",
        "    new_state_dcit = OrderedDict()\n",
        "    for k, v in state_dict.items():\n",
        "        if 'module' in k:\n",
        "            name = k[7:]\n",
        "        else:\n",
        "            name = k\n",
        "        new_state_dcit[name] = v\n",
        "    return new_state_dcit"
      ],
      "metadata": {
        "id": "EXZYkXBlJ-Lf"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#train.py"
      ],
      "metadata": {
        "id": "X6vhclL2Hmzg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse, os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import skimage.color as sc\n",
        "import random\n",
        "from collections import OrderedDict\n",
        "import datetime\n",
        "from importlib import import_module\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
        "\n",
        "# Training settings\n",
        "parser = argparse.ArgumentParser(description=\"ESRT\")\n",
        "parser.add_argument(\"--cuda\", action=\"store_true\", default=True, help=\"use cuda\")\n",
        "parser.add_argument(\"--batch_size\", type=int, default=8, help=\"training batch size\")\n",
        "parser.add_argument(\"--testBatchSize\", type=int, default=1, help=\"testing batch size\")\n",
        "parser.add_argument(\"--start-epoch\", default=200, type=int, help=\"manual epoch number\")\n",
        "parser.add_argument(\"-nEpochs\", type=int, default=1000, help=\"number of epochs to train\") #1000\n",
        "parser.add_argument(\"--lr\", type=float, default=2e-4, help=\"Learning Rate. Default=2e-4\")\n",
        "parser.add_argument(\"--step_size\", type=int, default=200, help=\"learning rate decay per N epochs\")\n",
        "parser.add_argument(\"--gamma\", type=int, default=0.5, help=\"learning rate decay factor for step decay\")\n",
        "parser.add_argument(\"--resume\", default=\"\", type=str, help=\"path to checkpoint\")\n",
        "parser.add_argument(\"--threads\", type=int, default=4, help=\"number of threads for data loading\") #8\n",
        "parser.add_argument(\"--root\", type=str, default=\"./drive/MyDrive/datasets/DIV2K/\", help='dataset directory')\n",
        "parser.add_argument(\"--n_train\", type=int, default=800, help=\"number of training set\")#800\n",
        "parser.add_argument(\"--n_val\", type=int, default=1, help=\"number of validation set\")\n",
        "parser.add_argument(\"--test_every\", type=int, default=1000)#1000\n",
        "parser.add_argument(\"--scale\", type=int, default=4, help=\"super-resolution scale\") # 2 , 3 ,4\n",
        "parser.add_argument(\"--patch_size\", type=int, default=192, help=\"output patch size\") # 96 , 144 , 192\n",
        "parser.add_argument(\"--rgb_range\", type=int, default=1, help=\"maxium value of RGB\")\n",
        "parser.add_argument(\"--n_colors\", type=int, default=3, help=\"number of color channels to use\")\n",
        "parser.add_argument(\"--pretrained\", default=\"./drive/MyDrive/ColabNotebooks/ESRT/experiment/checkpoint_ESRT_x4/epoch_200.pth\", type=str, help=\"path to pretrained models\")\n",
        "parser.add_argument(\"--seed\", type=int, default=1)\n",
        "parser.add_argument(\"--isY\", action=\"store_true\", default=True)\n",
        "parser.add_argument(\"--ext\", type=str, default='.npy')\n",
        "parser.add_argument(\"--phase\", type=str, default='train')\n",
        "parser.add_argument(\"--model\", type=str, default='ESRT')\n",
        "\n",
        "args = parser.parse_args(args = [])\n",
        "#print(args)\n",
        "torch.backends.cudnn.benchmark = True\n",
        "# random seed\n",
        "seed = args.seed\n",
        "if seed is None:\n",
        "    seed = random.randint(1, 10000)\n",
        "print(\"Random Seed: \", seed)\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "cuda = args.cuda\n",
        "device = torch.device('cuda' if cuda else 'cpu')\n",
        "\n",
        "print(\"===> Loading datasets\")\n",
        "\n",
        "trainset = div2k(args)\n",
        "testset = DatasetFromFolderVal(\"./drive/MyDrive/datasets/Set5/\", \"./drive/MyDrive/datasets/Set5/LRbicx{}/\".format(args.scale), args.scale) \n",
        "training_data_loader = DataLoader(dataset=trainset, num_workers=args.threads, batch_size=args.batch_size, shuffle=True, pin_memory=True, drop_last=True)\n",
        "testing_data_loader = DataLoader(dataset=testset, num_workers=args.threads, batch_size=args.testBatchSize, shuffle=False)\n",
        "\n",
        "print(\"===> Building models\")\n",
        "args.is_train = True\n",
        "\n",
        "model = ESRT(upscale = args.scale) #architecture.IMDN(upscale=args.scale)\n",
        "\n",
        "l1_criterion = nn.L1Loss()\n",
        "\n",
        "print(\"===> Setting GPU\")\n",
        "if cuda:\n",
        "    model = model.to(device)\n",
        "    l1_criterion = l1_criterion.to(device)\n",
        "\n",
        "if args.pretrained:\n",
        "\n",
        "    if os.path.isfile(args.pretrained):\n",
        "        print(\"===> loading models '{}'\".format(args.pretrained))\n",
        "        checkpoint = torch.load(args.pretrained)\n",
        "        new_state_dcit = OrderedDict()\n",
        "        for k, v in checkpoint.items():\n",
        "            if 'module' in k:\n",
        "                name = k[7:]\n",
        "            else:\n",
        "                name = k\n",
        "            new_state_dcit[name] = v\n",
        "        model_dict = model.state_dict()\n",
        "        pretrained_dict = {k: v for k, v in new_state_dcit.items() if k in model_dict}\n",
        "\n",
        "        for k, v in model_dict.items():\n",
        "            if k not in pretrained_dict:\n",
        "                print(k)\n",
        "        model.load_state_dict(pretrained_dict, strict=True)\n",
        "\n",
        "    else:\n",
        "        print(\"===> no models found at '{}'\".format(args.pretrained))\n",
        "\n",
        "print(\"===> Setting Optimizer\")\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
        "\n",
        "\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    adjust_learning_rate(optimizer, epoch, args.step_size, args.lr, args.gamma)\n",
        "    print('epoch =', epoch, 'lr = ', optimizer.param_groups[0]['lr'])\n",
        "    for iteration, (lr_tensor, hr_tensor) in enumerate(training_data_loader, 1):\n",
        "\n",
        "        if args.cuda:\n",
        "            lr_tensor = lr_tensor.to(device)  # ranges from [0, 1]\n",
        "            hr_tensor = hr_tensor.to(device)  # ranges from [0, 1]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        sr_tensor = model(lr_tensor)\n",
        "        loss_l1 = l1_criterion(sr_tensor, hr_tensor)\n",
        "        loss_sr = loss_l1\n",
        "\n",
        "        loss_sr.backward()\n",
        "        optimizer.step()\n",
        "        if iteration % 100 == 0:\n",
        "            print(\"===> Epoch[{}]({}/{}): Loss_l1: {:.5f}\".format(epoch, iteration, len(training_data_loader),\n",
        "                                                                  loss_l1.item()))\n",
        "def forward_chop(model, x, scale, shave=10, min_size=60000):\n",
        "    # scale = scale#self.scale[self.idx_scale]\n",
        "    n_GPUs = 1 #min(self.n_GPUs, 4)\n",
        "    b, c, h, w = x.size()\n",
        "    h_half, w_half = h // 2, w // 2\n",
        "    h_size, w_size = h_half + shave, w_half + shave\n",
        "    lr_list = [\n",
        "        x[:, :, 0:h_size, 0:w_size],\n",
        "        x[:, :, 0:h_size, (w - w_size):w],\n",
        "        x[:, :, (h - h_size):h, 0:w_size],\n",
        "        x[:, :, (h - h_size):h, (w - w_size):w]]\n",
        "\n",
        "    if w_size * h_size < min_size:\n",
        "        sr_list = []\n",
        "        for i in range(0, 4, n_GPUs):\n",
        "            lr_batch = torch.cat(lr_list[i:(i + n_GPUs)], dim=0)\n",
        "            sr_batch = model(lr_batch)\n",
        "            sr_list.extend(sr_batch.chunk(n_GPUs, dim=0))\n",
        "    else:\n",
        "        sr_list = [\n",
        "            forward_chop(model, patch, shave=shave, min_size=min_size) \\\n",
        "            for patch in lr_list\n",
        "        ]\n",
        "\n",
        "    h, w = scale * h, scale * w\n",
        "    h_half, w_half = scale * h_half, scale * w_half\n",
        "    h_size, w_size = scale * h_size, scale * w_size\n",
        "    shave *= scale\n",
        "\n",
        "    output = x.new(b, c, h, w)\n",
        "    output[:, :, 0:h_half, 0:w_half] \\\n",
        "        = sr_list[0][:, :, 0:h_half, 0:w_half]\n",
        "    output[:, :, 0:h_half, w_half:w] \\\n",
        "        = sr_list[1][:, :, 0:h_half, (w_size - w + w_half):w_size]\n",
        "    output[:, :, h_half:h, 0:w_half] \\\n",
        "        = sr_list[2][:, :, (h_size - h + h_half):h_size, 0:w_half]\n",
        "    output[:, :, h_half:h, w_half:w] \\\n",
        "        = sr_list[3][:, :, (h_size - h + h_half):h_size, (w_size - w + w_half):w_size]\n",
        "\n",
        "    return output\n",
        "\n",
        "def valid(scale):\n",
        "    model.eval()\n",
        "\n",
        "    avg_psnr, avg_ssim = 0, 0\n",
        "    for batch in testing_data_loader:\n",
        "        lr_tensor, hr_tensor = batch[0], batch[1]\n",
        "        if args.cuda:\n",
        "            lr_tensor = lr_tensor.to(device)\n",
        "            hr_tensor = hr_tensor.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            pre = forward_chop(model, lr_tensor, scale)#model(lr_tensor)\n",
        "\n",
        "        sr_img = tensor2np(pre.detach()[0])\n",
        "        gt_img = tensor2np(hr_tensor.detach()[0])\n",
        "        crop_size = args.scale\n",
        "        cropped_sr_img = shave(sr_img, crop_size)\n",
        "        cropped_gt_img = shave(gt_img, crop_size)\n",
        "        if args.isY is True:\n",
        "            im_label = quantize(sc.rgb2ycbcr(cropped_gt_img)[:, :, 0])\n",
        "            im_pre = quantize(sc.rgb2ycbcr(cropped_sr_img)[:, :, 0])\n",
        "        else:\n",
        "            im_label = cropped_gt_img\n",
        "            im_pre = cropped_sr_img\n",
        "        # print(im_pre.shape)\n",
        "        # print(im_label.shape)\n",
        "        avg_psnr += compute_psnr(im_pre, im_label)\n",
        "        avg_ssim += compute_ssim(im_pre, im_label)\n",
        "    print(\"===> Valid. psnr: {:.4f}, ssim: {:.4f}\".format(avg_psnr / len(testing_data_loader), avg_ssim / len(testing_data_loader)))\n",
        "\n",
        "\n",
        "def save_checkpoint(epoch):\n",
        "    model_folder = \"./drive/MyDrive/ColabNotebooks/ESRT/experiment/checkpoint_ESRT_x{}/\".format(args.scale)\n",
        "    model_out_path = model_folder + \"epoch_{}.pth\".format(epoch)\n",
        "    if not os.path.exists(model_folder):\n",
        "        os.makedirs(model_folder)\n",
        "    torch.save(model.state_dict(), model_out_path)\n",
        "    print(\"===> Checkpoint saved to {}\".format(model_out_path))\n",
        "\n",
        "def print_network(net):\n",
        "    num_params = 0\n",
        "    for param in net.parameters():\n",
        "        num_params += param.numel()\n",
        "    # print(net)\n",
        "    print('Total number of parameters: %d' % num_params)\n",
        "\n",
        "print(\"===> Training\")\n",
        "print_network(model)\n",
        "code_start = datetime.datetime.now()\n",
        "timer = Timer()\n",
        "\n",
        "for epoch in range(args.start_epoch, args.nEpochs + 1):\n",
        "    t_epoch_start = timer.t()\n",
        "    epoch_start = datetime.datetime.now()\n",
        "    valid(args.scale)\n",
        "    train(epoch)\n",
        "    if epoch%50==0:\n",
        "        save_checkpoint(epoch)\n",
        "    epoch_end = datetime.datetime.now()\n",
        "    print('Epoch cost times: %s' % str(epoch_end-epoch_start))\n",
        "    t = timer.t()\n",
        "    prog = (epoch-args.start_epoch+1)/(args.nEpochs + 1 - args.start_epoch + 1)\n",
        "    t_epoch = time_text(t - t_epoch_start)\n",
        "    t_elapsed, t_all = time_text(t), time_text(t / prog)\n",
        "    print('{} {}/{}'.format(t_epoch, t_elapsed, t_all))\n",
        "code_end = datetime.datetime.now()\n",
        "print('Code cost times: %s' % str(code_end-code_start))"
      ],
      "metadata": {
        "id": "6r6F0S7YHjjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#test.py"
      ],
      "metadata": {
        "id": "v1UnebsNJ-mF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import torch\n",
        "import os\n",
        "import numpy as np\n",
        "import skimage.color as sc\n",
        "import cv2\n",
        "\n",
        "# Testing settings\n",
        "\n",
        "parser = argparse.ArgumentParser(description='ESRT')\n",
        "parser.add_argument(\"--test_hr_folder\", type=str, default='./drive/MyDrive/datasets/Urban100/', help='the folder of the target images')\n",
        "parser.add_argument(\"--test_lr_folder\", type=str, default='./drive/MyDrive/datasets/Urban100/LRbicx4/', help='the folder of the input images')\n",
        "parser.add_argument(\"--output_folder\", type=str, default='./drive/MyDrive/ColabNotebooks/ESRT/results/Urban100/x4')\n",
        "parser.add_argument(\"--checkpoint\", type=str, default='./drive/MyDrive/ColabNotebooks/ESRT/experiment/checkpoint_ESRT_x4/epoch_200.pth', help='checkpoint folder to use')\n",
        "parser.add_argument('--cuda', action='store_true', default=True, help='use cuda')\n",
        "parser.add_argument(\"--upscale_factor\", type=int, default=4, help='upscaling factor')\n",
        "parser.add_argument(\"--is_y\", action='store_true', default=True, help='evaluate on y channel, if False evaluate on RGB channels')\n",
        "opt = parser.parse_args(args=[])\n",
        "\n",
        "#print(opt)\n",
        "\n",
        "def forward_chop(model, x, shave=10, min_size=60000):\n",
        "  \n",
        "  scale = 4 #self.scale[self.idx_scale]\n",
        "  n_GPUs = 1 #min(self.n_GPUs, 4)\n",
        "  b, c, h, w = x.size()\n",
        "  h_half, w_half = h // 2, w // 2\n",
        "  h_size, w_size = h_half + shave, w_half + shave\n",
        "  lr_list = [\n",
        "      x[:, :, 0:h_size, 0:w_size],\n",
        "      x[:, :, 0:h_size, (w - w_size):w],\n",
        "      x[:, :, (h - h_size):h, 0:w_size],\n",
        "      x[:, :, (h - h_size):h, (w - w_size):w]]\n",
        "\n",
        "  if w_size * h_size < min_size:\n",
        "    sr_list = []\n",
        "    for i in range(0, 4, n_GPUs):\n",
        "\n",
        "      lr_batch = torch.cat(lr_list[i:(i + n_GPUs)], dim=0)\n",
        "      sr_batch = model(lr_batch)\n",
        "      sr_list.extend(sr_batch.chunk(n_GPUs, dim=0))\n",
        "  \n",
        "  else:\n",
        "    sr_list = [\n",
        "        forward_chop(model, patch, shave=shave, min_size=min_size) \\\n",
        "        for patch in lr_list\n",
        "    ]\n",
        "\n",
        "  h, w = scale * h, scale * w\n",
        "  h_half, w_half = scale * h_half, scale * w_half\n",
        "  h_size, w_size = scale * h_size, scale * w_size\n",
        "  shave *= scale\n",
        "\n",
        "  output = x.new(b, c, h, w)\n",
        "  output[:, :, 0:h_half, 0:w_half] \\\n",
        "      = sr_list[0][:, :, 0:h_half, 0:w_half]\n",
        "  output[:, :, 0:h_half, w_half:w] \\\n",
        "      = sr_list[1][:, :, 0:h_half, (w_size - w + w_half):w_size]\n",
        "  output[:, :, h_half:h, 0:w_half] \\\n",
        "      = sr_list[2][:, :, (h_size - h + h_half):h_size, 0:w_half]\n",
        "  output[:, :, h_half:h, w_half:w] \\\n",
        "      = sr_list[3][:, :, (h_size - h + h_half):h_size, (w_size - w + w_half):w_size]\n",
        "\n",
        "  return output\n",
        "\n",
        "cuda = opt.cuda\n",
        "device = torch.device('cuda' if cuda else 'cpu')\n",
        "\n",
        "filepath = opt.test_hr_folder\n",
        "\n",
        "if filepath.split('/')[-2] == 'Set5' or filepath.split('/')[-2] == 'Set14' or filepath.split('/')[-2] == 'BSDS100' or filepath.split('/')[-2] == 'Urban100':\n",
        "  ext = '.png'\n",
        "else:\n",
        "  ext = '.bmp'\n",
        "\n",
        "filelist = get_list(filepath, ext=ext)\n",
        "psnr_list = np.zeros(len(filelist))\n",
        "ssim_list = np.zeros(len(filelist))\n",
        "time_list = np.zeros(len(filelist))\n",
        "\n",
        "model =  ESRT(upscale = opt.upscale_factor)\n",
        "model_dict = load_state_dict(opt.checkpoint)\n",
        "model.load_state_dict(model_dict, strict=False)#True)\n",
        "\n",
        "i = 0\n",
        "start = torch.cuda.Event(enable_timing=True)\n",
        "end = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "for imname in filelist:\n",
        "\n",
        "  im_gt = cv2.imread(imname, cv2.IMREAD_COLOR)[:, :, [2, 1, 0]]  # BGR to RGB\n",
        "  im_gt = modcrop(im_gt, opt.upscale_factor)\n",
        "  im_l = cv2.imread(opt.test_lr_folder + imname.split('/')[-1].split('.')[0] + ext, cv2.IMREAD_COLOR)[:, :, [2, 1, 0]]  # BGR to RGB    #'x' + str(opt.upscale_factor) +\n",
        "  \n",
        "  #print(\"im_gt:\\n\", im_gt.shape)\n",
        "  #print(\"im_l:\\n\" , im_l.shape)\n",
        "  \n",
        "  if len(im_gt.shape) < 3:\n",
        "\n",
        "    im_gt = im_gt[..., np.newaxis]\n",
        "    im_gt = np.concatenate([im_gt] * 3, 2)\n",
        "    im_l = im_l[..., np.newaxis]\n",
        "    im_l = np.concatenate([im_l] * 3, 2)\n",
        "\n",
        "    #print(\"len(im_gt.shape) < 3 shape:\\n\", im_gt.shape)\n",
        "    #print(\"len(im_lr.shape) < 3 shape:\\n\" , im_l.shape)\n",
        "\n",
        "\n",
        "  im_input = im_l / 255.0\n",
        "  im_input = np.transpose(im_input, (2, 0, 1))\n",
        "  im_input = im_input[np.newaxis, ...]\n",
        "  im_input = torch.from_numpy(im_input).float()\n",
        "\n",
        "  if cuda:\n",
        "    model = model.to(device)\n",
        "    im_input = im_input.to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    start.record()\n",
        "    out = forward_chop(model, im_input) #model(im_input)\n",
        "    end.record()\n",
        "    torch.cuda.synchronize()\n",
        "    time_list[i] = start.elapsed_time(end)  # milliseconds\n",
        "\n",
        "  out_img = tensor2np(out.detach()[0])\n",
        "  #print(\"im_sr:\\n\" , out_img.shape)\n",
        "\n",
        "  crop_size = opt.upscale_factor\n",
        "  cropped_sr_img = shave(out_img, crop_size)\n",
        "  #print(\"im_sr_crop:\\n\" , cropped_sr_img.shape)\n",
        "  cropped_gt_img = shave(im_gt, crop_size)\n",
        "  #print(\"im_gt_crop:\\n\" , cropped_gt_img.shape)\n",
        "\n",
        "  if opt.is_y is True:\n",
        "    #print(\"if opt.is_y is True\\n\")\n",
        "    im_label = quantize(sc.rgb2ycbcr(cropped_gt_img)[:, :, 0])\n",
        "    im_pre = quantize(sc.rgb2ycbcr(cropped_sr_img)[:, :, 0])\n",
        "    #print(\"y im_pre:\\n\" , im_pre.shape)\n",
        "    #print(\"y im_label:\\n\", im_label.shape)\n",
        "    if(im_label.shape != im_pre.shape):\n",
        "      #print(\"im_label shape:\\n\" , im_label.shape)\n",
        "      im_pre = cv2.resize((im_pre) , (int(im_label.shape[1]) , int(im_label.shape[0])))\n",
        "      #print(\"im_pre resized shape:\\n\" , im_pre.shape)\n",
        "  else:\n",
        "    im_label = cropped_gt_img\n",
        "    im_pre = cropped_sr_img\n",
        "\n",
        "  psnr_list[i] = compute_psnr(im_pre, im_label)\n",
        "  ssim_list[i] = compute_ssim(im_pre, im_label)\n",
        "\n",
        "\n",
        "  output_folder = os.path.join(opt.output_folder, imname.split('/')[-1].split('.')[0] + 'x' + str(opt.upscale_factor) + '.png')\n",
        "\n",
        "  if not os.path.exists(opt.output_folder):\n",
        "    os.makedirs(opt.output_folder)\n",
        "\n",
        "  cv2.imwrite(output_folder, out_img[:, :, [2, 1, 0]])\n",
        "  i += 1\n",
        "\n",
        "print(\"Mean PSNR: {}, SSIM: {}, TIME: {} ms\".format(np.mean(psnr_list), np.mean(ssim_list), np.mean(time_list)))"
      ],
      "metadata": {
        "id": "WkL0gOSUKARX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}